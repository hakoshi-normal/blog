[{"content":"ライブラリとは？ ライブラリとは，再利用可能なソフトウェア部品のことで，特定の機能をまとめたコードの再集合のことを指します．簡単に言えば，他の人が作ってくれた機能を自身のプログラムで使える便利なコード集のことです． Pythonには標準ライブラリと外部ライブラリの2種類があります．\n標準ライブラリ 標準ライブラリはPythonをインストールしたときに標準的に入っているライブラリのことです．通常のプログラム作成時に頻繁に必要となる機能をまとめたものが多くあります．例えば，\n乱数を作成する random 時刻を取得できる datetime, time ファイルシステムへのアクセスを実現する os, shutil 基礎的な数学関数 math HTMLサポート html URLを使ってインターネットにアクセスできる urllib スマートフォンで言うところのカメラ，アルバム，時計などのデフォルトアプリみたいなものです．これらはPythonが動く環境であれば勝手に入っているため，import ライブラリ名と記述すればすぐに利用することができます．助かる！！\n# サイコロの出目を出力 import random dice_roll = random.randint(1, 6) print(\u0026#34;サイコロの出目:\u0026#34;, dice_roll) 外部ライブラリ 外部ライブラリはPythonにはもともと組み込まれておらず，自身で導入するタイプのライブラリのことです．実装したいプログラムの機能に応じて適切に選択する必要があります．例えば，\n効率的な多次元配列の数値計算を実現する Numpy 画像処理の機能を提供する OpenCV グラフを簡単に描画できる Matplotlib 様々なアルゴリズムでの機械学習を可能とする scikit-learn Webアプリの開発に役立つ Django スマートフォンで言うところのゲームやチャットアプリなどのストアアプリみたいなものです．これらは自身でインストール作業をする必要がありますが，それさえ済ませば高機能な実装をいとも容易く実現できます！とても便利！！ 有志のプログラマが開発して公開してくれているため，感謝の気持ちを忘れずに使いましょう（Pythonもそうだけど）．\n# サイコロの出目を出力 import numpy as np dice_roll = np.random.randint(1, 7) print(\u0026#34;サイコロの出目:\u0026#34;, dice_roll) ライブラリを確認してみよう では，ここからは実際に手を動かしながら解説を進めます． まずはスタートメニューから Anaconda Prompt を開いてください．\nこれはコマンドプロンプトと呼ばれる画面で，ここにコマンドと呼ばれる命令文を入力することでコンピュータを操作できます． 試しに，ここにtreeと打ち込んでEnterキーを押してみてください．\n\u0026hellip;\u0026hellip;何かツリー構造の記述がたくさん出てきたかと思います．これはディレクトリ構造をツリー形式で表示するコマンドです（あんまり使わないけど）！ なかなか表示が終わらない場合はCtrl+Cキーでキャンセルしてください．\nではコマンドを使って，今使っているPythonで利用可能な外部ライブラリの一覧を見てみましょう！ Anaconda Promptにconda listと打ち込んでEnterキーを押してみてください．\nインストール済みライブラリとバージョンの一覧が表示されました．既にたくさん入っているようです！\nよくよく見てみると，数値計算ができるNumpyや表データを扱えるPandasなど，データサイエンスに役立つライブラリがたくさんあります． 実はAnacondaは科学計算のためのPython配布形式（ディストリビューション）の一つであり，既に役立つライブラリが揃えられているんです．\nこの状態，一見プログラムを書くのに便利な環境であるように見えるんですが実はそうでないケースがあります．\n実はライブラリは，機能を実現するために別のライブラリを参照するケースがあります．例えば，\n画像処理を実現するOpenCVには，画像を配列として扱うためにNumpyが必要 グラフを描画するMatplotlibには，画像描画のためにPillowが必要 機械学習を実現するscikit-learnには，高度な数値計算をするためにScipyが必要 またライブラリはバージョンによって提供される機能が変化することがあります．そのため，同じ実行環境に特定のバージョンを指定して参照ライブラリを導入しておく必要があるのです．\nこれを，ライブラリの依存関係と言います．この依存関係が原因で複数のライブラリを導入するときに問題が発生する可能性があるんです．\n例えば，ライブラリA-C及びB-Cの依存関係があるとき，AとBが共存するためにはCのバージョンが整合している必要があります．\nこうしたライブラリバージョンの試行錯誤を行う場合，先程表示させた複雑に依存関係が絡み合ったライブラリ群を扱うのはなかなか骨が折れます．\nまた，不要なライブラリは更に混乱を引き起こすので必要となる機能のための最小構成で開発環境を整える必要があります．\nそこで役立つのが，仮想環境です！！！（やっと本題）\n仮想環境 仮想環境とは，プログラムを動かすときに仮に作って利用する動作環境のことです． 簡単に言うと「依存関係の解決が面倒だから，まっさらな世界を一から創造すればいいじゃん！！」です．\nでは，実際に作ってみましょう．\n先ほど開いたAnaconda Promptにconda create -n test python=3.12と打ち込んで実行（Proceed([y]/n)?にはyを入力して処理を続行） conda activate testと打ち込んで実行 ここでは，testという名前の仮想環境を作成してその環境に切り替える，という作業をしてもらいました． 最新行の左側が(test)と表示されていれば成功です！\n先ほどまでは(base)と表示されていたはずです．Anacondaではデフォルトでbase環境があり，そこにたくさんのライブラリがありました． ではconda listで現在のtest環境のライブラリ構成を見てみましょう！\nbase環境と比べてライブラリの数がかなり少なくなったはずです．ここに表示されているライブラリはPython3.12に付随する必要最低限のライブラリです． このきれいな環境に最小限の適切なライブラリを導入することで，快適なコーディングライフを送ることができます！！\n番外編：Conda仮想環境の管理 複数の仮想環境を扱うときに便利なコマンドをまとめておきます！\n仮想環境の作成（環境名がtest，Python3.12の場合）\nconda create -n test python=3.12 仮想環境への移行\nconda activate test 仮想環境から離脱\nconda deactivate 仮想環境の削除\nconda remove -n test --all 仮想環境一覧の表示\nconda info -e ","permalink":"http://localhost:1313/blog/posts/20250506_condavenv/","summary":"\u003ch2 id=\"ライブラリとは\"\u003eライブラリとは？\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eライブラリ\u003c/strong\u003eとは，再利用可能なソフトウェア部品のことで，特定の機能をまとめたコードの再集合のことを指します．簡単に言えば，他の人が作ってくれた機能を自身のプログラムで使える便利なコード集のことです．\nPythonには\u003cstrong\u003e標準ライブラリ\u003c/strong\u003eと\u003cstrong\u003e外部ライブラリ\u003c/strong\u003eの2種類があります．\u003c/p\u003e\n\u003ch3 id=\"標準ライブラリ\"\u003e標準ライブラリ\u003c/h3\u003e\n\u003cp\u003e標準ライブラリはPythonをインストールしたときに標準的に入っているライブラリのことです．通常のプログラム作成時に頻繁に必要となる機能をまとめたものが多くあります．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e乱数を作成する random\u003c/li\u003e\n\u003cli\u003e時刻を取得できる datetime, time\u003c/li\u003e\n\u003cli\u003eファイルシステムへのアクセスを実現する os, shutil\u003c/li\u003e\n\u003cli\u003e基礎的な数学関数 math\u003c/li\u003e\n\u003cli\u003eHTMLサポート html\u003c/li\u003e\n\u003cli\u003eURLを使ってインターネットにアクセスできる urllib\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスマートフォンで言うところのカメラ，アルバム，時計などのデフォルトアプリみたいなものです．これらはPythonが動く環境であれば勝手に入っているため，\u003ccode\u003eimport ライブラリ名\u003c/code\u003eと記述すればすぐに利用することができます．助かる！！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# サイコロの出目を出力\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e random\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edice_roll \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e random\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandint(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;サイコロの出目:\u0026#34;\u003c/span\u003e, dice_roll)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"外部ライブラリ\"\u003e外部ライブラリ\u003c/h3\u003e\n\u003cp\u003e外部ライブラリはPythonにはもともと組み込まれておらず，自身で導入するタイプのライブラリのことです．実装したいプログラムの機能に応じて適切に選択する必要があります．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e効率的な多次元配列の数値計算を実現する Numpy\u003c/li\u003e\n\u003cli\u003e画像処理の機能を提供する OpenCV\u003c/li\u003e\n\u003cli\u003eグラフを簡単に描画できる Matplotlib\u003c/li\u003e\n\u003cli\u003e様々なアルゴリズムでの機械学習を可能とする scikit-learn\u003c/li\u003e\n\u003cli\u003eWebアプリの開発に役立つ Django\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスマートフォンで言うところのゲームやチャットアプリなどのストアアプリみたいなものです．これらは自身でインストール作業をする必要がありますが，それさえ済ませば高機能な実装をいとも容易く実現できます！とても便利！！\n有志のプログラマが開発して公開してくれているため，感謝の気持ちを忘れずに使いましょう（Pythonもそうだけど）．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# サイコロの出目を出力\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e np\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edice_roll \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandom\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandint(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;サイコロの出目:\u0026#34;\u003c/span\u003e, dice_roll)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"ライブラリを確認してみよう\"\u003eライブラリを確認してみよう\u003c/h2\u003e\n\u003cp\u003eでは，ここからは実際に手を動かしながら解説を進めます．\nまずはスタートメニューから \u003cstrong\u003eAnaconda Prompt\u003c/strong\u003e を開いてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"img_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"console_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこれは\u003cstrong\u003eコマンドプロンプト\u003c/strong\u003eと呼ばれる画面で，ここに\u003cstrong\u003eコマンド\u003c/strong\u003eと呼ばれる命令文を入力することでコンピュータを操作できます．\n試しに，ここに\u003ccode\u003etree\u003c/code\u003eと打ち込んでEnterキーを押してみてください．\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;何かツリー構造の記述がたくさん出てきたかと思います．これはディレクトリ構造をツリー形式で表示するコマンドです（あんまり使わないけど）！\nなかなか表示が終わらない場合はCtrl+Cキーでキャンセルしてください．\u003c/p\u003e\n\u003cp\u003eではコマンドを使って，今使っているPythonで利用可能な外部ライブラリの一覧を見てみましょう！\nAnaconda Promptに\u003ccode\u003econda list\u003c/code\u003eと打ち込んでEnterキーを押してみてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"modules.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eインストール済みライブラリとバージョンの一覧が表示されました．既にたくさん入っているようです！\u003c/p\u003e\n\u003cp\u003eよくよく見てみると，数値計算ができるNumpyや表データを扱えるPandasなど，データサイエンスに役立つライブラリがたくさんあります．\n実はAnacondaは科学計算のためのPython配布形式（ディストリビューション）の一つであり，既に役立つライブラリが揃えられているんです．\u003c/p\u003e\n\u003cp\u003eこの状態，一見プログラムを書くのに便利な環境であるように見えるんですが実はそうでないケースがあります．\u003c/p\u003e","title":"【入門1】Python仮想環境"},{"content":"JupyterLab+Visual Studio Codeを使ったPython実行 Visual Studio CodeにJupyter拡張機能をインストール 空のファイルを作成し，拡張子を.ipynbとする ＋コードボタンからコードブロックを作成し，Pythonプログラムを記述 右上カーネルの選択から実行したいPython環境を選択し コードブロック左側の実行ボタン▷を押下 ポップアップが出てきたらインストールをクリックしてipykernelをインストール （pip install ipykernelを当該環境で実行してインストールも出来ます） YOLOのセットアップ 今回は最新のYOLOを簡単に扱うことができるUltralyticsパッケージを利用します．\nconda create --name yolo-env python=3.11を実行して仮想環境を作成 conda activate yolo-envで仮想環境を有効化 pip install ultralyticsでUltralyticsパッケージをインストール ここまで済んだら，テストコードを動かしてみましょう！\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolo11n.pt\u0026#34;) results = model(\u0026#34;https://ultralytics.com/images/bus.jpg\u0026#34;) results[0].show() バスを背景にした画像の物体検出が表示されたはずです！\n作業ディレクトリを開くと，yolo11n.ptというファイルがダウンロードされています．これが，YOLOのモデルファイルです．これはYOLOのデフォルトの訓練済みモデルで，COCO Datasetに収録された80種類の物体を識別することが出来ます．\n","permalink":"http://localhost:1313/blog/posts/20250506_yolojupyter/","summary":"\u003ch2 id=\"jupyterlabvisual-studio-codeを使ったpython実行\"\u003eJupyterLab+Visual Studio Codeを使ったPython実行\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eVisual Studio CodeにJupyter拡張機能をインストール\u003c/li\u003e\n\u003cli\u003e空のファイルを作成し，拡張子を\u003ccode\u003e.ipynb\u003c/code\u003eとする\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e＋コード\u003c/code\u003eボタンからコードブロックを作成し，Pythonプログラムを記述\u003c/li\u003e\n\u003cli\u003e右上\u003ccode\u003eカーネルの選択\u003c/code\u003eから実行したいPython環境を選択し\u003c/li\u003e\n\u003cli\u003eコードブロック左側の実行ボタン\u003ccode\u003e▷\u003c/code\u003eを押下\u003c/li\u003e\n\u003cli\u003eポップアップが出てきたら\u003ccode\u003eインストール\u003c/code\u003eをクリックして\u003ccode\u003eipykernel\u003c/code\u003eをインストール\n（\u003ccode\u003epip install ipykernel\u003c/code\u003eを当該環境で実行してインストールも出来ます）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"vsjupyter.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"yoloのセットアップ\"\u003eYOLOのセットアップ\u003c/h2\u003e\n\u003cp\u003e今回は最新のYOLOを簡単に扱うことができるUltralyticsパッケージを利用します．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003econda create --name yolo-env python=3.11\u003c/code\u003eを実行して仮想環境を作成\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate yolo-env\u003c/code\u003eで仮想環境を有効化\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install ultralytics\u003c/code\u003eでUltralyticsパッケージをインストール\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eここまで済んだら，テストコードを動かしてみましょう！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e ultralytics \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e YOLO\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emodel \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e YOLO(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;yolo11n.pt\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresults \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://ultralytics.com/images/bus.jpg\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresults[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eバスを背景にした画像の物体検出が表示されたはずです！\u003c/p\u003e\n\u003cp\u003e作業ディレクトリを開くと，\u003ccode\u003eyolo11n.pt\u003c/code\u003eというファイルがダウンロードされています．これが，YOLOのモデルファイルです．これはYOLOのデフォルトの訓練済みモデルで，COCO Datasetに収録された80種類の物体を識別することが出来ます．\u003c/p\u003e","title":"【入門2】YOLO，Jupyterのセットアップ"},{"content":"アノテーションとは？ 機械学習の分類のひとつである教師あり学習では，まず入力データと出力データの組を複数用意し，それらを使って機械学習モデルを訓練します．\n入力データと出力データの組は，タスクによって様々です．例えば，\n入力：画像データ，出力：それぞれが犬or猫 入力：家の情報（間取り，立地，築年数など），出力：家の価格 入力：これまでの気温，出力：明日の気温 入力：画像データ，出力：各物体の座標 どんなタスクを前提とする場合でも，これら入力データと出力データの組を作る作業が必要となります．これがアノテーションです．\n環境構築 今回はYOLOによる画像物体認識を目標とします．そこで，アノテーションで作るべきデータは以下になります．\n入力：画像データ，出力：各物体の座標 頑張れば用意した画像の座標を数えてアノテーションできそうですが，今回はより簡単に作業を進めるためにLabelmeというツールを使います．GUIが整備されたソフトウェアですが，Pythonライブラリとして無償で提供されています！（ありがたい！！）\nAnaconda Promptを開く conda create -n annot_env python=3.12で仮想環境を作成 conda activate annot_envで仮想環境を有効化 pip install --upgrade labelmeと入力しLabelmeをインストール これでインストールは完了です．そのままannot_env環境内でlabelmeコマンドを実行してみてください！\nこの画面が表示されたら成功です！\nいざ，アノテーション 画像読み込み 今回は画像から麻雀牌を物体認識するタスクに挑戦してみます！サンプル画像は以下からダウンロード，解凍してください．\nサンプル画像 Zipファイルの解凍が済んだらLabelmeの画面左側にあるOpen Dirから，画像のディレクトリを開いてください．\nこれでアノテーションの準備は完了です！\n範囲選択，ラベル付け アノテーションの流れは以下になります．\n画像を右クリック，Create Rectangleをクリック 物体の範囲を四角形で囲んで選択，間違えたらEscキーで戻る 出てきたポップアップの上部Enter object labelにラベルを入力してOKをクリック 2と3を物体の数だけひたすら繰り返す 画像内のアノテーションが終わったら画面左部Saveから画像と同名のJSONファイルを保存 ちょっとやってみますね～\nはい，できました．ラベルは牌の種類（萬子筒子索子字牌）の略称です．\n\u0026hellip;\u0026hellip;\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\nめんどくさい\n機械学習では一般的にデータの数が大きいほど性能が上がりやすいとされています．つまり，（要件の要求レベルにもよりますが）精度を上げるためにはたくさんアノテーションする必要があるんです．\nそこで，特定のタスクの機械学習を考える場合はアノテーションに取り掛かる前に，データセットが利用可能なライセンスで公開されていないかどうか調べるところからスタートします．例えばYOLOの場合，Roboflow，VisualData，Google Open Images Datasetなどが画像データセットを公開している他，タスクに応じて論文と併せて公開されているデータセットも多数あります．\nまた今回はアノテーション入門ということで手作業での方法を紹介していますが，画像認識AIによる半自動ツールもあるので興味があれば調べてみてください．\n今回は，なんと，特別に，アノテーション済みファイルを用意してあります！\nアノテーション結果（JSON） ZIPファイルをダウンロード＆解凍したら，datasetsという名前のディレクトリを作成し，先程の画像ファイルとJSONファイルを以下の構造で配置してください．（image_1.jsonは自作のファイルで構いません！）\n.\r└── datasets/\r├── image_1.jpg\r├── image_1.json\r├── image_2.jpg\r├── image_2.json\r├── 〜省略〜\r├── image_10.jpg\r└── image_10.json データセットフォーマットの変換 では，学習させるデータの中身を一度見てみましょう！image_1.jsonを開いてみます．\nJSONはデータ記述形式のひとつで，キーと値を並べて書くのが特徴です．Pythonの辞書型そっくりですね． ラベルや座標がファイルの中に複数記述されていることがわかります．\n少しスクロールすると，よくわからない文字列が大量に記述されています．実はこれ，Base64と呼ばれるエンコード形式で画像そのものが書きつけられているんです．これにより，このJSONファイル単体で入力データを表現することが出来ます．\nさて，ひととおり学習データの中身を見てもらいましたが，実はこのままではYOLOの学習が出来ないんです．\nYOLOに学習させるためには，画像のアノテーションを以下の形式で記述する必要があります．\n画像ファイル名.txt\nlabel_id X Y Width Height 例えば，image_1.jsonをこの形式で記述すると以下のようになります．\nimage_1.txt\n4 0.171569 0.375817 0.142157 0.183007\r6 0.286765 0.382353 0.124183 0.183007\r0 0.401144 0.398693 0.127451 0.185185\r5 0.517157 0.408497 0.117647 0.178649\r10 0.638889 0.418301 0.125817 0.189542\r7 0.767157 0.436819 0.127451 0.174292 YOLOのデータセットでは，ラベルの番号，X座標，Y座標，幅，高さを列挙して記述します．\nこれを手作業で変換するのはかなり面倒です．\nそこで今回は，Labelmeで保存したJSONファイルをYOLOのデータセット形式に変換するツール，labelme2yoloを使います．condaのannot_env環境下で以下の手順で変換します．\npip install labelme2yoloでlabelme2yoloをインストール datasetディレクトリの一つ上の階層で以下のコマンドを実行 labelme2yolo --json_dir ./datasets/ --val_size 0.10 --output_format \u0026quot;bbox\u0026quot; これが済んだら，datasetsディレクトリの中にYOLODatasetが生成されているはずです．構造は以下のようになっています．\n.\r└── YOLODatasets/\r├── images/\r│ ├── train/\r│ │ ├── image_1.jpg\r│ │ └── ～省略～\r│ └── val/\r│ └── image_7.jpg\r├── labels/\r│ ├── train/\r│ │ ├── image_1.txt\r│ │ └── ～省略～\r│ └── val/\r│ └── image_7.txt\r└── dataset.yaml 先ほどのデータセット構造より複雑になっていますね．画像とラベルが別のフォルダに格納され，更にその中で訓練用(train)と評価用(val)に分かれています．\nlabelsディレクトリのテキストファイルを見てみましょう．YOLOのフォーマットに変換され，ラベルIDや座標が記録されているはずです．\ndataset.yamlの中身を見てみましょう．\nデータセットまでの絶対パス，訓練評価用ディレクトリのパス，ラベルIDとラベル名の対応が記載されています．YOLOの学習時にはこのdataset.yamlを利用してデータセットを読み込みます．\nYOLOの学習 それでは，いよいよ作ったデータセットを使ってYOLOの学習をしてみましょう！！\nまず，テスト用の画像を用意します．以下のリンクから画像ファイルimage_11.jpgをダウンロードしてdataset.yamlと同階層に配置します．\nテスト画像image_11.jpg dataset.yamlと同階層に.ipynbファイルを作成して，以下のPythonプログラムを実行してみましょう．\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolo11n.pt\u0026#34;) model.train(data=\u0026#39;dataset.yaml\u0026#39;, epochs=300, verbose=False) results = model(\u0026#34;image_11.jpg\u0026#34;) results[0].show() CPUのみの実行環境ではかなり時間がかかるので気長に待ちましょう～\n学習が終了すると，image_11.jpgに対する推論結果が表示されます．\n未検出や誤検出がちらほらありますが，部分的に検出が成功していることがわかります．データセットサイズやエポック数を変えることで性能の改善が期待できます．\nまた，データセットディレクトリのruns内を見ると，weightディレクトリにbest.ptが保存されていることがわかります． これは今回学習したモデルファイルで，実際に物体検出をシステム開発に組み込むときに使用します．\n","permalink":"http://localhost:1313/blog/posts/20250506_yoloannotation/","summary":"\u003ch2 id=\"アノテーションとは\"\u003eアノテーションとは？\u003c/h2\u003e\n\u003cp\u003e機械学習の分類のひとつである\u003cstrong\u003e教師あり学習\u003c/strong\u003eでは，まず入力データと出力データの組を複数用意し，それらを使って機械学習モデルを訓練します．\u003c/p\u003e\n\u003cp\u003e入力データと出力データの組は，タスクによって様々です．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e入力：画像データ，出力：それぞれが犬or猫\u003c/li\u003e\n\u003cli\u003e入力：家の情報（間取り，立地，築年数など），出力：家の価格\u003c/li\u003e\n\u003cli\u003e入力：これまでの気温，出力：明日の気温\u003c/li\u003e\n\u003cli\u003e入力：画像データ，出力：各物体の座標\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eどんなタスクを前提とする場合でも，これら\u003cstrong\u003e入力データと出力データの組を作る作業\u003c/strong\u003eが必要となります．これが\u003cstrong\u003eアノテーション\u003c/strong\u003eです．\u003c/p\u003e\n\u003ch2 id=\"環境構築\"\u003e環境構築\u003c/h2\u003e\n\u003cp\u003e今回はYOLOによる画像物体認識を目標とします．そこで，アノテーションで作るべきデータは以下になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e入力：画像データ，出力：各物体の座標\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e頑張れば用意した画像の座標を数えてアノテーションできそうですが，今回はより簡単に作業を進めるために\u003cstrong\u003eLabelme\u003c/strong\u003eというツールを使います．GUIが整備されたソフトウェアですが，Pythonライブラリとして無償で提供されています！（ありがたい！！）\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAnaconda Promptを開く\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda create -n annot_env python=3.12\u003c/code\u003eで仮想環境を作成\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate annot_env\u003c/code\u003eで仮想環境を有効化\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install --upgrade labelme\u003c/code\u003eと入力しLabelmeをインストール\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eこれでインストールは完了です．そのまま\u003ccode\u003eannot_env\u003c/code\u003e環境内で\u003ccode\u003elabelme\u003c/code\u003eコマンドを実行してみてください！\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"labelme.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこの画面が表示されたら成功です！\u003c/p\u003e\n\u003ch2 id=\"いざアノテーション\"\u003eいざ，アノテーション\u003c/h2\u003e\n\u003ch3 id=\"画像読み込み\"\u003e画像読み込み\u003c/h3\u003e\n\u003cp\u003e今回は画像から麻雀牌を物体認識するタスクに挑戦してみます！サンプル画像は以下からダウンロード，解凍してください．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hakoshi-normal/yolo_sample/raw/refs/heads/main/images/images.zip\"\u003eサンプル画像\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eZipファイルの解凍が済んだらLabelmeの画面左側にある\u003ccode\u003eOpen Dir\u003c/code\u003eから，画像のディレクトリを開いてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"labelme_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこれでアノテーションの準備は完了です！\u003c/p\u003e\n\u003ch3 id=\"範囲選択ラベル付け\"\u003e範囲選択，ラベル付け\u003c/h3\u003e\n\u003cp\u003eアノテーションの流れは以下になります．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e画像を右クリック，\u003ccode\u003eCreate Rectangle\u003c/code\u003eをクリック\u003c/li\u003e\n\u003cli\u003e物体の範囲を四角形で囲んで選択，間違えたら\u003ccode\u003eEsc\u003c/code\u003eキーで戻る\u003c/li\u003e\n\u003cli\u003e出てきたポップアップの上部\u003ccode\u003eEnter object label\u003c/code\u003eにラベルを入力して\u003ccode\u003eOK\u003c/code\u003eをクリック\u003c/li\u003e\n\u003cli\u003e2と3を物体の数だけひたすら繰り返す\u003c/li\u003e\n\u003cli\u003e画像内のアノテーションが終わったら画面左部\u003ccode\u003eSave\u003c/code\u003eから画像と同名のJSONファイルを保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eちょっとやってみますね～\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"annotated.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eはい，できました．ラベルは牌の種類（萬子筒子索子字牌）の略称です．\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 150%; color: red;\"\u003eめんどくさい\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e機械学習では一般的にデータの数が大きいほど性能が上がりやすいとされています．つまり，（要件の要求レベルにもよりますが）精度を上げるためにはたくさんアノテーションする必要があるんです．\u003c/p\u003e\n\u003cp\u003eそこで，特定のタスクの機械学習を考える場合はアノテーションに取り掛かる前に，データセットが利用可能なライセンスで公開されていないかどうか調べるところからスタートします．例えばYOLOの場合，\u003ca href=\"https://universe.roboflow.com/\"\u003eRoboflow\u003c/a\u003e，\u003ca href=\"https://visualdata.io/discovery\"\u003eVisualData\u003c/a\u003e，\u003ca href=\"https://storage.googleapis.com/openimages/web/index.html\"\u003eGoogle Open Images Dataset\u003c/a\u003eなどが画像データセットを公開している他，タスクに応じて論文と併せて公開されているデータセットも多数あります．\u003c/p\u003e\n\u003cp\u003eまた今回はアノテーション入門ということで手作業での方法を紹介していますが，画像認識AIによる半自動ツールもあるので興味があれば調べてみてください．\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e今回は，なんと，特別に，アノテーション済みファイルを用意してあります！\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hakoshi-normal/yolo_sample/raw/refs/heads/main/images/labels.zip\"\u003eアノテーション結果（JSON）\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eZIPファイルをダウンロード＆解凍したら，\u003ccode\u003edatasets\u003c/code\u003eという名前のディレクトリを作成し，先程の画像ファイルとJSONファイルを以下の構造で配置してください．（\u003ccode\u003eimage_1.json\u003c/code\u003eは自作のファイルで構いません！）\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-planetext\" data-lang=\"planetext\"\u003e.\r\n└── datasets/\r\n    ├── image_1.jpg\r\n    ├── image_1.json\r\n    ├── image_2.jpg\r\n    ├── image_2.json\r\n    ├── 〜省略〜\r\n    ├── image_10.jpg\r\n    └── image_10.json\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"データセットフォーマットの変換\"\u003eデータセットフォーマットの変換\u003c/h3\u003e\n\u003cp\u003eでは，学習させるデータの中身を一度見てみましょう！\u003ccode\u003eimage_1.json\u003c/code\u003eを開いてみます．\u003c/p\u003e","title":"【入門3】YOLOアノテーションと学習"},{"content":"はじめに 自動採譜プロジェクトの息抜き的な記事です．\nせっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして， ところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．\nカメラ切り替えトリガー カメラの切り替えトリガーを「牌山にプレーヤーが触れたタイミング」と設定しました．\n自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．\nツモ位置の検出 まず，天カメの映像から卓上のツモ位置を特定する必要があります．\n牌山の形を見てみましょう．\n対局開始前はこんな感じ．\n対局開始直後はこんな感じ．\n対局中はこんな感じ．\n対局開始前は4つ牌山がありますが，対局開始以降は牌山が必ず3つ以下になります王牌の扱い上，4つのときもあります．\n牌山が3つのとき\n牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．\n牌山が4つのとき\n一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．\n実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．\n牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．\n実装概要 元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます． cv2.inRange()を使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．\n二値化した画像に対し，cv2.findContours()を使用し，牌山となりうる矩形を検出します．\n手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます． その後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．\nツモプレイヤーの検出 ツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します． MediaPipe のハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します． 今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．\nプレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．\nオートスイッチャー実装！！！ OBSを使用して配信するため，カメラ切り替えには obs-websocket-py を使用しました．こちらの サンプル がそのまま動作して助かりました．\nGUI実装にはHTML/JSでの記述が可能な Eel を使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．\n実際の動作の様子がこちら．\nYour browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．\nあと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．\n","permalink":"http://localhost:1313/blog/posts/20240210_mahjongautocamswitcher/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e自動採譜プロジェクト\u003c/a\u003eの息抜き的な記事です．\u003c/p\u003e\n\u003cp\u003eせっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして，\nところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．\u003c/p\u003e\n\u003ch2 id=\"カメラ切り替えトリガー\"\u003eカメラ切り替えトリガー\u003c/h2\u003e\n\u003cp\u003eカメラの切り替えトリガーを「\u003cstrong\u003e牌山にプレーヤーが触れたタイミング\u003c/strong\u003e」と設定しました．\u003c/p\u003e\n\u003cp\u003e自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．\u003c/p\u003e\n\u003ch2 id=\"ツモ位置の検出\"\u003eツモ位置の検出\u003c/h2\u003e\n\u003cp\u003eまず，天カメの映像から卓上のツモ位置を特定する必要があります．\u003cbr\u003e牌山の形を見てみましょう．\u003c/p\u003e\n\u003cp\u003e対局開始前はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_0.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局開始直後はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局中はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_2.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局開始前は4つ牌山がありますが，対局開始以降は牌山が\u003cdel\u003e必ず3つ以下になります\u003c/del\u003e王牌の扱い上，4つのときもあります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e牌山が3つのとき\u003c/p\u003e\n\u003cp\u003e牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e牌山が4つのとき\u003c/p\u003e\n\u003cp\u003e一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"fig1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．\u003c/p\u003e\n\u003cp\u003e牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．\u003c/p\u003e\n\u003ch3 id=\"実装概要\"\u003e実装概要\u003c/h3\u003e\n\u003cp\u003e元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます．\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"original_image.png\"/\u003e \r\n\u003c/figure\u003e\r\n\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecv2.inRange()\u003c/code\u003eを使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"mask_hai.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e二値化した画像に対し，\u003ccode\u003ecv2.findContours()\u003c/code\u003eを使用し，牌山となりうる矩形を検出します．\u003c/p\u003e\n\u003cp\u003e手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます．\nその後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumohai.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"ツモプレイヤーの検出\"\u003eツモプレイヤーの検出\u003c/h2\u003e\n\u003cp\u003eツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します．\n\u003ca href=\"https://developers.google.com/mediapipe/solutions/vision/hand_landmarker\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eMediaPipe\u003c/a\u003e\nのハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します．\n今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumo1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eプレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumo2.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"オートスイッチャー実装\"\u003eオートスイッチャー実装！！！\u003c/h2\u003e\n\u003cp\u003eOBSを使用して配信するため，カメラ切り替えには\n\u003ca href=\"https://github.com/Elektordi/obs-websocket-py/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eobs-websocket-py\u003c/a\u003e\nを使用しました．こちらの\n\u003ca href=\"https://github.com/Elektordi/obs-websocket-py/blob/master/samples/switch_scenes.py\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eサンプル\u003c/a\u003e\nがそのまま動作して助かりました．\u003c/p\u003e\n\u003cp\u003eGUI実装にはHTML/JSでの記述が可能な\n\u003ca href=\"https://github.com/python-eel/Eel\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eEel\u003c/a\u003e\nを使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．\u003c/p\u003e\n\u003cp\u003e実際の動作の様子がこちら．\u003c/p\u003e\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\n\u003cp\u003e現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．\u003c/p\u003e\n\u003cp\u003eあと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．\u003c/p\u003e","title":"麻雀放送対局用のオートスイッチャーを作りたい"},{"content":"はじめに 複数カメラの入力に対しYOLOv8で並列推論するのソースコード解説です． 解説のため，一部表現を変えている部分があります．\n大まかな流れは以下の通り．\n共有メモリ確保 フレーム入力プロセス カメラからフレームを取得 フレームを共有メモリ1に保存 推論プロセス 共有フレームを呼び出し 推論 推論結果を描画したフレームを共有メモリ2に保存 フレーム表示プロセス 共有フレームを呼び出し フレームの画面描画 プロセス間の遷移はmultiprocessingモジュールのEvent関数をフラグとして利用します． カメラの台数が増えた場合はプロセス数を増やせば対応できます．\n事前準備 設定情報 cam_N = 4 # カメラ台数 proccess_N = 3 # カメラ1台あたりのプロセス数 frame_info = {\u0026#34;nbytes\u0026#34; : frame.nbytes, \u0026#34;shape\u0026#34; : frame.shape, \u0026#34;dtype\u0026#34; : frame.dtype} フレームのサイズや型はメモリ確保，呼び出し時に必要になります．\n共有メモリの確保 shm = shared_memory.SharedMemory(create=True, size=frame.nbytes, name = mem_name) 共有メモリの確保はこの一文でできます．めっちゃ便利ですね． ただし，mem_nameを動的に作成して複数の共有メモリを確保する場合，注意事項があります． shared_memory.SharedMemoryの戻り値を格納する変数（上記コードではshm）を設定しなかった場合や上書きした場合，それ以前の共有メモリ領域は参照できなくなります．その場合はshmを格納するリストを用意しておくとアクセス可能になります．\nプロセスの立ち上げ p = Process(target=関数名, args=(引数,)) p.start() # プロセス開始 p.join() # プロセス終了 複数プロセスを立ち上げる場合は p をリストに入れておくと管理が楽です．\nフレーム入力プロセス def rec_cam(cam_id, pre_flags, frame_info): cap = cv2.VideoCapture(cam_id) while True: _, frame = cap.read() idx = (cam_id)*proccess_N+counter%proccess_N mem_name = f\u0026#39;shared{idx+1:02}\u0026#39; shm = shared_memory.SharedMemory(name=mem_name) frame_sh = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) frame_sh[:] = frame[:] # 推論開始 pre_flags[idx].set() cap.close() multiprocessingのSharedMemoryでは共有メモリを名前をつけて呼び出すことができます．便利！ ここでは，\u0026ldquo;shared1[番号]\u0026ldquo;という名前で共有メモリを確保している（後述）．カメラ台数✕カメラあたりのプロセス数分の共有メモリ領域が必要になるため， あるカメラのあるフレームに対する保存領域は，カメラ番号✕カメラ1台あたりのプロセス数＋フレーム番号%カメラ1台あたりのプロセス数で求めることができる．と同時にこの番号は推論プロセスを動作させるフラグ管理のインデックスとしても使う．\n推論プロセス def predict_frame(pre_flag, view_flag, mem_name, cam_id, frame_info): while True: if pre_flag.is_set(): # 推論開始 pre_flag.clear() shm = shared_memory.SharedMemory(name=mem_name) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # 重い推論処理がここに入る（frameに結果を反映する） # 結果統合用共有メモリにのせる shm_result = shared_memory.SharedMemory(name=f\u0026#39;shared2{cam_id}\u0026#39;) frame_new = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm_result.buf) frame_new[:]=frame[:] # 結果表示 view_flag.set() カメラ台数✕カメラあたりのプロセス数分の推論プロセスを立ち上げます． 推論開始用のフラグや共有メモリの呼び出し名は予め与えた状態でプロセスを立ち上げる．フレーム入力プロセスでの推論開始用フラグの有効化を待ち受けています． 処理済みのフレーを格納する共有メモリ領域名は\u0026rsquo;shared2[番号]\u0026lsquo;としており，カメラの台数分事前に確保してあります． 検出結果を描画する必要がない場合は，フレーム表示プロセスを介する必要がないため，別プロセスに推論結果を渡したり，UDP通信などで端末間の結果送信が実装できると思います．\nフレーム表示プロセス def view_frame(view_flag, cam_id, frame_info): while True: if view_flag.is_set(): view_flag.clear() shm = shared_memory.SharedMemory(name=f\u0026#39;shared2{cam_id}\u0026#39;) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) cv2.imshow(f\u0026#39;window_{cam_id}\u0026#39;, frame) cv2.waitKey(1) 推論プロセスでのフレーム表示用フラグの有効化を待ち受けています．\nおわりに 今回は，以前実装した並列画像処理プログラムの解説でした．コードブロックを断片的に示しただけなので，具体的な実装コードを知りたい方はソースコードから参照できます．\n","permalink":"http://localhost:1313/blog/posts/20231226_multiprocessingforrealtimevideo/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/2023_12_01_multiprocessingforyolov8/\" target=\"_blank\"\u003e複数カメラの入力に対しYOLOv8で並列推論する\u003c/a\u003eのソースコード解説です．\n解説のため，一部表現を変えている部分があります．\u003c/p\u003e\n\u003cp\u003e大まかな流れは以下の通り．\u003c/p\u003e\n\u003col start=\"0\"\u003e\n\u003cli\u003e共有メモリ確保\u003c/li\u003e\n\u003cli\u003eフレーム入力プロセス\n\u003col\u003e\n\u003cli\u003eカメラからフレームを取得\u003c/li\u003e\n\u003cli\u003eフレームを共有メモリ1に保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e推論プロセス\n\u003col\u003e\n\u003cli\u003e共有フレームを呼び出し\u003c/li\u003e\n\u003cli\u003e推論\u003c/li\u003e\n\u003cli\u003e推論結果を描画したフレームを共有メモリ2に保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eフレーム表示プロセス\n\u003col\u003e\n\u003cli\u003e共有フレームを呼び出し\u003c/li\u003e\n\u003cli\u003eフレームの画面描画\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eプロセス間の遷移はmultiprocessingモジュールのEvent関数をフラグとして利用します．\nカメラの台数が増えた場合はプロセス数を増やせば対応できます．\u003c/p\u003e\n\u003ch2 id=\"事前準備\"\u003e事前準備\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e設定情報\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecam_N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# カメラ台数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eproccess_N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# カメラ1台あたりのプロセス数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eframe_info \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;nbytes\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enbytes,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;shape\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshape,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dtype\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edtype}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eフレームのサイズや型はメモリ確保，呼び出し時に必要になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e共有メモリの確保\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eshm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e shared_memory\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSharedMemory(create\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enbytes, name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e mem_name)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e共有メモリの確保はこの一文でできます．めっちゃ便利ですね．\nただし，mem_nameを動的に作成して複数の共有メモリを確保する場合，注意事項があります．\nshared_memory.SharedMemoryの戻り値を格納する変数（上記コードではshm）を設定しなかった場合や上書きした場合，それ以前の共有メモリ領域は参照できなくなります．その場合はshmを格納するリストを用意しておくとアクセス可能になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプロセスの立ち上げ\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Process(target\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e関数名, args\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e(引数,))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estart() \u003cspan style=\"color:#75715e\"\u003e# プロセス開始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ejoin() \u003cspan style=\"color:#75715e\"\u003e# プロセス終了\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e複数プロセスを立ち上げる場合は p をリストに入れておくと管理が楽です．\u003c/p\u003e\n\u003ch2 id=\"フレーム入力プロセス\"\u003eフレーム入力プロセス\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erec_cam\u003c/span\u003e(cam_id, pre_flags, frame_info):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cv2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eVideoCapture(cam_id)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            _, frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            idx \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (cam_id)\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003eproccess_N\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003ecounter\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003eproccess_N\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            mem_name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;shared\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003eidx\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e02\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            shm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e shared_memory\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSharedMemory(name\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003emem_name)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            frame_sh \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003endarray(shape\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;shape\u0026#34;\u003c/span\u003e], dtype\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dtype\u0026#34;\u003c/span\u003e], buffer\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eshm\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebuf)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            frame_sh[:] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e frame[:]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# 推論開始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            pre_flags[idx]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eset()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003emultiprocessingのSharedMemoryでは共有メモリを名前をつけて呼び出すことができます．便利！\nここでは，\u0026ldquo;shared1[番号]\u0026ldquo;という名前で共有メモリを確保している（後述）．カメラ台数✕カメラあたりのプロセス数分の共有メモリ領域が必要になるため，\nあるカメラのあるフレームに対する保存領域は，カメラ番号✕カメラ1台あたりのプロセス数＋フレーム番号%カメラ1台あたりのプロセス数で求めることができる．と同時にこの番号は推論プロセスを動作させるフラグ管理のインデックスとしても使う．\u003c/p\u003e","title":"リアルタイムの映像入力に対してFPSを落とさずに画像処理したい"},{"content":"ビデオカメラからOpenCVで映像取得（低品質） 麻雀自動採譜の実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．\nビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できるビデオキャプチャカードを導入． これでビデオカメラをWebカメラみたく使えるぞ！やった！！\nと思った矢先，\nなんか遅い気がする！！\nよくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，1秒遅延＋低FPS＋低画質という散々な結果に\nお前 Full HD で 30FPS 出るって言ってたじゃないか\u0026hellip;\u0026hellip;\nキャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．\nRTSP通信でカメラの映像を受信してみる\nこのときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった． 今回もPyAVを使えば解決するかもしれない．\nビデオカメラからPyAVで映像取得 PyAVでPCに有線接続されたカメラから映像を取得する．\nimport cv2 import av con_options = dict( video_size=\u0026#39;1920x1080\u0026#39;, vcodec=\u0026#39;mjpeg\u0026#39;, framerate=\u0026#39;30\u0026#39;, rtbufsize=\u0026#39;1\u0026#39;, ) device_name = \u0026#34;USB Video\u0026#34; con_def = dict( format=\u0026#39;dshow\u0026#39;, file=f\u0026#39;video={device_name}\u0026#39;, options=dict(con_options, video_device_number=\u0026#39;0\u0026#39;) ) container = av.open(**con_def) for frame in container.decode(video=0): frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) # cv2.imshow(\u0026#34;test_window\u0026#34;, frame) # cv2.waitKey(1) ちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．\nffmpeg -list_devices true -f dshow -i dummy PyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．\nこれで遅延ほぼなし，30FPSでの映像受信ができた．\n","permalink":"http://localhost:1313/blog/posts/20231225_getframebypyav/","summary":"\u003ch2 id=\"ビデオカメラからopencvで映像取得低品質\"\u003eビデオカメラからOpenCVで映像取得（低品質）\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003eの実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．\u003c/p\u003e\n\u003cp\u003eビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できる\u003ca href=\"https://amzn.asia/d/jclJASJ\"\u003eビデオキャプチャカード\u003c/a\u003eを導入．\nこれでビデオカメラをWebカメラみたく使えるぞ！やった！！\u003c/p\u003e\n\u003cp\u003eと思った矢先，\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eなんか遅い気がする！！\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eよくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，\u003cstrong\u003e1秒遅延＋低FPS＋低画質\u003c/strong\u003eという散々な結果に\u003c/p\u003e\n\u003cp\u003eお前 Full HD で 30FPS 出るって言ってたじゃないか\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003eキャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/blog/2023_12_01_getframertsp/\"\u003eRTSP通信でカメラの映像を受信してみる\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこのときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった．\n今回も\u003ca href=\"https://pyav.org/docs/develop/index.html\" target=\"_blank\"\u003ePyAV\u003c/a\u003eを使えば解決するかもしれない．\u003c/p\u003e\n\u003ch2 id=\"ビデオカメラからpyavで映像取得\"\u003eビデオカメラからPyAVで映像取得\u003c/h2\u003e\n\u003cp\u003ePyAVでPCに有線接続されたカメラから映像を取得する．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e cv2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e av\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econ_options  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  dict(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    video_size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1920x1080\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    vcodec\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;mjpeg\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    framerate\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;30\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    rtbufsize\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edevice_name  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;USB Video\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econ_def  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  dict(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;dshow\u0026#39;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    file\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;video=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003edevice_name\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    options\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003edict(con_options, video_device_number\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;0\u0026#39;\u003c/span\u003e) )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econtainer \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e av\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eopen(\u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003econ_def)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e frame \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e container\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edecode(video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto_ndarray(format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;bgr24\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# cv2.imshow(\u0026#34;test_window\u0026#34;, frame)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# cv2.waitKey(1)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-console\" data-lang=\"console\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003effmpeg -list_devices true -f dshow -i dummy\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．\u003c/p\u003e","title":"映像入力の遅延をPyAVで解決してみる"},{"content":" multiprocessing × YOLOv8の詳細解説 Denoを利用した基本的なWebサイト作成 Deno Deploy設定 ","permalink":"http://localhost:1313/blog/posts/20231212_scheduled_update/","summary":"\u003cul\u003e\n\u003cli\u003emultiprocessing × YOLOv8の詳細解説\u003c/li\u003e\n\u003cli\u003eDenoを利用した基本的なWebサイト作成\u003c/li\u003e\n\u003cli\u003eDeno Deploy設定\u003c/li\u003e\n\u003c/ul\u003e","title":"今後の更新予定"},{"content":"RTSPでフレーム取得（OpenCV） RTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．\nimport cv2 url = \u0026#34;rtsp://username:password@ipaddress\u0026#34; # Tapo C200は末尾に画質を指定 url = \u0026#34;rtsp://username:password@ipaddress/stream1\u0026#34; cap = cv2.VideoCapture(url) while True: ret, frame = cap.read() pass ただこれだとTapo C200では1秒程度遅延が発生してしまう． 麻雀自動採譜 の実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．\nRTSPでフレーム取得（PyAV） RTSP映像の受信にはFFmpegのPythonバインディングである PyAV を利用した方法もあり、以下で映像受信が可能でした．\nimport av\rurl = \u0026#34;rtsp://username:password@ipaddress\u0026#34;\rcontainer = av.open(url)\rfor frame in container.decode(video=0):\rframe = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;)\rpass PyAVを利用した場合は遅延が大幅に減少した．やったね．\nRTSPで一定時間毎にフレーム取得（PyAV） 麻雀自動採譜 牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚でtime.sleep(n)を使用したところ以下のエラーが複数出現．\nmax delay reached. need to consume packet\rRTP: missed 6066 packets\rRTP: PT=60: bad cseq a657 expected=8ea5\rmax delay reached. need to consume packet sleepで無理に処理を中断させたため接続が不安定になったのかも． 通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．\nimport av interval = 10 # 10秒ごとに撮影 url = \u0026#34;rtsp://username:password@ipaddress\u0026#34; container = av.open(url) counter = 0 for frame in container.decode(video=0): if frame is not None and frame.time//interval\u0026gt;counter: counter+=1 frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) cv2.imwrite(f\u0026#39;save_{counter:04}.png\u0026#39;, frame) PyAVのcontainerから呼び出したフレームは撮影開始を0としたときの撮影時刻をtimeで取得できる． 撮影開始後数フレームはNoneが返ることがあるため注意． これで安定したn秒毎の撮影が実行できる．\n","permalink":"http://localhost:1313/blog/posts/20231201_getframertsp/","summary":"\u003ch2 id=\"rtspでフレーム取得opencv\"\u003eRTSPでフレーム取得（OpenCV）\u003c/h2\u003e\n\u003cp\u003eRTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e cv2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rtsp://username:password@ipaddress\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Tapo C200は末尾に画質を指定\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rtsp://username:password@ipaddress/stream1\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cv2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eVideoCapture(url)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ret, frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eただこれだとTapo C200では\u003cstrong\u003e1秒程度遅延\u003c/strong\u003eが発生してしまう．\n\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003e\nの実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．\u003c/p\u003e\n\u003ch2 id=\"rtspでフレーム取得pyav\"\u003eRTSPでフレーム取得（PyAV）\u003c/h2\u003e\n\u003cp\u003eRTSP映像の受信にはFFmpegのPythonバインディングである\n\u003ca href=\"https://pyav.org/docs/develop/index.html\" target=\"_blank\"\u003ePyAV\u003c/a\u003e\nを利用した方法もあり、以下で映像受信が可能でした．\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-python:\" data-lang=\"python:\"\u003eimport av\r\n\r\nurl = \u0026#34;rtsp://username:password@ipaddress\u0026#34;\r\ncontainer = av.open(url)\r\nfor frame in container.decode(video=0):\r\n    frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;)\r\n    pass\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ePyAVを利用した場合は遅延が大幅に減少した．やったね．\u003c/p\u003e\n\u003ch2 id=\"rtspで一定時間毎にフレーム取得pyav\"\u003eRTSPで一定時間毎にフレーム取得（PyAV）\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003e\n牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚で\u003ccode\u003etime.sleep(n)\u003c/code\u003eを使用したところ以下のエラーが複数出現．\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-raw:\" data-lang=\"raw:\"\u003emax delay reached. need to consume packet\r\nRTP: missed 6066 packets\r\nRTP: PT=60: bad cseq a657 expected=8ea5\r\nmax delay reached. need to consume packet\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003esleepで無理に処理を中断させたため接続が不安定になったのかも．\n通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．\u003c/p\u003e","title":"RTSP通信でカメラの映像を受信してみる"},{"content":"実装過程あれこれ１ これまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の Tapo C200 からフレームを受信することができるようになりました． 早速各カメラに割り当てたプロセス毎に適当に学習させておいた YOLOv8 による牌推論を実行し，結果を描画してみました．わくわく！\nはい，映像出力が不穏な感じに． Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．\nプロセス間でまったく映像出力のタイミングがあっていない フレーム間隔がまちまち 映像の乱れが酷い FPSが低い（Tapo入力時15→10弱） 特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね\u0026hellip;\u0026hellip;\nフレーム出力がばらつく理由を勝手に予想．こんな感じだろうか\n出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える． こんなときこそマルチプロセス処理の出番でしょ↓\n（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）\n既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．\n映像受信プロセス ×4（カメラの台数） 推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当） 推論結果統合プロセス ×4（カメラの台数） さあやるぞ！\n実装過程あれこれ２ ここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．\nbase64→内部UDPが一瞬頭を過るが無視して 公式ドキュメント を漁ってみます．\nドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか． Value, Array, Queue, Pipe, RawArray, Manager, shared_memory\u0026hellip;\u0026hellip;\n多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．\n試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに． フラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．\nそんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります． Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r概ね同じタイミングでフレームが描画されているように見えますね．\nFPSはいずれのカメラも15程度であり，入力時のFPSを保持したままリアルタイムで推論が実行できました． ちなみに，\nmultiprocessingではなくthreadingを使ってYOLOv8のモデルをスレッド間で共有する方法があるが，提供元である UltralyticsのMedium記事 では推奨方法， 公式ドキュメント では非推奨方法として紹介されていたため今回は見送り． 試験的に使用してるYOLOv8モデルは，現環境（GeForce RTX 3070）で学習可能な最大サイズのモデル（YOLOv8l）に対して複数の公開データセットを統合した5万枚強の牌画像を学習させたものである．統合作業時にデータセット間でラベルのズレが発生したため，検出結果に誤りがある．ラベルが混合した状態のデータセットではあるが安定した検出ができているため，データセットの整理によって更に高精度な検出の実現が期待できる． 背景部分の予期せぬ牌検出は信頼度で棄却できるため問題なし． 検証動画は双方画面録画の仕様上FPSが低下している． ソースコード 複数台のカメラのフレーム入力に対してYOLOv8による推論を並列実行するPythonプログラム ソースコード解説はこちらからどうぞ．\nimport time import msvcrt from multiprocessing import Process, Event, shared_memory import numpy as np import av import cv2 from ultralytics import YOLO # RTSPカメラ設定情報 info_list = [[\u0026#34;toncamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;nancamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;shacamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;peicamera\u0026#34;, \u0026#34;*\u0026#34;]] password = \u0026#34;*\u0026#34; # 1:1280x720, 2:640x360 mode = 1 # 推論プロセス数指定 N = 3 def rec_cam(cam_id, username, ipaddress, pre_flags, stop_flag, frame_info): url = f\u0026#34;rtsp://{username}:{password}@{ipaddress}:554/stream{mode}\u0026#34; container = av.open(url) counter = 0 start = time.time() for frame in container.decode(video=0): counter+=1 frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) # 共有メモリにフレームを書き込み idx = (cam_id)*N+counter%N mem_name = f\u0026#39;shared{idx+1:02}\u0026#39; shm = shared_memory.SharedMemory(name=mem_name) frame_sh = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) frame_sh[:] = frame[:] # 推論開始 pre_flags[idx].set() if stop_flag.is_set(): t = time.time() - start print(f\u0026#34;{username}, TIME:{round(t, 3)}, FRAME:{counter}, FPS:{round(counter/t, 3)}\u0026#34;) break container.close() cv2.destroyAllWindows() return def finish_monitor(stop_flag): while True: if msvcrt.kbhit() and msvcrt.getch() == b\u0026#39;q\u0026#39;: print(\u0026#34;Finished\u0026#34;) stop_flag.set() break return def predict_frame(pre_flag, pre_fin_flag, mem_name, stop_flag, username, frame_info): model = YOLO(\u0026#39;best.pt\u0026#39;) while True: if pre_flag.is_set(): # 推論 pre_flag.clear() shm = shared_memory.SharedMemory(name=mem_name) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # YOLOv8による推論実行 result = model(frame)[0] frame = result.plot() # 結果統合用共有メモリにのせる shm_result = shared_memory.SharedMemory(name=username) frame_new = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm_result.buf) frame_new[:]=frame[:] # 結果統合用 pre_fin_flag.set() if stop_flag.is_set(): break def view_frame(pre_fin_flag, username, stop_flag, frame_info): while True: if pre_fin_flag.is_set(): pre_fin_flag.clear() shm = shared_memory.SharedMemory(name=username) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # Full-HDだと大抵画面が埋まっちゃう # frame = cv2.resize(frame, None, None, 0.6, 0.6) cv2.imshow(username, frame) cv2.waitKey(1) if stop_flag.is_set(): break if __name__ == \u0026#39;__main__\u0026#39;: # メモリ確保情報 url = f\u0026#34;rtsp://{info_list[0][0]}:{password}@{info_list[0][1]}:554/stream{mode}\u0026#34; container = av.open(url) for i, frame in enumerate(container.decode(video=0)): frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) if frame is not None: break container.close() frame_info = {\u0026#34;nbytes\u0026#34; : frame.nbytes, \u0026#34;shape\u0026#34; : frame.shape, \u0026#34;dtype\u0026#34; : frame.dtype} # 停止プロセス stop_flag = Event() p = Process(target=finish_monitor, args=(stop_flag,)) p.start() # 表示プロセス群（カメラ台数分） view_processes = [] pre_fin_flags = [] mem_space = [] for i in range(4): pre_fin_flag = Event() pre_fin_flags.append(pre_fin_flag) username = info_list[i][0] # メモリ確保 今回は画像用 shm = shared_memory.SharedMemory(create=True, size=frame_info[\u0026#34;nbytes\u0026#34;], name = username) mem_space.append(shm) p = Process(target=view_frame, args=(pre_fin_flag, username, stop_flag, frame_info)) view_processes.append(p) p.start() # 推論プロセス群（カメラ台数×推論プロセス数） pre_processes = [] pre_flags = [] mem_names = [] for i in range(N*len(info_list)): pre_flag = Event() pre_flags.append(pre_flag) mem_name = f\u0026#39;shared{i+1:02}\u0026#39; mem_names.append(mem_name) # メモリ確保 shm = shared_memory.SharedMemory(create=True, size=frame_info[\u0026#34;nbytes\u0026#34;], name = mem_name) mem_space.append(shm) # frame_sh = np.ndarray(shape=frame_shape, dtype=frame_dtype, buffer=shm.buf) username = info_list[i//N][0] pre_fin_flag = pre_fin_flags[i//N] p = Process(target=predict_frame, args=(pre_flag, pre_fin_flag, mem_name, stop_flag, username, frame_info)) pre_processes.append(p) p.start() # 撮影プロセス群（カメラ台数） rec_processes = [] for cam_id, info in enumerate(info_list): p = Process(target=rec_cam, args=(cam_id, info[0], info[1], pre_flags, stop_flag, frame_info,)) rec_processes.append(p) p.start() p.join() for p in view_processes: p.join() for p in pre_processes: p.join() for p in rec_processes: p.join() # 共有メモリのリソース開放、削除 for mem_name in mem_names + [info[0] for info in info_list[:4]]: shm = shared_memory.SharedMemory(name = mem_name) shm.close() shm.unlink() ","permalink":"http://localhost:1313/blog/posts/20231201_multiprocessingforyolov8/","summary":"\u003ch2 id=\"実装過程あれこれ１\"\u003e実装過程あれこれ１\u003c/h2\u003e\n\u003cp\u003eこれまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の\n\u003ca href=\"https://www.tp-link.com/jp/smart-home/tapo/tapo-c200/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eTapo C200\u003c/a\u003e\nからフレームを受信することができるようになりました．\n\u003cbr\u003e\n早速各カメラに割り当てたプロセス毎に適当に学習させておいた\n\u003ca href=\"https://docs.ultralytics.com/ja/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eYOLOv8\u003c/a\u003e\nによる牌推論を実行し，結果を描画してみました．わくわく！\u003c/p\u003e\n\u003cp\u003eはい，映像出力が不穏な感じに．\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20231201_multiprocessingforyolov8/before.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20231201_multiprocessingforyolov8/before.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\u003c/p\u003e\n\u003cp\u003e卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプロセス間でまったく映像出力のタイミングがあっていない\u003c/li\u003e\n\u003cli\u003eフレーム間隔がまちまち\u003c/li\u003e\n\u003cli\u003e映像の乱れが酷い\u003c/li\u003e\n\u003cli\u003eFPSが低い（Tapo入力時15→10弱）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね\u0026hellip;\u0026hellip;\u003cbr\u003e\u003c/p\u003e\n\u003cp\u003eフレーム出力がばらつく理由を勝手に予想．こんな感じだろうか\u003c/p\u003e\n\u003cfigure\u003e\r\n    \u003cimg loading=\"lazy\" src=\"multiprocess_fig1.jpg\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える．\n\u003cbr\u003e\u003cbr\u003e\n\u003cspan style=\"font-size: 200%; color: red;\"\u003e\nこんなときこそマルチプロセス処理の出番でしょ↓\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\r\n    \u003cimg loading=\"lazy\" src=\"multiprocess_fig2.jpg\"/\u003e \r\n\u003c/figure\u003e\r\n\n（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）\u003c/p\u003e\n\u003cp\u003e既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e映像受信プロセス ×4（カメラの台数）\u003c/li\u003e\n\u003cli\u003e推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当）\u003c/li\u003e\n\u003cli\u003e推論結果統合プロセス ×4（カメラの台数）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eさあやるぞ！\u003c/p\u003e\n\u003ch2 id=\"実装過程あれこれ２\"\u003e実装過程あれこれ２\u003c/h2\u003e\n\u003cp\u003eここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．\u003cbr\u003e\nbase64→内部UDPが一瞬頭を過るが無視して\n\u003ca href=\"https://docs.python.org/ja/3.10/library/multiprocessing.html\" target=\"_blank\"\u003e公式ドキュメント\u003c/a\u003e\nを漁ってみます．\u003c/p\u003e\n\u003cp\u003eドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか．\nValue, Array, Queue, Pipe, RawArray, Manager, shared_memory\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．\u003c/p\u003e\n\u003cp\u003e試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに．\nフラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．\u003c/p\u003e\n\u003cp\u003eそんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります．\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20231201_multiprocessingforyolov8/after.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20231201_multiprocessingforyolov8/after.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\u003c/p\u003e","title":"複数カメラの入力に対しYOLOv8で並列推論する"},{"content":"ライブラリとは？ ライブラリとは，再利用可能なソフトウェア部品のことで，特定の機能をまとめたコードの再集合のことを指します．簡単に言えば，他の人が作ってくれた機能を自身のプログラムで使える便利なコード集のことです． Pythonには標準ライブラリと外部ライブラリの2種類があります．\n標準ライブラリ 標準ライブラリはPythonをインストールしたときに標準的に入っているライブラリのことです．通常のプログラム作成時に頻繁に必要となる機能をまとめたものが多くあります．例えば，\n乱数を作成する random 時刻を取得できる datetime, time ファイルシステムへのアクセスを実現する os, shutil 基礎的な数学関数 math HTMLサポート html URLを使ってインターネットにアクセスできる urllib スマートフォンで言うところのカメラ，アルバム，時計などのデフォルトアプリみたいなものです．これらはPythonが動く環境であれば勝手に入っているため，import ライブラリ名と記述すればすぐに利用することができます．助かる！！\n# サイコロの出目を出力 import random dice_roll = random.randint(1, 6) print(\u0026#34;サイコロの出目:\u0026#34;, dice_roll) 外部ライブラリ 外部ライブラリはPythonにはもともと組み込まれておらず，自身で導入するタイプのライブラリのことです．実装したいプログラムの機能に応じて適切に選択する必要があります．例えば，\n効率的な多次元配列の数値計算を実現する Numpy 画像処理の機能を提供する OpenCV グラフを簡単に描画できる Matplotlib 様々なアルゴリズムでの機械学習を可能とする scikit-learn Webアプリの開発に役立つ Django スマートフォンで言うところのゲームやチャットアプリなどのストアアプリみたいなものです．これらは自身でインストール作業をする必要がありますが，それさえ済ませば高機能な実装をいとも容易く実現できます！とても便利！！ 有志のプログラマが開発して公開してくれているため，感謝の気持ちを忘れずに使いましょう（Pythonもそうだけど）．\n# サイコロの出目を出力 import numpy as np dice_roll = np.random.randint(1, 7) print(\u0026#34;サイコロの出目:\u0026#34;, dice_roll) ライブラリを確認してみよう では，ここからは実際に手を動かしながら解説を進めます． まずはスタートメニューから Anaconda Prompt を開いてください．\nこれはコマンドプロンプトと呼ばれる画面で，ここにコマンドと呼ばれる命令文を入力することでコンピュータを操作できます． 試しに，ここにtreeと打ち込んでEnterキーを押してみてください．\n\u0026hellip;\u0026hellip;何かツリー構造の記述がたくさん出てきたかと思います．これはディレクトリ構造をツリー形式で表示するコマンドです（あんまり使わないけど）！ なかなか表示が終わらない場合はCtrl+Cキーでキャンセルしてください．\nではコマンドを使って，今使っているPythonで利用可能な外部ライブラリの一覧を見てみましょう！ Anaconda Promptにconda listと打ち込んでEnterキーを押してみてください．\nインストール済みライブラリとバージョンの一覧が表示されました．既にたくさん入っているようです！\nよくよく見てみると，数値計算ができるNumpyや表データを扱えるPandasなど，データサイエンスに役立つライブラリがたくさんあります． 実はAnacondaは科学計算のためのPython配布形式（ディストリビューション）の一つであり，既に役立つライブラリが揃えられているんです．\nこの状態，一見プログラムを書くのに便利な環境であるように見えるんですが実はそうでないケースがあります．\n実はライブラリは，機能を実現するために別のライブラリを参照するケースがあります．例えば，\n画像処理を実現するOpenCVには，画像を配列として扱うためにNumpyが必要 グラフを描画するMatplotlibには，画像描画のためにPillowが必要 機械学習を実現するscikit-learnには，高度な数値計算をするためにScipyが必要 またライブラリはバージョンによって提供される機能が変化することがあります．そのため，同じ実行環境に特定のバージョンを指定して参照ライブラリを導入しておく必要があるのです．\nこれを，ライブラリの依存関係と言います．この依存関係が原因で複数のライブラリを導入するときに問題が発生する可能性があるんです．\n例えば，ライブラリA-C及びB-Cの依存関係があるとき，AとBが共存するためにはCのバージョンが整合している必要があります．\nこうしたライブラリバージョンの試行錯誤を行う場合，先程表示させた複雑に依存関係が絡み合ったライブラリ群を扱うのはなかなか骨が折れます．\nまた，不要なライブラリは更に混乱を引き起こすので必要となる機能のための最小構成で開発環境を整える必要があります．\nそこで役立つのが，仮想環境です！！！（やっと本題）\n仮想環境 仮想環境とは，プログラムを動かすときに仮に作って利用する動作環境のことです． 簡単に言うと「依存関係の解決が面倒だから，まっさらな世界を一から創造すればいいじゃん！！」です．\nでは，実際に作ってみましょう．\n先ほど開いたAnaconda Promptにconda create -n test python=3.12と打ち込んで実行（Proceed([y]/n)?にはyを入力して処理を続行） conda activate testと打ち込んで実行 ここでは，testという名前の仮想環境を作成してその環境に切り替える，という作業をしてもらいました． 最新行の左側が(test)と表示されていれば成功です！\n先ほどまでは(base)と表示されていたはずです．Anacondaではデフォルトでbase環境があり，そこにたくさんのライブラリがありました． ではconda listで現在のtest環境のライブラリ構成を見てみましょう！\nbase環境と比べてライブラリの数がかなり少なくなったはずです．ここに表示されているライブラリはPython3.12に付随する必要最低限のライブラリです． このきれいな環境に最小限の適切なライブラリを導入することで，快適なコーディングライフを送ることができます！！\n番外編：Conda仮想環境の管理 複数の仮想環境を扱うときに便利なコマンドをまとめておきます！\n仮想環境の作成（環境名がtest，Python3.12の場合）\nconda create -n test python=3.12 仮想環境への移行\nconda activate test 仮想環境から離脱\nconda deactivate 仮想環境の削除\nconda remove -n test --all 仮想環境一覧の表示\nconda info -e ","permalink":"http://localhost:1313/blog/posts/20250506_condavenv/","summary":"\u003ch2 id=\"ライブラリとは\"\u003eライブラリとは？\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eライブラリ\u003c/strong\u003eとは，再利用可能なソフトウェア部品のことで，特定の機能をまとめたコードの再集合のことを指します．簡単に言えば，他の人が作ってくれた機能を自身のプログラムで使える便利なコード集のことです．\nPythonには\u003cstrong\u003e標準ライブラリ\u003c/strong\u003eと\u003cstrong\u003e外部ライブラリ\u003c/strong\u003eの2種類があります．\u003c/p\u003e\n\u003ch3 id=\"標準ライブラリ\"\u003e標準ライブラリ\u003c/h3\u003e\n\u003cp\u003e標準ライブラリはPythonをインストールしたときに標準的に入っているライブラリのことです．通常のプログラム作成時に頻繁に必要となる機能をまとめたものが多くあります．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e乱数を作成する random\u003c/li\u003e\n\u003cli\u003e時刻を取得できる datetime, time\u003c/li\u003e\n\u003cli\u003eファイルシステムへのアクセスを実現する os, shutil\u003c/li\u003e\n\u003cli\u003e基礎的な数学関数 math\u003c/li\u003e\n\u003cli\u003eHTMLサポート html\u003c/li\u003e\n\u003cli\u003eURLを使ってインターネットにアクセスできる urllib\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスマートフォンで言うところのカメラ，アルバム，時計などのデフォルトアプリみたいなものです．これらはPythonが動く環境であれば勝手に入っているため，\u003ccode\u003eimport ライブラリ名\u003c/code\u003eと記述すればすぐに利用することができます．助かる！！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# サイコロの出目を出力\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e random\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edice_roll \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e random\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandint(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;サイコロの出目:\u0026#34;\u003c/span\u003e, dice_roll)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"外部ライブラリ\"\u003e外部ライブラリ\u003c/h3\u003e\n\u003cp\u003e外部ライブラリはPythonにはもともと組み込まれておらず，自身で導入するタイプのライブラリのことです．実装したいプログラムの機能に応じて適切に選択する必要があります．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e効率的な多次元配列の数値計算を実現する Numpy\u003c/li\u003e\n\u003cli\u003e画像処理の機能を提供する OpenCV\u003c/li\u003e\n\u003cli\u003eグラフを簡単に描画できる Matplotlib\u003c/li\u003e\n\u003cli\u003e様々なアルゴリズムでの機械学習を可能とする scikit-learn\u003c/li\u003e\n\u003cli\u003eWebアプリの開発に役立つ Django\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスマートフォンで言うところのゲームやチャットアプリなどのストアアプリみたいなものです．これらは自身でインストール作業をする必要がありますが，それさえ済ませば高機能な実装をいとも容易く実現できます！とても便利！！\n有志のプログラマが開発して公開してくれているため，感謝の気持ちを忘れずに使いましょう（Pythonもそうだけど）．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# サイコロの出目を出力\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e np\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edice_roll \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandom\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandint(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;サイコロの出目:\u0026#34;\u003c/span\u003e, dice_roll)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"ライブラリを確認してみよう\"\u003eライブラリを確認してみよう\u003c/h2\u003e\n\u003cp\u003eでは，ここからは実際に手を動かしながら解説を進めます．\nまずはスタートメニューから \u003cstrong\u003eAnaconda Prompt\u003c/strong\u003e を開いてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"img_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"console_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこれは\u003cstrong\u003eコマンドプロンプト\u003c/strong\u003eと呼ばれる画面で，ここに\u003cstrong\u003eコマンド\u003c/strong\u003eと呼ばれる命令文を入力することでコンピュータを操作できます．\n試しに，ここに\u003ccode\u003etree\u003c/code\u003eと打ち込んでEnterキーを押してみてください．\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;何かツリー構造の記述がたくさん出てきたかと思います．これはディレクトリ構造をツリー形式で表示するコマンドです（あんまり使わないけど）！\nなかなか表示が終わらない場合はCtrl+Cキーでキャンセルしてください．\u003c/p\u003e\n\u003cp\u003eではコマンドを使って，今使っているPythonで利用可能な外部ライブラリの一覧を見てみましょう！\nAnaconda Promptに\u003ccode\u003econda list\u003c/code\u003eと打ち込んでEnterキーを押してみてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"modules.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eインストール済みライブラリとバージョンの一覧が表示されました．既にたくさん入っているようです！\u003c/p\u003e\n\u003cp\u003eよくよく見てみると，数値計算ができるNumpyや表データを扱えるPandasなど，データサイエンスに役立つライブラリがたくさんあります．\n実はAnacondaは科学計算のためのPython配布形式（ディストリビューション）の一つであり，既に役立つライブラリが揃えられているんです．\u003c/p\u003e\n\u003cp\u003eこの状態，一見プログラムを書くのに便利な環境であるように見えるんですが実はそうでないケースがあります．\u003c/p\u003e","title":"【入門1】Python仮想環境"},{"content":"JupyterLab+Visual Studio Codeを使ったPython実行 Visual Studio CodeにJupyter拡張機能をインストール 空のファイルを作成し，拡張子を.ipynbとする ＋コードボタンからコードブロックを作成し，Pythonプログラムを記述 右上カーネルの選択から実行したいPython環境を選択し コードブロック左側の実行ボタン▷を押下 ポップアップが出てきたらインストールをクリックしてipykernelをインストール （pip install ipykernelを当該環境で実行してインストールも出来ます） YOLOのセットアップ 今回は最新のYOLOを簡単に扱うことができるUltralyticsパッケージを利用します．\nconda create --name yolo-env python=3.11を実行して仮想環境を作成 conda activate yolo-envで仮想環境を有効化 pip install ultralyticsでUltralyticsパッケージをインストール ここまで済んだら，テストコードを動かしてみましょう！\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolo11n.pt\u0026#34;) results = model(\u0026#34;https://ultralytics.com/images/bus.jpg\u0026#34;) results[0].show() バスを背景にした画像の物体検出が表示されたはずです！\n作業ディレクトリを開くと，yolo11n.ptというファイルがダウンロードされています．これが，YOLOのモデルファイルです．これはYOLOのデフォルトの訓練済みモデルで，COCO Datasetに収録された80種類の物体を識別することが出来ます．\n","permalink":"http://localhost:1313/blog/posts/20250506_yolojupyter/","summary":"\u003ch2 id=\"jupyterlabvisual-studio-codeを使ったpython実行\"\u003eJupyterLab+Visual Studio Codeを使ったPython実行\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eVisual Studio CodeにJupyter拡張機能をインストール\u003c/li\u003e\n\u003cli\u003e空のファイルを作成し，拡張子を\u003ccode\u003e.ipynb\u003c/code\u003eとする\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e＋コード\u003c/code\u003eボタンからコードブロックを作成し，Pythonプログラムを記述\u003c/li\u003e\n\u003cli\u003e右上\u003ccode\u003eカーネルの選択\u003c/code\u003eから実行したいPython環境を選択し\u003c/li\u003e\n\u003cli\u003eコードブロック左側の実行ボタン\u003ccode\u003e▷\u003c/code\u003eを押下\u003c/li\u003e\n\u003cli\u003eポップアップが出てきたら\u003ccode\u003eインストール\u003c/code\u003eをクリックして\u003ccode\u003eipykernel\u003c/code\u003eをインストール\n（\u003ccode\u003epip install ipykernel\u003c/code\u003eを当該環境で実行してインストールも出来ます）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"vsjupyter.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"yoloのセットアップ\"\u003eYOLOのセットアップ\u003c/h2\u003e\n\u003cp\u003e今回は最新のYOLOを簡単に扱うことができるUltralyticsパッケージを利用します．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003econda create --name yolo-env python=3.11\u003c/code\u003eを実行して仮想環境を作成\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate yolo-env\u003c/code\u003eで仮想環境を有効化\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install ultralytics\u003c/code\u003eでUltralyticsパッケージをインストール\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eここまで済んだら，テストコードを動かしてみましょう！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e ultralytics \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e YOLO\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emodel \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e YOLO(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;yolo11n.pt\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresults \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://ultralytics.com/images/bus.jpg\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresults[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eバスを背景にした画像の物体検出が表示されたはずです！\u003c/p\u003e\n\u003cp\u003e作業ディレクトリを開くと，\u003ccode\u003eyolo11n.pt\u003c/code\u003eというファイルがダウンロードされています．これが，YOLOのモデルファイルです．これはYOLOのデフォルトの訓練済みモデルで，COCO Datasetに収録された80種類の物体を識別することが出来ます．\u003c/p\u003e","title":"【入門2】YOLO，Jupyterのセットアップ"},{"content":"アノテーションとは？ 機械学習の分類のひとつである教師あり学習では，まず入力データと出力データの組を複数用意し，それらを使って機械学習モデルを訓練します．\n入力データと出力データの組は，タスクによって様々です．例えば，\n入力：画像データ，出力：それぞれが犬or猫 入力：家の情報（間取り，立地，築年数など），出力：家の価格 入力：これまでの気温，出力：明日の気温 入力：画像データ，出力：各物体の座標 どんなタスクを前提とする場合でも，これら入力データと出力データの組を作る作業が必要となります．これがアノテーションです．\n環境構築 今回はYOLOによる画像物体認識を目標とします．そこで，アノテーションで作るべきデータは以下になります．\n入力：画像データ，出力：各物体の座標 頑張れば用意した画像の座標を数えてアノテーションできそうですが，今回はより簡単に作業を進めるためにLabelmeというツールを使います．GUIが整備されたソフトウェアですが，Pythonライブラリとして無償で提供されています！（ありがたい！！）\nAnaconda Promptを開く conda create -n annot_env python=3.12で仮想環境を作成 conda activate annot_envで仮想環境を有効化 pip install --upgrade labelmeと入力しLabelmeをインストール これでインストールは完了です．そのままannot_env環境内でlabelmeコマンドを実行してみてください！\nこの画面が表示されたら成功です！\nいざ，アノテーション 画像読み込み 今回は画像から麻雀牌を物体認識するタスクに挑戦してみます！サンプル画像は以下からダウンロード，解凍してください．\nサンプル画像 Zipファイルの解凍が済んだらLabelmeの画面左側にあるOpen Dirから，画像のディレクトリを開いてください．\nこれでアノテーションの準備は完了です！\n範囲選択，ラベル付け アノテーションの流れは以下になります．\n画像を右クリック，Create Rectangleをクリック 物体の範囲を四角形で囲んで選択，間違えたらEscキーで戻る 出てきたポップアップの上部Enter object labelにラベルを入力してOKをクリック 2と3を物体の数だけひたすら繰り返す 画像内のアノテーションが終わったら画面左部Saveから画像と同名のJSONファイルを保存 ちょっとやってみますね～\nはい，できました．ラベルは牌の種類（萬子筒子索子字牌）の略称です．\n\u0026hellip;\u0026hellip;\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\nめんどくさい\n機械学習では一般的にデータの数が大きいほど性能が上がりやすいとされています．つまり，（要件の要求レベルにもよりますが）精度を上げるためにはたくさんアノテーションする必要があるんです．\nそこで，特定のタスクの機械学習を考える場合はアノテーションに取り掛かる前に，データセットが利用可能なライセンスで公開されていないかどうか調べるところからスタートします．例えばYOLOの場合，Roboflow，VisualData，Google Open Images Datasetなどが画像データセットを公開している他，タスクに応じて論文と併せて公開されているデータセットも多数あります．\nまた今回はアノテーション入門ということで手作業での方法を紹介していますが，画像認識AIによる半自動ツールもあるので興味があれば調べてみてください．\n今回は，なんと，特別に，アノテーション済みファイルを用意してあります！\nアノテーション結果（JSON） ZIPファイルをダウンロード＆解凍したら，datasetsという名前のディレクトリを作成し，先程の画像ファイルとJSONファイルを以下の構造で配置してください．（image_1.jsonは自作のファイルで構いません！）\n.\r└── datasets/\r├── image_1.jpg\r├── image_1.json\r├── image_2.jpg\r├── image_2.json\r├── 〜省略〜\r├── image_10.jpg\r└── image_10.json データセットフォーマットの変換 では，学習させるデータの中身を一度見てみましょう！image_1.jsonを開いてみます．\nJSONはデータ記述形式のひとつで，キーと値を並べて書くのが特徴です．Pythonの辞書型そっくりですね． ラベルや座標がファイルの中に複数記述されていることがわかります．\n少しスクロールすると，よくわからない文字列が大量に記述されています．実はこれ，Base64と呼ばれるエンコード形式で画像そのものが書きつけられているんです．これにより，このJSONファイル単体で入力データを表現することが出来ます．\nさて，ひととおり学習データの中身を見てもらいましたが，実はこのままではYOLOの学習が出来ないんです．\nYOLOに学習させるためには，画像のアノテーションを以下の形式で記述する必要があります．\n画像ファイル名.txt\nlabel_id X Y Width Height 例えば，image_1.jsonをこの形式で記述すると以下のようになります．\nimage_1.txt\n4 0.171569 0.375817 0.142157 0.183007\r6 0.286765 0.382353 0.124183 0.183007\r0 0.401144 0.398693 0.127451 0.185185\r5 0.517157 0.408497 0.117647 0.178649\r10 0.638889 0.418301 0.125817 0.189542\r7 0.767157 0.436819 0.127451 0.174292 YOLOのデータセットでは，ラベルの番号，X座標，Y座標，幅，高さを列挙して記述します．\nこれを手作業で変換するのはかなり面倒です．\nそこで今回は，Labelmeで保存したJSONファイルをYOLOのデータセット形式に変換するツール，labelme2yoloを使います．condaのannot_env環境下で以下の手順で変換します．\npip install labelme2yoloでlabelme2yoloをインストール datasetディレクトリの一つ上の階層で以下のコマンドを実行 labelme2yolo --json_dir ./datasets/ --val_size 0.10 --output_format \u0026quot;bbox\u0026quot; これが済んだら，datasetsディレクトリの中にYOLODatasetが生成されているはずです．構造は以下のようになっています．\n.\r└── YOLODatasets/\r├── images/\r│ ├── train/\r│ │ ├── image_1.jpg\r│ │ └── ～省略～\r│ └── val/\r│ └── image_7.jpg\r├── labels/\r│ ├── train/\r│ │ ├── image_1.txt\r│ │ └── ～省略～\r│ └── val/\r│ └── image_7.txt\r└── dataset.yaml 先ほどのデータセット構造より複雑になっていますね．画像とラベルが別のフォルダに格納され，更にその中で訓練用(train)と評価用(val)に分かれています．\nlabelsディレクトリのテキストファイルを見てみましょう．YOLOのフォーマットに変換され，ラベルIDや座標が記録されているはずです．\ndataset.yamlの中身を見てみましょう．\nデータセットまでの絶対パス，訓練評価用ディレクトリのパス，ラベルIDとラベル名の対応が記載されています．YOLOの学習時にはこのdataset.yamlを利用してデータセットを読み込みます．\nYOLOの学習 それでは，いよいよ作ったデータセットを使ってYOLOの学習をしてみましょう！！\nまず，テスト用の画像を用意します．以下のリンクから画像ファイルimage_11.jpgをダウンロードしてdataset.yamlと同階層に配置します．\nテスト画像image_11.jpg dataset.yamlと同階層に.ipynbファイルを作成して，以下のPythonプログラムを実行してみましょう．\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolo11n.pt\u0026#34;) model.train(data=\u0026#39;dataset.yaml\u0026#39;, epochs=300, verbose=False) results = model(\u0026#34;image_11.jpg\u0026#34;) results[0].show() CPUのみの実行環境ではかなり時間がかかるので気長に待ちましょう～\n学習が終了すると，image_11.jpgに対する推論結果が表示されます．\n未検出や誤検出がちらほらありますが，部分的に検出が成功していることがわかります．データセットサイズやエポック数を変えることで性能の改善が期待できます．\nまた，データセットディレクトリのruns内を見ると，weightディレクトリにbest.ptが保存されていることがわかります． これは今回学習したモデルファイルで，実際に物体検出をシステム開発に組み込むときに使用します．\n","permalink":"http://localhost:1313/blog/posts/20250506_yoloannotation/","summary":"\u003ch2 id=\"アノテーションとは\"\u003eアノテーションとは？\u003c/h2\u003e\n\u003cp\u003e機械学習の分類のひとつである\u003cstrong\u003e教師あり学習\u003c/strong\u003eでは，まず入力データと出力データの組を複数用意し，それらを使って機械学習モデルを訓練します．\u003c/p\u003e\n\u003cp\u003e入力データと出力データの組は，タスクによって様々です．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e入力：画像データ，出力：それぞれが犬or猫\u003c/li\u003e\n\u003cli\u003e入力：家の情報（間取り，立地，築年数など），出力：家の価格\u003c/li\u003e\n\u003cli\u003e入力：これまでの気温，出力：明日の気温\u003c/li\u003e\n\u003cli\u003e入力：画像データ，出力：各物体の座標\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eどんなタスクを前提とする場合でも，これら\u003cstrong\u003e入力データと出力データの組を作る作業\u003c/strong\u003eが必要となります．これが\u003cstrong\u003eアノテーション\u003c/strong\u003eです．\u003c/p\u003e\n\u003ch2 id=\"環境構築\"\u003e環境構築\u003c/h2\u003e\n\u003cp\u003e今回はYOLOによる画像物体認識を目標とします．そこで，アノテーションで作るべきデータは以下になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e入力：画像データ，出力：各物体の座標\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e頑張れば用意した画像の座標を数えてアノテーションできそうですが，今回はより簡単に作業を進めるために\u003cstrong\u003eLabelme\u003c/strong\u003eというツールを使います．GUIが整備されたソフトウェアですが，Pythonライブラリとして無償で提供されています！（ありがたい！！）\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAnaconda Promptを開く\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda create -n annot_env python=3.12\u003c/code\u003eで仮想環境を作成\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate annot_env\u003c/code\u003eで仮想環境を有効化\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install --upgrade labelme\u003c/code\u003eと入力しLabelmeをインストール\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eこれでインストールは完了です．そのまま\u003ccode\u003eannot_env\u003c/code\u003e環境内で\u003ccode\u003elabelme\u003c/code\u003eコマンドを実行してみてください！\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"labelme.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこの画面が表示されたら成功です！\u003c/p\u003e\n\u003ch2 id=\"いざアノテーション\"\u003eいざ，アノテーション\u003c/h2\u003e\n\u003ch3 id=\"画像読み込み\"\u003e画像読み込み\u003c/h3\u003e\n\u003cp\u003e今回は画像から麻雀牌を物体認識するタスクに挑戦してみます！サンプル画像は以下からダウンロード，解凍してください．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hakoshi-normal/yolo_sample/raw/refs/heads/main/images/images.zip\"\u003eサンプル画像\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eZipファイルの解凍が済んだらLabelmeの画面左側にある\u003ccode\u003eOpen Dir\u003c/code\u003eから，画像のディレクトリを開いてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"labelme_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこれでアノテーションの準備は完了です！\u003c/p\u003e\n\u003ch3 id=\"範囲選択ラベル付け\"\u003e範囲選択，ラベル付け\u003c/h3\u003e\n\u003cp\u003eアノテーションの流れは以下になります．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e画像を右クリック，\u003ccode\u003eCreate Rectangle\u003c/code\u003eをクリック\u003c/li\u003e\n\u003cli\u003e物体の範囲を四角形で囲んで選択，間違えたら\u003ccode\u003eEsc\u003c/code\u003eキーで戻る\u003c/li\u003e\n\u003cli\u003e出てきたポップアップの上部\u003ccode\u003eEnter object label\u003c/code\u003eにラベルを入力して\u003ccode\u003eOK\u003c/code\u003eをクリック\u003c/li\u003e\n\u003cli\u003e2と3を物体の数だけひたすら繰り返す\u003c/li\u003e\n\u003cli\u003e画像内のアノテーションが終わったら画面左部\u003ccode\u003eSave\u003c/code\u003eから画像と同名のJSONファイルを保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eちょっとやってみますね～\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"annotated.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eはい，できました．ラベルは牌の種類（萬子筒子索子字牌）の略称です．\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 150%; color: red;\"\u003eめんどくさい\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e機械学習では一般的にデータの数が大きいほど性能が上がりやすいとされています．つまり，（要件の要求レベルにもよりますが）精度を上げるためにはたくさんアノテーションする必要があるんです．\u003c/p\u003e\n\u003cp\u003eそこで，特定のタスクの機械学習を考える場合はアノテーションに取り掛かる前に，データセットが利用可能なライセンスで公開されていないかどうか調べるところからスタートします．例えばYOLOの場合，\u003ca href=\"https://universe.roboflow.com/\"\u003eRoboflow\u003c/a\u003e，\u003ca href=\"https://visualdata.io/discovery\"\u003eVisualData\u003c/a\u003e，\u003ca href=\"https://storage.googleapis.com/openimages/web/index.html\"\u003eGoogle Open Images Dataset\u003c/a\u003eなどが画像データセットを公開している他，タスクに応じて論文と併せて公開されているデータセットも多数あります．\u003c/p\u003e\n\u003cp\u003eまた今回はアノテーション入門ということで手作業での方法を紹介していますが，画像認識AIによる半自動ツールもあるので興味があれば調べてみてください．\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e今回は，なんと，特別に，アノテーション済みファイルを用意してあります！\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hakoshi-normal/yolo_sample/raw/refs/heads/main/images/labels.zip\"\u003eアノテーション結果（JSON）\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eZIPファイルをダウンロード＆解凍したら，\u003ccode\u003edatasets\u003c/code\u003eという名前のディレクトリを作成し，先程の画像ファイルとJSONファイルを以下の構造で配置してください．（\u003ccode\u003eimage_1.json\u003c/code\u003eは自作のファイルで構いません！）\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-planetext\" data-lang=\"planetext\"\u003e.\r\n└── datasets/\r\n    ├── image_1.jpg\r\n    ├── image_1.json\r\n    ├── image_2.jpg\r\n    ├── image_2.json\r\n    ├── 〜省略〜\r\n    ├── image_10.jpg\r\n    └── image_10.json\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"データセットフォーマットの変換\"\u003eデータセットフォーマットの変換\u003c/h3\u003e\n\u003cp\u003eでは，学習させるデータの中身を一度見てみましょう！\u003ccode\u003eimage_1.json\u003c/code\u003eを開いてみます．\u003c/p\u003e","title":"【入門3】YOLOアノテーションと学習"},{"content":"はじめに 自動採譜プロジェクトの息抜き的な記事です．\nせっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして， ところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．\nカメラ切り替えトリガー カメラの切り替えトリガーを「牌山にプレーヤーが触れたタイミング」と設定しました．\n自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．\nツモ位置の検出 まず，天カメの映像から卓上のツモ位置を特定する必要があります．\n牌山の形を見てみましょう．\n対局開始前はこんな感じ．\n対局開始直後はこんな感じ．\n対局中はこんな感じ．\n対局開始前は4つ牌山がありますが，対局開始以降は牌山が必ず3つ以下になります王牌の扱い上，4つのときもあります．\n牌山が3つのとき\n牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．\n牌山が4つのとき\n一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．\n実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．\n牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．\n実装概要 元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます． cv2.inRange()を使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．\n二値化した画像に対し，cv2.findContours()を使用し，牌山となりうる矩形を検出します．\n手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます． その後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．\nツモプレイヤーの検出 ツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します． MediaPipe のハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します． 今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．\nプレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．\nオートスイッチャー実装！！！ OBSを使用して配信するため，カメラ切り替えには obs-websocket-py を使用しました．こちらの サンプル がそのまま動作して助かりました．\nGUI実装にはHTML/JSでの記述が可能な Eel を使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．\n実際の動作の様子がこちら．\nYour browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．\nあと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．\n","permalink":"http://localhost:1313/blog/posts/20240210_mahjongautocamswitcher/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e自動採譜プロジェクト\u003c/a\u003eの息抜き的な記事です．\u003c/p\u003e\n\u003cp\u003eせっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして，\nところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．\u003c/p\u003e\n\u003ch2 id=\"カメラ切り替えトリガー\"\u003eカメラ切り替えトリガー\u003c/h2\u003e\n\u003cp\u003eカメラの切り替えトリガーを「\u003cstrong\u003e牌山にプレーヤーが触れたタイミング\u003c/strong\u003e」と設定しました．\u003c/p\u003e\n\u003cp\u003e自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．\u003c/p\u003e\n\u003ch2 id=\"ツモ位置の検出\"\u003eツモ位置の検出\u003c/h2\u003e\n\u003cp\u003eまず，天カメの映像から卓上のツモ位置を特定する必要があります．\u003cbr\u003e牌山の形を見てみましょう．\u003c/p\u003e\n\u003cp\u003e対局開始前はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_0.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局開始直後はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局中はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_2.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局開始前は4つ牌山がありますが，対局開始以降は牌山が\u003cdel\u003e必ず3つ以下になります\u003c/del\u003e王牌の扱い上，4つのときもあります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e牌山が3つのとき\u003c/p\u003e\n\u003cp\u003e牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e牌山が4つのとき\u003c/p\u003e\n\u003cp\u003e一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"fig1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．\u003c/p\u003e\n\u003cp\u003e牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．\u003c/p\u003e\n\u003ch3 id=\"実装概要\"\u003e実装概要\u003c/h3\u003e\n\u003cp\u003e元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます．\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"original_image.png\"/\u003e \r\n\u003c/figure\u003e\r\n\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecv2.inRange()\u003c/code\u003eを使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"mask_hai.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e二値化した画像に対し，\u003ccode\u003ecv2.findContours()\u003c/code\u003eを使用し，牌山となりうる矩形を検出します．\u003c/p\u003e\n\u003cp\u003e手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます．\nその後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumohai.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"ツモプレイヤーの検出\"\u003eツモプレイヤーの検出\u003c/h2\u003e\n\u003cp\u003eツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します．\n\u003ca href=\"https://developers.google.com/mediapipe/solutions/vision/hand_landmarker\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eMediaPipe\u003c/a\u003e\nのハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します．\n今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumo1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eプレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumo2.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"オートスイッチャー実装\"\u003eオートスイッチャー実装！！！\u003c/h2\u003e\n\u003cp\u003eOBSを使用して配信するため，カメラ切り替えには\n\u003ca href=\"https://github.com/Elektordi/obs-websocket-py/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eobs-websocket-py\u003c/a\u003e\nを使用しました．こちらの\n\u003ca href=\"https://github.com/Elektordi/obs-websocket-py/blob/master/samples/switch_scenes.py\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eサンプル\u003c/a\u003e\nがそのまま動作して助かりました．\u003c/p\u003e\n\u003cp\u003eGUI実装にはHTML/JSでの記述が可能な\n\u003ca href=\"https://github.com/python-eel/Eel\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eEel\u003c/a\u003e\nを使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．\u003c/p\u003e\n\u003cp\u003e実際の動作の様子がこちら．\u003c/p\u003e\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\n\u003cp\u003e現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．\u003c/p\u003e\n\u003cp\u003eあと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．\u003c/p\u003e","title":"麻雀放送対局用のオートスイッチャーを作りたい"},{"content":"はじめに 複数カメラの入力に対しYOLOv8で並列推論するのソースコード解説です． 解説のため，一部表現を変えている部分があります．\n大まかな流れは以下の通り．\n共有メモリ確保 フレーム入力プロセス カメラからフレームを取得 フレームを共有メモリ1に保存 推論プロセス 共有フレームを呼び出し 推論 推論結果を描画したフレームを共有メモリ2に保存 フレーム表示プロセス 共有フレームを呼び出し フレームの画面描画 プロセス間の遷移はmultiprocessingモジュールのEvent関数をフラグとして利用します． カメラの台数が増えた場合はプロセス数を増やせば対応できます．\n事前準備 設定情報 cam_N = 4 # カメラ台数 proccess_N = 3 # カメラ1台あたりのプロセス数 frame_info = {\u0026#34;nbytes\u0026#34; : frame.nbytes, \u0026#34;shape\u0026#34; : frame.shape, \u0026#34;dtype\u0026#34; : frame.dtype} フレームのサイズや型はメモリ確保，呼び出し時に必要になります．\n共有メモリの確保 shm = shared_memory.SharedMemory(create=True, size=frame.nbytes, name = mem_name) 共有メモリの確保はこの一文でできます．めっちゃ便利ですね． ただし，mem_nameを動的に作成して複数の共有メモリを確保する場合，注意事項があります． shared_memory.SharedMemoryの戻り値を格納する変数（上記コードではshm）を設定しなかった場合や上書きした場合，それ以前の共有メモリ領域は参照できなくなります．その場合はshmを格納するリストを用意しておくとアクセス可能になります．\nプロセスの立ち上げ p = Process(target=関数名, args=(引数,)) p.start() # プロセス開始 p.join() # プロセス終了 複数プロセスを立ち上げる場合は p をリストに入れておくと管理が楽です．\nフレーム入力プロセス def rec_cam(cam_id, pre_flags, frame_info): cap = cv2.VideoCapture(cam_id) while True: _, frame = cap.read() idx = (cam_id)*proccess_N+counter%proccess_N mem_name = f\u0026#39;shared{idx+1:02}\u0026#39; shm = shared_memory.SharedMemory(name=mem_name) frame_sh = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) frame_sh[:] = frame[:] # 推論開始 pre_flags[idx].set() cap.close() multiprocessingのSharedMemoryでは共有メモリを名前をつけて呼び出すことができます．便利！ ここでは，\u0026ldquo;shared1[番号]\u0026ldquo;という名前で共有メモリを確保している（後述）．カメラ台数✕カメラあたりのプロセス数分の共有メモリ領域が必要になるため， あるカメラのあるフレームに対する保存領域は，カメラ番号✕カメラ1台あたりのプロセス数＋フレーム番号%カメラ1台あたりのプロセス数で求めることができる．と同時にこの番号は推論プロセスを動作させるフラグ管理のインデックスとしても使う．\n推論プロセス def predict_frame(pre_flag, view_flag, mem_name, cam_id, frame_info): while True: if pre_flag.is_set(): # 推論開始 pre_flag.clear() shm = shared_memory.SharedMemory(name=mem_name) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # 重い推論処理がここに入る（frameに結果を反映する） # 結果統合用共有メモリにのせる shm_result = shared_memory.SharedMemory(name=f\u0026#39;shared2{cam_id}\u0026#39;) frame_new = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm_result.buf) frame_new[:]=frame[:] # 結果表示 view_flag.set() カメラ台数✕カメラあたりのプロセス数分の推論プロセスを立ち上げます． 推論開始用のフラグや共有メモリの呼び出し名は予め与えた状態でプロセスを立ち上げる．フレーム入力プロセスでの推論開始用フラグの有効化を待ち受けています． 処理済みのフレーを格納する共有メモリ領域名は\u0026rsquo;shared2[番号]\u0026lsquo;としており，カメラの台数分事前に確保してあります． 検出結果を描画する必要がない場合は，フレーム表示プロセスを介する必要がないため，別プロセスに推論結果を渡したり，UDP通信などで端末間の結果送信が実装できると思います．\nフレーム表示プロセス def view_frame(view_flag, cam_id, frame_info): while True: if view_flag.is_set(): view_flag.clear() shm = shared_memory.SharedMemory(name=f\u0026#39;shared2{cam_id}\u0026#39;) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) cv2.imshow(f\u0026#39;window_{cam_id}\u0026#39;, frame) cv2.waitKey(1) 推論プロセスでのフレーム表示用フラグの有効化を待ち受けています．\nおわりに 今回は，以前実装した並列画像処理プログラムの解説でした．コードブロックを断片的に示しただけなので，具体的な実装コードを知りたい方はソースコードから参照できます．\n","permalink":"http://localhost:1313/blog/posts/20231226_multiprocessingforrealtimevideo/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/2023_12_01_multiprocessingforyolov8/\" target=\"_blank\"\u003e複数カメラの入力に対しYOLOv8で並列推論する\u003c/a\u003eのソースコード解説です．\n解説のため，一部表現を変えている部分があります．\u003c/p\u003e\n\u003cp\u003e大まかな流れは以下の通り．\u003c/p\u003e\n\u003col start=\"0\"\u003e\n\u003cli\u003e共有メモリ確保\u003c/li\u003e\n\u003cli\u003eフレーム入力プロセス\n\u003col\u003e\n\u003cli\u003eカメラからフレームを取得\u003c/li\u003e\n\u003cli\u003eフレームを共有メモリ1に保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e推論プロセス\n\u003col\u003e\n\u003cli\u003e共有フレームを呼び出し\u003c/li\u003e\n\u003cli\u003e推論\u003c/li\u003e\n\u003cli\u003e推論結果を描画したフレームを共有メモリ2に保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eフレーム表示プロセス\n\u003col\u003e\n\u003cli\u003e共有フレームを呼び出し\u003c/li\u003e\n\u003cli\u003eフレームの画面描画\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eプロセス間の遷移はmultiprocessingモジュールのEvent関数をフラグとして利用します．\nカメラの台数が増えた場合はプロセス数を増やせば対応できます．\u003c/p\u003e\n\u003ch2 id=\"事前準備\"\u003e事前準備\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e設定情報\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecam_N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# カメラ台数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eproccess_N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# カメラ1台あたりのプロセス数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eframe_info \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;nbytes\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enbytes,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;shape\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshape,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dtype\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edtype}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eフレームのサイズや型はメモリ確保，呼び出し時に必要になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e共有メモリの確保\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eshm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e shared_memory\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSharedMemory(create\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enbytes, name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e mem_name)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e共有メモリの確保はこの一文でできます．めっちゃ便利ですね．\nただし，mem_nameを動的に作成して複数の共有メモリを確保する場合，注意事項があります．\nshared_memory.SharedMemoryの戻り値を格納する変数（上記コードではshm）を設定しなかった場合や上書きした場合，それ以前の共有メモリ領域は参照できなくなります．その場合はshmを格納するリストを用意しておくとアクセス可能になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプロセスの立ち上げ\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Process(target\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e関数名, args\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e(引数,))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estart() \u003cspan style=\"color:#75715e\"\u003e# プロセス開始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ejoin() \u003cspan style=\"color:#75715e\"\u003e# プロセス終了\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e複数プロセスを立ち上げる場合は p をリストに入れておくと管理が楽です．\u003c/p\u003e\n\u003ch2 id=\"フレーム入力プロセス\"\u003eフレーム入力プロセス\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erec_cam\u003c/span\u003e(cam_id, pre_flags, frame_info):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cv2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eVideoCapture(cam_id)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            _, frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            idx \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (cam_id)\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003eproccess_N\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003ecounter\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003eproccess_N\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            mem_name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;shared\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003eidx\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e02\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            shm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e shared_memory\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSharedMemory(name\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003emem_name)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            frame_sh \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003endarray(shape\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;shape\u0026#34;\u003c/span\u003e], dtype\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dtype\u0026#34;\u003c/span\u003e], buffer\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eshm\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebuf)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            frame_sh[:] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e frame[:]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# 推論開始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            pre_flags[idx]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eset()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003emultiprocessingのSharedMemoryでは共有メモリを名前をつけて呼び出すことができます．便利！\nここでは，\u0026ldquo;shared1[番号]\u0026ldquo;という名前で共有メモリを確保している（後述）．カメラ台数✕カメラあたりのプロセス数分の共有メモリ領域が必要になるため，\nあるカメラのあるフレームに対する保存領域は，カメラ番号✕カメラ1台あたりのプロセス数＋フレーム番号%カメラ1台あたりのプロセス数で求めることができる．と同時にこの番号は推論プロセスを動作させるフラグ管理のインデックスとしても使う．\u003c/p\u003e","title":"リアルタイムの映像入力に対してFPSを落とさずに画像処理したい"},{"content":"ビデオカメラからOpenCVで映像取得（低品質） 麻雀自動採譜の実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．\nビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できるビデオキャプチャカードを導入． これでビデオカメラをWebカメラみたく使えるぞ！やった！！\nと思った矢先，\nなんか遅い気がする！！\nよくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，1秒遅延＋低FPS＋低画質という散々な結果に\nお前 Full HD で 30FPS 出るって言ってたじゃないか\u0026hellip;\u0026hellip;\nキャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．\nRTSP通信でカメラの映像を受信してみる\nこのときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった． 今回もPyAVを使えば解決するかもしれない．\nビデオカメラからPyAVで映像取得 PyAVでPCに有線接続されたカメラから映像を取得する．\nimport cv2 import av con_options = dict( video_size=\u0026#39;1920x1080\u0026#39;, vcodec=\u0026#39;mjpeg\u0026#39;, framerate=\u0026#39;30\u0026#39;, rtbufsize=\u0026#39;1\u0026#39;, ) device_name = \u0026#34;USB Video\u0026#34; con_def = dict( format=\u0026#39;dshow\u0026#39;, file=f\u0026#39;video={device_name}\u0026#39;, options=dict(con_options, video_device_number=\u0026#39;0\u0026#39;) ) container = av.open(**con_def) for frame in container.decode(video=0): frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) # cv2.imshow(\u0026#34;test_window\u0026#34;, frame) # cv2.waitKey(1) ちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．\nffmpeg -list_devices true -f dshow -i dummy PyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．\nこれで遅延ほぼなし，30FPSでの映像受信ができた．\n","permalink":"http://localhost:1313/blog/posts/20231225_getframebypyav/","summary":"\u003ch2 id=\"ビデオカメラからopencvで映像取得低品質\"\u003eビデオカメラからOpenCVで映像取得（低品質）\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003eの実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．\u003c/p\u003e\n\u003cp\u003eビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できる\u003ca href=\"https://amzn.asia/d/jclJASJ\"\u003eビデオキャプチャカード\u003c/a\u003eを導入．\nこれでビデオカメラをWebカメラみたく使えるぞ！やった！！\u003c/p\u003e\n\u003cp\u003eと思った矢先，\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eなんか遅い気がする！！\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eよくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，\u003cstrong\u003e1秒遅延＋低FPS＋低画質\u003c/strong\u003eという散々な結果に\u003c/p\u003e\n\u003cp\u003eお前 Full HD で 30FPS 出るって言ってたじゃないか\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003eキャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/blog/2023_12_01_getframertsp/\"\u003eRTSP通信でカメラの映像を受信してみる\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこのときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった．\n今回も\u003ca href=\"https://pyav.org/docs/develop/index.html\" target=\"_blank\"\u003ePyAV\u003c/a\u003eを使えば解決するかもしれない．\u003c/p\u003e\n\u003ch2 id=\"ビデオカメラからpyavで映像取得\"\u003eビデオカメラからPyAVで映像取得\u003c/h2\u003e\n\u003cp\u003ePyAVでPCに有線接続されたカメラから映像を取得する．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e cv2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e av\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econ_options  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  dict(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    video_size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1920x1080\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    vcodec\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;mjpeg\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    framerate\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;30\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    rtbufsize\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edevice_name  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;USB Video\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econ_def  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  dict(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;dshow\u0026#39;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    file\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;video=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003edevice_name\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    options\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003edict(con_options, video_device_number\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;0\u0026#39;\u003c/span\u003e) )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econtainer \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e av\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eopen(\u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003econ_def)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e frame \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e container\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edecode(video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto_ndarray(format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;bgr24\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# cv2.imshow(\u0026#34;test_window\u0026#34;, frame)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# cv2.waitKey(1)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-console\" data-lang=\"console\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003effmpeg -list_devices true -f dshow -i dummy\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．\u003c/p\u003e","title":"映像入力の遅延をPyAVで解決してみる"},{"content":" multiprocessing × YOLOv8の詳細解説 Denoを利用した基本的なWebサイト作成 Deno Deploy設定 ","permalink":"http://localhost:1313/blog/posts/20231212_scheduled_update/","summary":"\u003cul\u003e\n\u003cli\u003emultiprocessing × YOLOv8の詳細解説\u003c/li\u003e\n\u003cli\u003eDenoを利用した基本的なWebサイト作成\u003c/li\u003e\n\u003cli\u003eDeno Deploy設定\u003c/li\u003e\n\u003c/ul\u003e","title":"今後の更新予定"},{"content":"RTSPでフレーム取得（OpenCV） RTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．\nimport cv2 url = \u0026#34;rtsp://username:password@ipaddress\u0026#34; # Tapo C200は末尾に画質を指定 url = \u0026#34;rtsp://username:password@ipaddress/stream1\u0026#34; cap = cv2.VideoCapture(url) while True: ret, frame = cap.read() pass ただこれだとTapo C200では1秒程度遅延が発生してしまう． 麻雀自動採譜 の実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．\nRTSPでフレーム取得（PyAV） RTSP映像の受信にはFFmpegのPythonバインディングである PyAV を利用した方法もあり、以下で映像受信が可能でした．\nimport av\rurl = \u0026#34;rtsp://username:password@ipaddress\u0026#34;\rcontainer = av.open(url)\rfor frame in container.decode(video=0):\rframe = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;)\rpass PyAVを利用した場合は遅延が大幅に減少した．やったね．\nRTSPで一定時間毎にフレーム取得（PyAV） 麻雀自動採譜 牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚でtime.sleep(n)を使用したところ以下のエラーが複数出現．\nmax delay reached. need to consume packet\rRTP: missed 6066 packets\rRTP: PT=60: bad cseq a657 expected=8ea5\rmax delay reached. need to consume packet sleepで無理に処理を中断させたため接続が不安定になったのかも． 通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．\nimport av interval = 10 # 10秒ごとに撮影 url = \u0026#34;rtsp://username:password@ipaddress\u0026#34; container = av.open(url) counter = 0 for frame in container.decode(video=0): if frame is not None and frame.time//interval\u0026gt;counter: counter+=1 frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) cv2.imwrite(f\u0026#39;save_{counter:04}.png\u0026#39;, frame) PyAVのcontainerから呼び出したフレームは撮影開始を0としたときの撮影時刻をtimeで取得できる． 撮影開始後数フレームはNoneが返ることがあるため注意． これで安定したn秒毎の撮影が実行できる．\n","permalink":"http://localhost:1313/blog/posts/20231201_getframertsp/","summary":"\u003ch2 id=\"rtspでフレーム取得opencv\"\u003eRTSPでフレーム取得（OpenCV）\u003c/h2\u003e\n\u003cp\u003eRTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e cv2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rtsp://username:password@ipaddress\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Tapo C200は末尾に画質を指定\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rtsp://username:password@ipaddress/stream1\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cv2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eVideoCapture(url)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ret, frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eただこれだとTapo C200では\u003cstrong\u003e1秒程度遅延\u003c/strong\u003eが発生してしまう．\n\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003e\nの実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．\u003c/p\u003e\n\u003ch2 id=\"rtspでフレーム取得pyav\"\u003eRTSPでフレーム取得（PyAV）\u003c/h2\u003e\n\u003cp\u003eRTSP映像の受信にはFFmpegのPythonバインディングである\n\u003ca href=\"https://pyav.org/docs/develop/index.html\" target=\"_blank\"\u003ePyAV\u003c/a\u003e\nを利用した方法もあり、以下で映像受信が可能でした．\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-python:\" data-lang=\"python:\"\u003eimport av\r\n\r\nurl = \u0026#34;rtsp://username:password@ipaddress\u0026#34;\r\ncontainer = av.open(url)\r\nfor frame in container.decode(video=0):\r\n    frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;)\r\n    pass\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ePyAVを利用した場合は遅延が大幅に減少した．やったね．\u003c/p\u003e\n\u003ch2 id=\"rtspで一定時間毎にフレーム取得pyav\"\u003eRTSPで一定時間毎にフレーム取得（PyAV）\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003e\n牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚で\u003ccode\u003etime.sleep(n)\u003c/code\u003eを使用したところ以下のエラーが複数出現．\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-raw:\" data-lang=\"raw:\"\u003emax delay reached. need to consume packet\r\nRTP: missed 6066 packets\r\nRTP: PT=60: bad cseq a657 expected=8ea5\r\nmax delay reached. need to consume packet\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003esleepで無理に処理を中断させたため接続が不安定になったのかも．\n通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．\u003c/p\u003e","title":"RTSP通信でカメラの映像を受信してみる"},{"content":"実装過程あれこれ１ これまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の Tapo C200 からフレームを受信することができるようになりました． 早速各カメラに割り当てたプロセス毎に適当に学習させておいた YOLOv8 による牌推論を実行し，結果を描画してみました．わくわく！\nはい，映像出力が不穏な感じに． Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．\nプロセス間でまったく映像出力のタイミングがあっていない フレーム間隔がまちまち 映像の乱れが酷い FPSが低い（Tapo入力時15→10弱） 特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね\u0026hellip;\u0026hellip;\nフレーム出力がばらつく理由を勝手に予想．こんな感じだろうか\n出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える． こんなときこそマルチプロセス処理の出番でしょ↓\n（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）\n既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．\n映像受信プロセス ×4（カメラの台数） 推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当） 推論結果統合プロセス ×4（カメラの台数） さあやるぞ！\n実装過程あれこれ２ ここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．\nbase64→内部UDPが一瞬頭を過るが無視して 公式ドキュメント を漁ってみます．\nドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか． Value, Array, Queue, Pipe, RawArray, Manager, shared_memory\u0026hellip;\u0026hellip;\n多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．\n試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに． フラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．\nそんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります． Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r概ね同じタイミングでフレームが描画されているように見えますね．\nFPSはいずれのカメラも15程度であり，入力時のFPSを保持したままリアルタイムで推論が実行できました． ちなみに，\nmultiprocessingではなくthreadingを使ってYOLOv8のモデルをスレッド間で共有する方法があるが，提供元である UltralyticsのMedium記事 では推奨方法， 公式ドキュメント では非推奨方法として紹介されていたため今回は見送り． 試験的に使用してるYOLOv8モデルは，現環境（GeForce RTX 3070）で学習可能な最大サイズのモデル（YOLOv8l）に対して複数の公開データセットを統合した5万枚強の牌画像を学習させたものである．統合作業時にデータセット間でラベルのズレが発生したため，検出結果に誤りがある．ラベルが混合した状態のデータセットではあるが安定した検出ができているため，データセットの整理によって更に高精度な検出の実現が期待できる． 背景部分の予期せぬ牌検出は信頼度で棄却できるため問題なし． 検証動画は双方画面録画の仕様上FPSが低下している． ソースコード 複数台のカメラのフレーム入力に対してYOLOv8による推論を並列実行するPythonプログラム ソースコード解説はこちらからどうぞ．\nimport time import msvcrt from multiprocessing import Process, Event, shared_memory import numpy as np import av import cv2 from ultralytics import YOLO # RTSPカメラ設定情報 info_list = [[\u0026#34;toncamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;nancamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;shacamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;peicamera\u0026#34;, \u0026#34;*\u0026#34;]] password = \u0026#34;*\u0026#34; # 1:1280x720, 2:640x360 mode = 1 # 推論プロセス数指定 N = 3 def rec_cam(cam_id, username, ipaddress, pre_flags, stop_flag, frame_info): url = f\u0026#34;rtsp://{username}:{password}@{ipaddress}:554/stream{mode}\u0026#34; container = av.open(url) counter = 0 start = time.time() for frame in container.decode(video=0): counter+=1 frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) # 共有メモリにフレームを書き込み idx = (cam_id)*N+counter%N mem_name = f\u0026#39;shared{idx+1:02}\u0026#39; shm = shared_memory.SharedMemory(name=mem_name) frame_sh = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) frame_sh[:] = frame[:] # 推論開始 pre_flags[idx].set() if stop_flag.is_set(): t = time.time() - start print(f\u0026#34;{username}, TIME:{round(t, 3)}, FRAME:{counter}, FPS:{round(counter/t, 3)}\u0026#34;) break container.close() cv2.destroyAllWindows() return def finish_monitor(stop_flag): while True: if msvcrt.kbhit() and msvcrt.getch() == b\u0026#39;q\u0026#39;: print(\u0026#34;Finished\u0026#34;) stop_flag.set() break return def predict_frame(pre_flag, pre_fin_flag, mem_name, stop_flag, username, frame_info): model = YOLO(\u0026#39;best.pt\u0026#39;) while True: if pre_flag.is_set(): # 推論 pre_flag.clear() shm = shared_memory.SharedMemory(name=mem_name) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # YOLOv8による推論実行 result = model(frame)[0] frame = result.plot() # 結果統合用共有メモリにのせる shm_result = shared_memory.SharedMemory(name=username) frame_new = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm_result.buf) frame_new[:]=frame[:] # 結果統合用 pre_fin_flag.set() if stop_flag.is_set(): break def view_frame(pre_fin_flag, username, stop_flag, frame_info): while True: if pre_fin_flag.is_set(): pre_fin_flag.clear() shm = shared_memory.SharedMemory(name=username) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # Full-HDだと大抵画面が埋まっちゃう # frame = cv2.resize(frame, None, None, 0.6, 0.6) cv2.imshow(username, frame) cv2.waitKey(1) if stop_flag.is_set(): break if __name__ == \u0026#39;__main__\u0026#39;: # メモリ確保情報 url = f\u0026#34;rtsp://{info_list[0][0]}:{password}@{info_list[0][1]}:554/stream{mode}\u0026#34; container = av.open(url) for i, frame in enumerate(container.decode(video=0)): frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) if frame is not None: break container.close() frame_info = {\u0026#34;nbytes\u0026#34; : frame.nbytes, \u0026#34;shape\u0026#34; : frame.shape, \u0026#34;dtype\u0026#34; : frame.dtype} # 停止プロセス stop_flag = Event() p = Process(target=finish_monitor, args=(stop_flag,)) p.start() # 表示プロセス群（カメラ台数分） view_processes = [] pre_fin_flags = [] mem_space = [] for i in range(4): pre_fin_flag = Event() pre_fin_flags.append(pre_fin_flag) username = info_list[i][0] # メモリ確保 今回は画像用 shm = shared_memory.SharedMemory(create=True, size=frame_info[\u0026#34;nbytes\u0026#34;], name = username) mem_space.append(shm) p = Process(target=view_frame, args=(pre_fin_flag, username, stop_flag, frame_info)) view_processes.append(p) p.start() # 推論プロセス群（カメラ台数×推論プロセス数） pre_processes = [] pre_flags = [] mem_names = [] for i in range(N*len(info_list)): pre_flag = Event() pre_flags.append(pre_flag) mem_name = f\u0026#39;shared{i+1:02}\u0026#39; mem_names.append(mem_name) # メモリ確保 shm = shared_memory.SharedMemory(create=True, size=frame_info[\u0026#34;nbytes\u0026#34;], name = mem_name) mem_space.append(shm) # frame_sh = np.ndarray(shape=frame_shape, dtype=frame_dtype, buffer=shm.buf) username = info_list[i//N][0] pre_fin_flag = pre_fin_flags[i//N] p = Process(target=predict_frame, args=(pre_flag, pre_fin_flag, mem_name, stop_flag, username, frame_info)) pre_processes.append(p) p.start() # 撮影プロセス群（カメラ台数） rec_processes = [] for cam_id, info in enumerate(info_list): p = Process(target=rec_cam, args=(cam_id, info[0], info[1], pre_flags, stop_flag, frame_info,)) rec_processes.append(p) p.start() p.join() for p in view_processes: p.join() for p in pre_processes: p.join() for p in rec_processes: p.join() # 共有メモリのリソース開放、削除 for mem_name in mem_names + [info[0] for info in info_list[:4]]: shm = shared_memory.SharedMemory(name = mem_name) shm.close() shm.unlink() ","permalink":"http://localhost:1313/blog/posts/20231201_multiprocessingforyolov8/","summary":"\u003ch2 id=\"実装過程あれこれ１\"\u003e実装過程あれこれ１\u003c/h2\u003e\n\u003cp\u003eこれまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の\n\u003ca href=\"https://www.tp-link.com/jp/smart-home/tapo/tapo-c200/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eTapo C200\u003c/a\u003e\nからフレームを受信することができるようになりました．\n\u003cbr\u003e\n早速各カメラに割り当てたプロセス毎に適当に学習させておいた\n\u003ca href=\"https://docs.ultralytics.com/ja/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eYOLOv8\u003c/a\u003e\nによる牌推論を実行し，結果を描画してみました．わくわく！\u003c/p\u003e\n\u003cp\u003eはい，映像出力が不穏な感じに．\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20231201_multiprocessingforyolov8/before.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20231201_multiprocessingforyolov8/before.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\u003c/p\u003e\n\u003cp\u003e卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプロセス間でまったく映像出力のタイミングがあっていない\u003c/li\u003e\n\u003cli\u003eフレーム間隔がまちまち\u003c/li\u003e\n\u003cli\u003e映像の乱れが酷い\u003c/li\u003e\n\u003cli\u003eFPSが低い（Tapo入力時15→10弱）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね\u0026hellip;\u0026hellip;\u003cbr\u003e\u003c/p\u003e\n\u003cp\u003eフレーム出力がばらつく理由を勝手に予想．こんな感じだろうか\u003c/p\u003e\n\u003cfigure\u003e\r\n    \u003cimg loading=\"lazy\" src=\"multiprocess_fig1.jpg\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える．\n\u003cbr\u003e\u003cbr\u003e\n\u003cspan style=\"font-size: 200%; color: red;\"\u003e\nこんなときこそマルチプロセス処理の出番でしょ↓\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\r\n    \u003cimg loading=\"lazy\" src=\"multiprocess_fig2.jpg\"/\u003e \r\n\u003c/figure\u003e\r\n\n（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）\u003c/p\u003e\n\u003cp\u003e既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e映像受信プロセス ×4（カメラの台数）\u003c/li\u003e\n\u003cli\u003e推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当）\u003c/li\u003e\n\u003cli\u003e推論結果統合プロセス ×4（カメラの台数）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eさあやるぞ！\u003c/p\u003e\n\u003ch2 id=\"実装過程あれこれ２\"\u003e実装過程あれこれ２\u003c/h2\u003e\n\u003cp\u003eここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．\u003cbr\u003e\nbase64→内部UDPが一瞬頭を過るが無視して\n\u003ca href=\"https://docs.python.org/ja/3.10/library/multiprocessing.html\" target=\"_blank\"\u003e公式ドキュメント\u003c/a\u003e\nを漁ってみます．\u003c/p\u003e\n\u003cp\u003eドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか．\nValue, Array, Queue, Pipe, RawArray, Manager, shared_memory\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．\u003c/p\u003e\n\u003cp\u003e試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに．\nフラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．\u003c/p\u003e\n\u003cp\u003eそんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります．\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20231201_multiprocessingforyolov8/after.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20231201_multiprocessingforyolov8/after.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\u003c/p\u003e","title":"複数カメラの入力に対しYOLOv8で並列推論する"},{"content":"ライブラリとは？ ライブラリとは，再利用可能なソフトウェア部品のことで，特定の機能をまとめたコードの再集合のことを指します．簡単に言えば，他の人が作ってくれた機能を自身のプログラムで使える便利なコード集のことです． Pythonには標準ライブラリと外部ライブラリの2種類があります．\n標準ライブラリ 標準ライブラリはPythonをインストールしたときに標準的に入っているライブラリのことです．通常のプログラム作成時に頻繁に必要となる機能をまとめたものが多くあります．例えば，\n乱数を作成する random 時刻を取得できる datetime, time ファイルシステムへのアクセスを実現する os, shutil 基礎的な数学関数 math HTMLサポート html URLを使ってインターネットにアクセスできる urllib スマートフォンで言うところのカメラ，アルバム，時計などのデフォルトアプリみたいなものです．これらはPythonが動く環境であれば勝手に入っているため，import ライブラリ名と記述すればすぐに利用することができます．助かる！！\n# サイコロの出目を出力 import random dice_roll = random.randint(1, 6) print(\u0026#34;サイコロの出目:\u0026#34;, dice_roll) 外部ライブラリ 外部ライブラリはPythonにはもともと組み込まれておらず，自身で導入するタイプのライブラリのことです．実装したいプログラムの機能に応じて適切に選択する必要があります．例えば，\n効率的な多次元配列の数値計算を実現する Numpy 画像処理の機能を提供する OpenCV グラフを簡単に描画できる Matplotlib 様々なアルゴリズムでの機械学習を可能とする scikit-learn Webアプリの開発に役立つ Django スマートフォンで言うところのゲームやチャットアプリなどのストアアプリみたいなものです．これらは自身でインストール作業をする必要がありますが，それさえ済ませば高機能な実装をいとも容易く実現できます！とても便利！！ 有志のプログラマが開発して公開してくれているため，感謝の気持ちを忘れずに使いましょう（Pythonもそうだけど）．\n# サイコロの出目を出力 import numpy as np dice_roll = np.random.randint(1, 7) print(\u0026#34;サイコロの出目:\u0026#34;, dice_roll) ライブラリを確認してみよう では，ここからは実際に手を動かしながら解説を進めます． まずはスタートメニューから Anaconda Prompt を開いてください．\nこれはコマンドプロンプトと呼ばれる画面で，ここにコマンドと呼ばれる命令文を入力することでコンピュータを操作できます． 試しに，ここにtreeと打ち込んでEnterキーを押してみてください．\n\u0026hellip;\u0026hellip;何かツリー構造の記述がたくさん出てきたかと思います．これはディレクトリ構造をツリー形式で表示するコマンドです（あんまり使わないけど）！ なかなか表示が終わらない場合はCtrl+Cキーでキャンセルしてください．\nではコマンドを使って，今使っているPythonで利用可能な外部ライブラリの一覧を見てみましょう！ Anaconda Promptにconda listと打ち込んでEnterキーを押してみてください．\nインストール済みライブラリとバージョンの一覧が表示されました．既にたくさん入っているようです！\nよくよく見てみると，数値計算ができるNumpyや表データを扱えるPandasなど，データサイエンスに役立つライブラリがたくさんあります． 実はAnacondaは科学計算のためのPython配布形式（ディストリビューション）の一つであり，既に役立つライブラリが揃えられているんです．\nこの状態，一見プログラムを書くのに便利な環境であるように見えるんですが実はそうでないケースがあります．\n実はライブラリは，機能を実現するために別のライブラリを参照するケースがあります．例えば，\n画像処理を実現するOpenCVには，画像を配列として扱うためにNumpyが必要 グラフを描画するMatplotlibには，画像描画のためにPillowが必要 機械学習を実現するscikit-learnには，高度な数値計算をするためにScipyが必要 またライブラリはバージョンによって提供される機能が変化することがあります．そのため，同じ実行環境に特定のバージョンを指定して参照ライブラリを導入しておく必要があるのです．\nこれを，ライブラリの依存関係と言います．この依存関係が原因で複数のライブラリを導入するときに問題が発生する可能性があるんです．\n例えば，ライブラリA-C及びB-Cの依存関係があるとき，AとBが共存するためにはCのバージョンが整合している必要があります．\nこうしたライブラリバージョンの試行錯誤を行う場合，先程表示させた複雑に依存関係が絡み合ったライブラリ群を扱うのはなかなか骨が折れます．\nまた，不要なライブラリは更に混乱を引き起こすので必要となる機能のための最小構成で開発環境を整える必要があります．\nそこで役立つのが，仮想環境です！！！（やっと本題）\n仮想環境 仮想環境とは，プログラムを動かすときに仮に作って利用する動作環境のことです． 簡単に言うと「依存関係の解決が面倒だから，まっさらな世界を一から創造すればいいじゃん！！」です．\nでは，実際に作ってみましょう．\n先ほど開いたAnaconda Promptにconda create -n test python=3.12と打ち込んで実行（Proceed([y]/n)?にはyを入力して処理を続行） conda activate testと打ち込んで実行 ここでは，testという名前の仮想環境を作成してその環境に切り替える，という作業をしてもらいました． 最新行の左側が(test)と表示されていれば成功です！\n先ほどまでは(base)と表示されていたはずです．Anacondaではデフォルトでbase環境があり，そこにたくさんのライブラリがありました． ではconda listで現在のtest環境のライブラリ構成を見てみましょう！\nbase環境と比べてライブラリの数がかなり少なくなったはずです．ここに表示されているライブラリはPython3.12に付随する必要最低限のライブラリです． このきれいな環境に最小限の適切なライブラリを導入することで，快適なコーディングライフを送ることができます！！\n番外編：Conda仮想環境の管理 複数の仮想環境を扱うときに便利なコマンドをまとめておきます！\n仮想環境の作成（環境名がtest，Python3.12の場合）\nconda create -n test python=3.12 仮想環境への移行\nconda activate test 仮想環境から離脱\nconda deactivate 仮想環境の削除\nconda remove -n test --all 仮想環境一覧の表示\nconda info -e ","permalink":"http://localhost:1313/blog/posts/20250506_condavenv/","summary":"\u003ch2 id=\"ライブラリとは\"\u003eライブラリとは？\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eライブラリ\u003c/strong\u003eとは，再利用可能なソフトウェア部品のことで，特定の機能をまとめたコードの再集合のことを指します．簡単に言えば，他の人が作ってくれた機能を自身のプログラムで使える便利なコード集のことです．\nPythonには\u003cstrong\u003e標準ライブラリ\u003c/strong\u003eと\u003cstrong\u003e外部ライブラリ\u003c/strong\u003eの2種類があります．\u003c/p\u003e\n\u003ch3 id=\"標準ライブラリ\"\u003e標準ライブラリ\u003c/h3\u003e\n\u003cp\u003e標準ライブラリはPythonをインストールしたときに標準的に入っているライブラリのことです．通常のプログラム作成時に頻繁に必要となる機能をまとめたものが多くあります．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e乱数を作成する random\u003c/li\u003e\n\u003cli\u003e時刻を取得できる datetime, time\u003c/li\u003e\n\u003cli\u003eファイルシステムへのアクセスを実現する os, shutil\u003c/li\u003e\n\u003cli\u003e基礎的な数学関数 math\u003c/li\u003e\n\u003cli\u003eHTMLサポート html\u003c/li\u003e\n\u003cli\u003eURLを使ってインターネットにアクセスできる urllib\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスマートフォンで言うところのカメラ，アルバム，時計などのデフォルトアプリみたいなものです．これらはPythonが動く環境であれば勝手に入っているため，\u003ccode\u003eimport ライブラリ名\u003c/code\u003eと記述すればすぐに利用することができます．助かる！！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# サイコロの出目を出力\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e random\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edice_roll \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e random\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandint(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;サイコロの出目:\u0026#34;\u003c/span\u003e, dice_roll)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"外部ライブラリ\"\u003e外部ライブラリ\u003c/h3\u003e\n\u003cp\u003e外部ライブラリはPythonにはもともと組み込まれておらず，自身で導入するタイプのライブラリのことです．実装したいプログラムの機能に応じて適切に選択する必要があります．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e効率的な多次元配列の数値計算を実現する Numpy\u003c/li\u003e\n\u003cli\u003e画像処理の機能を提供する OpenCV\u003c/li\u003e\n\u003cli\u003eグラフを簡単に描画できる Matplotlib\u003c/li\u003e\n\u003cli\u003e様々なアルゴリズムでの機械学習を可能とする scikit-learn\u003c/li\u003e\n\u003cli\u003eWebアプリの開発に役立つ Django\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスマートフォンで言うところのゲームやチャットアプリなどのストアアプリみたいなものです．これらは自身でインストール作業をする必要がありますが，それさえ済ませば高機能な実装をいとも容易く実現できます！とても便利！！\n有志のプログラマが開発して公開してくれているため，感謝の気持ちを忘れずに使いましょう（Pythonもそうだけど）．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# サイコロの出目を出力\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e np\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edice_roll \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandom\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandint(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;サイコロの出目:\u0026#34;\u003c/span\u003e, dice_roll)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"ライブラリを確認してみよう\"\u003eライブラリを確認してみよう\u003c/h2\u003e\n\u003cp\u003eでは，ここからは実際に手を動かしながら解説を進めます．\nまずはスタートメニューから \u003cstrong\u003eAnaconda Prompt\u003c/strong\u003e を開いてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"img_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"console_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこれは\u003cstrong\u003eコマンドプロンプト\u003c/strong\u003eと呼ばれる画面で，ここに\u003cstrong\u003eコマンド\u003c/strong\u003eと呼ばれる命令文を入力することでコンピュータを操作できます．\n試しに，ここに\u003ccode\u003etree\u003c/code\u003eと打ち込んでEnterキーを押してみてください．\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;何かツリー構造の記述がたくさん出てきたかと思います．これはディレクトリ構造をツリー形式で表示するコマンドです（あんまり使わないけど）！\nなかなか表示が終わらない場合はCtrl+Cキーでキャンセルしてください．\u003c/p\u003e\n\u003cp\u003eではコマンドを使って，今使っているPythonで利用可能な外部ライブラリの一覧を見てみましょう！\nAnaconda Promptに\u003ccode\u003econda list\u003c/code\u003eと打ち込んでEnterキーを押してみてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"modules.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eインストール済みライブラリとバージョンの一覧が表示されました．既にたくさん入っているようです！\u003c/p\u003e\n\u003cp\u003eよくよく見てみると，数値計算ができるNumpyや表データを扱えるPandasなど，データサイエンスに役立つライブラリがたくさんあります．\n実はAnacondaは科学計算のためのPython配布形式（ディストリビューション）の一つであり，既に役立つライブラリが揃えられているんです．\u003c/p\u003e\n\u003cp\u003eこの状態，一見プログラムを書くのに便利な環境であるように見えるんですが実はそうでないケースがあります．\u003c/p\u003e","title":"【入門1】Python仮想環境"},{"content":"JupyterLab+Visual Studio Codeを使ったPython実行 Visual Studio CodeにJupyter拡張機能をインストール 空のファイルを作成し，拡張子を.ipynbとする ＋コードボタンからコードブロックを作成し，Pythonプログラムを記述 右上カーネルの選択から実行したいPython環境を選択し コードブロック左側の実行ボタン▷を押下 ポップアップが出てきたらインストールをクリックしてipykernelをインストール （pip install ipykernelを当該環境で実行してインストールも出来ます） YOLOのセットアップ 今回は最新のYOLOを簡単に扱うことができるUltralyticsパッケージを利用します．\nconda create --name yolo-env python=3.11を実行して仮想環境を作成 conda activate yolo-envで仮想環境を有効化 pip install ultralyticsでUltralyticsパッケージをインストール ここまで済んだら，テストコードを動かしてみましょう！\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolo11n.pt\u0026#34;) results = model(\u0026#34;https://ultralytics.com/images/bus.jpg\u0026#34;) results[0].show() バスを背景にした画像の物体検出が表示されたはずです！\n作業ディレクトリを開くと，yolo11n.ptというファイルがダウンロードされています．これが，YOLOのモデルファイルです．これはYOLOのデフォルトの訓練済みモデルで，COCO Datasetに収録された80種類の物体を識別することが出来ます．\n","permalink":"http://localhost:1313/blog/posts/20250506_yolojupyter/","summary":"\u003ch2 id=\"jupyterlabvisual-studio-codeを使ったpython実行\"\u003eJupyterLab+Visual Studio Codeを使ったPython実行\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eVisual Studio CodeにJupyter拡張機能をインストール\u003c/li\u003e\n\u003cli\u003e空のファイルを作成し，拡張子を\u003ccode\u003e.ipynb\u003c/code\u003eとする\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e＋コード\u003c/code\u003eボタンからコードブロックを作成し，Pythonプログラムを記述\u003c/li\u003e\n\u003cli\u003e右上\u003ccode\u003eカーネルの選択\u003c/code\u003eから実行したいPython環境を選択し\u003c/li\u003e\n\u003cli\u003eコードブロック左側の実行ボタン\u003ccode\u003e▷\u003c/code\u003eを押下\u003c/li\u003e\n\u003cli\u003eポップアップが出てきたら\u003ccode\u003eインストール\u003c/code\u003eをクリックして\u003ccode\u003eipykernel\u003c/code\u003eをインストール\n（\u003ccode\u003epip install ipykernel\u003c/code\u003eを当該環境で実行してインストールも出来ます）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"vsjupyter.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"yoloのセットアップ\"\u003eYOLOのセットアップ\u003c/h2\u003e\n\u003cp\u003e今回は最新のYOLOを簡単に扱うことができるUltralyticsパッケージを利用します．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003econda create --name yolo-env python=3.11\u003c/code\u003eを実行して仮想環境を作成\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate yolo-env\u003c/code\u003eで仮想環境を有効化\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install ultralytics\u003c/code\u003eでUltralyticsパッケージをインストール\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eここまで済んだら，テストコードを動かしてみましょう！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e ultralytics \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e YOLO\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emodel \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e YOLO(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;yolo11n.pt\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresults \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://ultralytics.com/images/bus.jpg\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresults[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eバスを背景にした画像の物体検出が表示されたはずです！\u003c/p\u003e\n\u003cp\u003e作業ディレクトリを開くと，\u003ccode\u003eyolo11n.pt\u003c/code\u003eというファイルがダウンロードされています．これが，YOLOのモデルファイルです．これはYOLOのデフォルトの訓練済みモデルで，COCO Datasetに収録された80種類の物体を識別することが出来ます．\u003c/p\u003e","title":"【入門2】YOLO，Jupyterのセットアップ"},{"content":"アノテーションとは？ 機械学習の分類のひとつである教師あり学習では，まず入力データと出力データの組を複数用意し，それらを使って機械学習モデルを訓練します．\n入力データと出力データの組は，タスクによって様々です．例えば，\n入力：画像データ，出力：それぞれが犬or猫 入力：家の情報（間取り，立地，築年数など），出力：家の価格 入力：これまでの気温，出力：明日の気温 入力：画像データ，出力：各物体の座標 どんなタスクを前提とする場合でも，これら入力データと出力データの組を作る作業が必要となります．これがアノテーションです．\n環境構築 今回はYOLOによる画像物体認識を目標とします．そこで，アノテーションで作るべきデータは以下になります．\n入力：画像データ，出力：各物体の座標 頑張れば用意した画像の座標を数えてアノテーションできそうですが，今回はより簡単に作業を進めるためにLabelmeというツールを使います．GUIが整備されたソフトウェアですが，Pythonライブラリとして無償で提供されています！（ありがたい！！）\nAnaconda Promptを開く conda create -n annot_env python=3.12で仮想環境を作成 conda activate annot_envで仮想環境を有効化 pip install --upgrade labelmeと入力しLabelmeをインストール これでインストールは完了です．そのままannot_env環境内でlabelmeコマンドを実行してみてください！\nこの画面が表示されたら成功です！\nいざ，アノテーション 画像読み込み 今回は画像から麻雀牌を物体認識するタスクに挑戦してみます！サンプル画像は以下からダウンロード，解凍してください．\nサンプル画像 Zipファイルの解凍が済んだらLabelmeの画面左側にあるOpen Dirから，画像のディレクトリを開いてください．\nこれでアノテーションの準備は完了です！\n範囲選択，ラベル付け アノテーションの流れは以下になります．\n画像を右クリック，Create Rectangleをクリック 物体の範囲を四角形で囲んで選択，間違えたらEscキーで戻る 出てきたポップアップの上部Enter object labelにラベルを入力してOKをクリック 2と3を物体の数だけひたすら繰り返す 画像内のアノテーションが終わったら画面左部Saveから画像と同名のJSONファイルを保存 ちょっとやってみますね～\nはい，できました．ラベルは牌の種類（萬子筒子索子字牌）の略称です．\n\u0026hellip;\u0026hellip;\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\nめんどくさい\n機械学習では一般的にデータの数が大きいほど性能が上がりやすいとされています．つまり，（要件の要求レベルにもよりますが）精度を上げるためにはたくさんアノテーションする必要があるんです．\nそこで，特定のタスクの機械学習を考える場合はアノテーションに取り掛かる前に，データセットが利用可能なライセンスで公開されていないかどうか調べるところからスタートします．例えばYOLOの場合，Roboflow，VisualData，Google Open Images Datasetなどが画像データセットを公開している他，タスクに応じて論文と併せて公開されているデータセットも多数あります．\nまた今回はアノテーション入門ということで手作業での方法を紹介していますが，画像認識AIによる半自動ツールもあるので興味があれば調べてみてください．\n今回は，なんと，特別に，アノテーション済みファイルを用意してあります！\nアノテーション結果（JSON） ZIPファイルをダウンロード＆解凍したら，datasetsという名前のディレクトリを作成し，先程の画像ファイルとJSONファイルを以下の構造で配置してください．（image_1.jsonは自作のファイルで構いません！）\n.\r└── datasets/\r├── image_1.jpg\r├── image_1.json\r├── image_2.jpg\r├── image_2.json\r├── 〜省略〜\r├── image_10.jpg\r└── image_10.json データセットフォーマットの変換 では，学習させるデータの中身を一度見てみましょう！image_1.jsonを開いてみます．\nJSONはデータ記述形式のひとつで，キーと値を並べて書くのが特徴です．Pythonの辞書型そっくりですね． ラベルや座標がファイルの中に複数記述されていることがわかります．\n少しスクロールすると，よくわからない文字列が大量に記述されています．実はこれ，Base64と呼ばれるエンコード形式で画像そのものが書きつけられているんです．これにより，このJSONファイル単体で入力データを表現することが出来ます．\nさて，ひととおり学習データの中身を見てもらいましたが，実はこのままではYOLOの学習が出来ないんです．\nYOLOに学習させるためには，画像のアノテーションを以下の形式で記述する必要があります．\n画像ファイル名.txt\nlabel_id X Y Width Height 例えば，image_1.jsonをこの形式で記述すると以下のようになります．\nimage_1.txt\n4 0.171569 0.375817 0.142157 0.183007\r6 0.286765 0.382353 0.124183 0.183007\r0 0.401144 0.398693 0.127451 0.185185\r5 0.517157 0.408497 0.117647 0.178649\r10 0.638889 0.418301 0.125817 0.189542\r7 0.767157 0.436819 0.127451 0.174292 YOLOのデータセットでは，ラベルの番号，X座標，Y座標，幅，高さを列挙して記述します．\nこれを手作業で変換するのはかなり面倒です．\nそこで今回は，Labelmeで保存したJSONファイルをYOLOのデータセット形式に変換するツール，labelme2yoloを使います．condaのannot_env環境下で以下の手順で変換します．\npip install labelme2yoloでlabelme2yoloをインストール datasetディレクトリの一つ上の階層で以下のコマンドを実行 labelme2yolo --json_dir ./datasets/ --val_size 0.10 --output_format \u0026quot;bbox\u0026quot; これが済んだら，datasetsディレクトリの中にYOLODatasetが生成されているはずです．構造は以下のようになっています．\n.\r└── YOLODatasets/\r├── images/\r│ ├── train/\r│ │ ├── image_1.jpg\r│ │ └── ～省略～\r│ └── val/\r│ └── image_7.jpg\r├── labels/\r│ ├── train/\r│ │ ├── image_1.txt\r│ │ └── ～省略～\r│ └── val/\r│ └── image_7.txt\r└── dataset.yaml 先ほどのデータセット構造より複雑になっていますね．画像とラベルが別のフォルダに格納され，更にその中で訓練用(train)と評価用(val)に分かれています．\nlabelsディレクトリのテキストファイルを見てみましょう．YOLOのフォーマットに変換され，ラベルIDや座標が記録されているはずです．\ndataset.yamlの中身を見てみましょう．\nデータセットまでの絶対パス，訓練評価用ディレクトリのパス，ラベルIDとラベル名の対応が記載されています．YOLOの学習時にはこのdataset.yamlを利用してデータセットを読み込みます．\nYOLOの学習 それでは，いよいよ作ったデータセットを使ってYOLOの学習をしてみましょう！！\nまず，テスト用の画像を用意します．以下のリンクから画像ファイルimage_11.jpgをダウンロードしてdataset.yamlと同階層に配置します．\nテスト画像image_11.jpg dataset.yamlと同階層に.ipynbファイルを作成して，以下のPythonプログラムを実行してみましょう．\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolo11n.pt\u0026#34;) model.train(data=\u0026#39;dataset.yaml\u0026#39;, epochs=300, verbose=False) results = model(\u0026#34;image_11.jpg\u0026#34;) results[0].show() CPUのみの実行環境ではかなり時間がかかるので気長に待ちましょう～\n学習が終了すると，image_11.jpgに対する推論結果が表示されます．\n未検出や誤検出がちらほらありますが，部分的に検出が成功していることがわかります．データセットサイズやエポック数を変えることで性能の改善が期待できます．\nまた，データセットディレクトリのruns内を見ると，weightディレクトリにbest.ptが保存されていることがわかります． これは今回学習したモデルファイルで，実際に物体検出をシステム開発に組み込むときに使用します．\n","permalink":"http://localhost:1313/blog/posts/20250506_yoloannotation/","summary":"\u003ch2 id=\"アノテーションとは\"\u003eアノテーションとは？\u003c/h2\u003e\n\u003cp\u003e機械学習の分類のひとつである\u003cstrong\u003e教師あり学習\u003c/strong\u003eでは，まず入力データと出力データの組を複数用意し，それらを使って機械学習モデルを訓練します．\u003c/p\u003e\n\u003cp\u003e入力データと出力データの組は，タスクによって様々です．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e入力：画像データ，出力：それぞれが犬or猫\u003c/li\u003e\n\u003cli\u003e入力：家の情報（間取り，立地，築年数など），出力：家の価格\u003c/li\u003e\n\u003cli\u003e入力：これまでの気温，出力：明日の気温\u003c/li\u003e\n\u003cli\u003e入力：画像データ，出力：各物体の座標\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eどんなタスクを前提とする場合でも，これら\u003cstrong\u003e入力データと出力データの組を作る作業\u003c/strong\u003eが必要となります．これが\u003cstrong\u003eアノテーション\u003c/strong\u003eです．\u003c/p\u003e\n\u003ch2 id=\"環境構築\"\u003e環境構築\u003c/h2\u003e\n\u003cp\u003e今回はYOLOによる画像物体認識を目標とします．そこで，アノテーションで作るべきデータは以下になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e入力：画像データ，出力：各物体の座標\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e頑張れば用意した画像の座標を数えてアノテーションできそうですが，今回はより簡単に作業を進めるために\u003cstrong\u003eLabelme\u003c/strong\u003eというツールを使います．GUIが整備されたソフトウェアですが，Pythonライブラリとして無償で提供されています！（ありがたい！！）\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAnaconda Promptを開く\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda create -n annot_env python=3.12\u003c/code\u003eで仮想環境を作成\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate annot_env\u003c/code\u003eで仮想環境を有効化\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install --upgrade labelme\u003c/code\u003eと入力しLabelmeをインストール\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eこれでインストールは完了です．そのまま\u003ccode\u003eannot_env\u003c/code\u003e環境内で\u003ccode\u003elabelme\u003c/code\u003eコマンドを実行してみてください！\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"labelme.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこの画面が表示されたら成功です！\u003c/p\u003e\n\u003ch2 id=\"いざアノテーション\"\u003eいざ，アノテーション\u003c/h2\u003e\n\u003ch3 id=\"画像読み込み\"\u003e画像読み込み\u003c/h3\u003e\n\u003cp\u003e今回は画像から麻雀牌を物体認識するタスクに挑戦してみます！サンプル画像は以下からダウンロード，解凍してください．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hakoshi-normal/yolo_sample/raw/refs/heads/main/images/images.zip\"\u003eサンプル画像\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eZipファイルの解凍が済んだらLabelmeの画面左側にある\u003ccode\u003eOpen Dir\u003c/code\u003eから，画像のディレクトリを開いてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"labelme_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこれでアノテーションの準備は完了です！\u003c/p\u003e\n\u003ch3 id=\"範囲選択ラベル付け\"\u003e範囲選択，ラベル付け\u003c/h3\u003e\n\u003cp\u003eアノテーションの流れは以下になります．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e画像を右クリック，\u003ccode\u003eCreate Rectangle\u003c/code\u003eをクリック\u003c/li\u003e\n\u003cli\u003e物体の範囲を四角形で囲んで選択，間違えたら\u003ccode\u003eEsc\u003c/code\u003eキーで戻る\u003c/li\u003e\n\u003cli\u003e出てきたポップアップの上部\u003ccode\u003eEnter object label\u003c/code\u003eにラベルを入力して\u003ccode\u003eOK\u003c/code\u003eをクリック\u003c/li\u003e\n\u003cli\u003e2と3を物体の数だけひたすら繰り返す\u003c/li\u003e\n\u003cli\u003e画像内のアノテーションが終わったら画面左部\u003ccode\u003eSave\u003c/code\u003eから画像と同名のJSONファイルを保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eちょっとやってみますね～\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"annotated.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eはい，できました．ラベルは牌の種類（萬子筒子索子字牌）の略称です．\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 150%; color: red;\"\u003eめんどくさい\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e機械学習では一般的にデータの数が大きいほど性能が上がりやすいとされています．つまり，（要件の要求レベルにもよりますが）精度を上げるためにはたくさんアノテーションする必要があるんです．\u003c/p\u003e\n\u003cp\u003eそこで，特定のタスクの機械学習を考える場合はアノテーションに取り掛かる前に，データセットが利用可能なライセンスで公開されていないかどうか調べるところからスタートします．例えばYOLOの場合，\u003ca href=\"https://universe.roboflow.com/\"\u003eRoboflow\u003c/a\u003e，\u003ca href=\"https://visualdata.io/discovery\"\u003eVisualData\u003c/a\u003e，\u003ca href=\"https://storage.googleapis.com/openimages/web/index.html\"\u003eGoogle Open Images Dataset\u003c/a\u003eなどが画像データセットを公開している他，タスクに応じて論文と併せて公開されているデータセットも多数あります．\u003c/p\u003e\n\u003cp\u003eまた今回はアノテーション入門ということで手作業での方法を紹介していますが，画像認識AIによる半自動ツールもあるので興味があれば調べてみてください．\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e今回は，なんと，特別に，アノテーション済みファイルを用意してあります！\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hakoshi-normal/yolo_sample/raw/refs/heads/main/images/labels.zip\"\u003eアノテーション結果（JSON）\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eZIPファイルをダウンロード＆解凍したら，\u003ccode\u003edatasets\u003c/code\u003eという名前のディレクトリを作成し，先程の画像ファイルとJSONファイルを以下の構造で配置してください．（\u003ccode\u003eimage_1.json\u003c/code\u003eは自作のファイルで構いません！）\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-planetext\" data-lang=\"planetext\"\u003e.\r\n└── datasets/\r\n    ├── image_1.jpg\r\n    ├── image_1.json\r\n    ├── image_2.jpg\r\n    ├── image_2.json\r\n    ├── 〜省略〜\r\n    ├── image_10.jpg\r\n    └── image_10.json\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"データセットフォーマットの変換\"\u003eデータセットフォーマットの変換\u003c/h3\u003e\n\u003cp\u003eでは，学習させるデータの中身を一度見てみましょう！\u003ccode\u003eimage_1.json\u003c/code\u003eを開いてみます．\u003c/p\u003e","title":"【入門3】YOLOアノテーションと学習"},{"content":"はじめに 自動採譜プロジェクトの息抜き的な記事です．\nせっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして， ところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．\nカメラ切り替えトリガー カメラの切り替えトリガーを「牌山にプレーヤーが触れたタイミング」と設定しました．\n自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．\nツモ位置の検出 まず，天カメの映像から卓上のツモ位置を特定する必要があります．\n牌山の形を見てみましょう．\n対局開始前はこんな感じ．\n対局開始直後はこんな感じ．\n対局中はこんな感じ．\n対局開始前は4つ牌山がありますが，対局開始以降は牌山が必ず3つ以下になります王牌の扱い上，4つのときもあります．\n牌山が3つのとき\n牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．\n牌山が4つのとき\n一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．\n実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．\n牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．\n実装概要 元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます． cv2.inRange()を使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．\n二値化した画像に対し，cv2.findContours()を使用し，牌山となりうる矩形を検出します．\n手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます． その後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．\nツモプレイヤーの検出 ツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します． MediaPipe のハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します． 今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．\nプレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．\nオートスイッチャー実装！！！ OBSを使用して配信するため，カメラ切り替えには obs-websocket-py を使用しました．こちらの サンプル がそのまま動作して助かりました．\nGUI実装にはHTML/JSでの記述が可能な Eel を使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．\n実際の動作の様子がこちら．\nYour browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．\nあと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．\n","permalink":"http://localhost:1313/blog/posts/20240210_mahjongautocamswitcher/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e自動採譜プロジェクト\u003c/a\u003eの息抜き的な記事です．\u003c/p\u003e\n\u003cp\u003eせっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして，\nところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．\u003c/p\u003e\n\u003ch2 id=\"カメラ切り替えトリガー\"\u003eカメラ切り替えトリガー\u003c/h2\u003e\n\u003cp\u003eカメラの切り替えトリガーを「\u003cstrong\u003e牌山にプレーヤーが触れたタイミング\u003c/strong\u003e」と設定しました．\u003c/p\u003e\n\u003cp\u003e自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．\u003c/p\u003e\n\u003ch2 id=\"ツモ位置の検出\"\u003eツモ位置の検出\u003c/h2\u003e\n\u003cp\u003eまず，天カメの映像から卓上のツモ位置を特定する必要があります．\u003cbr\u003e牌山の形を見てみましょう．\u003c/p\u003e\n\u003cp\u003e対局開始前はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_0.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局開始直後はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局中はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_2.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局開始前は4つ牌山がありますが，対局開始以降は牌山が\u003cdel\u003e必ず3つ以下になります\u003c/del\u003e王牌の扱い上，4つのときもあります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e牌山が3つのとき\u003c/p\u003e\n\u003cp\u003e牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e牌山が4つのとき\u003c/p\u003e\n\u003cp\u003e一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"fig1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．\u003c/p\u003e\n\u003cp\u003e牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．\u003c/p\u003e\n\u003ch3 id=\"実装概要\"\u003e実装概要\u003c/h3\u003e\n\u003cp\u003e元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます．\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"original_image.png\"/\u003e \r\n\u003c/figure\u003e\r\n\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecv2.inRange()\u003c/code\u003eを使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"mask_hai.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e二値化した画像に対し，\u003ccode\u003ecv2.findContours()\u003c/code\u003eを使用し，牌山となりうる矩形を検出します．\u003c/p\u003e\n\u003cp\u003e手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます．\nその後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumohai.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"ツモプレイヤーの検出\"\u003eツモプレイヤーの検出\u003c/h2\u003e\n\u003cp\u003eツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します．\n\u003ca href=\"https://developers.google.com/mediapipe/solutions/vision/hand_landmarker\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eMediaPipe\u003c/a\u003e\nのハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します．\n今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumo1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eプレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumo2.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"オートスイッチャー実装\"\u003eオートスイッチャー実装！！！\u003c/h2\u003e\n\u003cp\u003eOBSを使用して配信するため，カメラ切り替えには\n\u003ca href=\"https://github.com/Elektordi/obs-websocket-py/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eobs-websocket-py\u003c/a\u003e\nを使用しました．こちらの\n\u003ca href=\"https://github.com/Elektordi/obs-websocket-py/blob/master/samples/switch_scenes.py\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eサンプル\u003c/a\u003e\nがそのまま動作して助かりました．\u003c/p\u003e\n\u003cp\u003eGUI実装にはHTML/JSでの記述が可能な\n\u003ca href=\"https://github.com/python-eel/Eel\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eEel\u003c/a\u003e\nを使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．\u003c/p\u003e\n\u003cp\u003e実際の動作の様子がこちら．\u003c/p\u003e\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\n\u003cp\u003e現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．\u003c/p\u003e\n\u003cp\u003eあと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．\u003c/p\u003e","title":"麻雀放送対局用のオートスイッチャーを作りたい"},{"content":"はじめに 複数カメラの入力に対しYOLOv8で並列推論するのソースコード解説です． 解説のため，一部表現を変えている部分があります．\n大まかな流れは以下の通り．\n共有メモリ確保 フレーム入力プロセス カメラからフレームを取得 フレームを共有メモリ1に保存 推論プロセス 共有フレームを呼び出し 推論 推論結果を描画したフレームを共有メモリ2に保存 フレーム表示プロセス 共有フレームを呼び出し フレームの画面描画 プロセス間の遷移はmultiprocessingモジュールのEvent関数をフラグとして利用します． カメラの台数が増えた場合はプロセス数を増やせば対応できます．\n事前準備 設定情報 cam_N = 4 # カメラ台数 proccess_N = 3 # カメラ1台あたりのプロセス数 frame_info = {\u0026#34;nbytes\u0026#34; : frame.nbytes, \u0026#34;shape\u0026#34; : frame.shape, \u0026#34;dtype\u0026#34; : frame.dtype} フレームのサイズや型はメモリ確保，呼び出し時に必要になります．\n共有メモリの確保 shm = shared_memory.SharedMemory(create=True, size=frame.nbytes, name = mem_name) 共有メモリの確保はこの一文でできます．めっちゃ便利ですね． ただし，mem_nameを動的に作成して複数の共有メモリを確保する場合，注意事項があります． shared_memory.SharedMemoryの戻り値を格納する変数（上記コードではshm）を設定しなかった場合や上書きした場合，それ以前の共有メモリ領域は参照できなくなります．その場合はshmを格納するリストを用意しておくとアクセス可能になります．\nプロセスの立ち上げ p = Process(target=関数名, args=(引数,)) p.start() # プロセス開始 p.join() # プロセス終了 複数プロセスを立ち上げる場合は p をリストに入れておくと管理が楽です．\nフレーム入力プロセス def rec_cam(cam_id, pre_flags, frame_info): cap = cv2.VideoCapture(cam_id) while True: _, frame = cap.read() idx = (cam_id)*proccess_N+counter%proccess_N mem_name = f\u0026#39;shared{idx+1:02}\u0026#39; shm = shared_memory.SharedMemory(name=mem_name) frame_sh = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) frame_sh[:] = frame[:] # 推論開始 pre_flags[idx].set() cap.close() multiprocessingのSharedMemoryでは共有メモリを名前をつけて呼び出すことができます．便利！ ここでは，\u0026ldquo;shared1[番号]\u0026ldquo;という名前で共有メモリを確保している（後述）．カメラ台数✕カメラあたりのプロセス数分の共有メモリ領域が必要になるため， あるカメラのあるフレームに対する保存領域は，カメラ番号✕カメラ1台あたりのプロセス数＋フレーム番号%カメラ1台あたりのプロセス数で求めることができる．と同時にこの番号は推論プロセスを動作させるフラグ管理のインデックスとしても使う．\n推論プロセス def predict_frame(pre_flag, view_flag, mem_name, cam_id, frame_info): while True: if pre_flag.is_set(): # 推論開始 pre_flag.clear() shm = shared_memory.SharedMemory(name=mem_name) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # 重い推論処理がここに入る（frameに結果を反映する） # 結果統合用共有メモリにのせる shm_result = shared_memory.SharedMemory(name=f\u0026#39;shared2{cam_id}\u0026#39;) frame_new = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm_result.buf) frame_new[:]=frame[:] # 結果表示 view_flag.set() カメラ台数✕カメラあたりのプロセス数分の推論プロセスを立ち上げます． 推論開始用のフラグや共有メモリの呼び出し名は予め与えた状態でプロセスを立ち上げる．フレーム入力プロセスでの推論開始用フラグの有効化を待ち受けています． 処理済みのフレーを格納する共有メモリ領域名は\u0026rsquo;shared2[番号]\u0026lsquo;としており，カメラの台数分事前に確保してあります． 検出結果を描画する必要がない場合は，フレーム表示プロセスを介する必要がないため，別プロセスに推論結果を渡したり，UDP通信などで端末間の結果送信が実装できると思います．\nフレーム表示プロセス def view_frame(view_flag, cam_id, frame_info): while True: if view_flag.is_set(): view_flag.clear() shm = shared_memory.SharedMemory(name=f\u0026#39;shared2{cam_id}\u0026#39;) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) cv2.imshow(f\u0026#39;window_{cam_id}\u0026#39;, frame) cv2.waitKey(1) 推論プロセスでのフレーム表示用フラグの有効化を待ち受けています．\nおわりに 今回は，以前実装した並列画像処理プログラムの解説でした．コードブロックを断片的に示しただけなので，具体的な実装コードを知りたい方はソースコードから参照できます．\n","permalink":"http://localhost:1313/blog/posts/20231226_multiprocessingforrealtimevideo/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/2023_12_01_multiprocessingforyolov8/\" target=\"_blank\"\u003e複数カメラの入力に対しYOLOv8で並列推論する\u003c/a\u003eのソースコード解説です．\n解説のため，一部表現を変えている部分があります．\u003c/p\u003e\n\u003cp\u003e大まかな流れは以下の通り．\u003c/p\u003e\n\u003col start=\"0\"\u003e\n\u003cli\u003e共有メモリ確保\u003c/li\u003e\n\u003cli\u003eフレーム入力プロセス\n\u003col\u003e\n\u003cli\u003eカメラからフレームを取得\u003c/li\u003e\n\u003cli\u003eフレームを共有メモリ1に保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e推論プロセス\n\u003col\u003e\n\u003cli\u003e共有フレームを呼び出し\u003c/li\u003e\n\u003cli\u003e推論\u003c/li\u003e\n\u003cli\u003e推論結果を描画したフレームを共有メモリ2に保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eフレーム表示プロセス\n\u003col\u003e\n\u003cli\u003e共有フレームを呼び出し\u003c/li\u003e\n\u003cli\u003eフレームの画面描画\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eプロセス間の遷移はmultiprocessingモジュールのEvent関数をフラグとして利用します．\nカメラの台数が増えた場合はプロセス数を増やせば対応できます．\u003c/p\u003e\n\u003ch2 id=\"事前準備\"\u003e事前準備\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e設定情報\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecam_N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# カメラ台数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eproccess_N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# カメラ1台あたりのプロセス数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eframe_info \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;nbytes\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enbytes,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;shape\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshape,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dtype\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edtype}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eフレームのサイズや型はメモリ確保，呼び出し時に必要になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e共有メモリの確保\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eshm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e shared_memory\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSharedMemory(create\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enbytes, name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e mem_name)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e共有メモリの確保はこの一文でできます．めっちゃ便利ですね．\nただし，mem_nameを動的に作成して複数の共有メモリを確保する場合，注意事項があります．\nshared_memory.SharedMemoryの戻り値を格納する変数（上記コードではshm）を設定しなかった場合や上書きした場合，それ以前の共有メモリ領域は参照できなくなります．その場合はshmを格納するリストを用意しておくとアクセス可能になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプロセスの立ち上げ\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Process(target\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e関数名, args\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e(引数,))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estart() \u003cspan style=\"color:#75715e\"\u003e# プロセス開始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ejoin() \u003cspan style=\"color:#75715e\"\u003e# プロセス終了\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e複数プロセスを立ち上げる場合は p をリストに入れておくと管理が楽です．\u003c/p\u003e\n\u003ch2 id=\"フレーム入力プロセス\"\u003eフレーム入力プロセス\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erec_cam\u003c/span\u003e(cam_id, pre_flags, frame_info):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cv2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eVideoCapture(cam_id)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            _, frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            idx \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (cam_id)\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003eproccess_N\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003ecounter\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003eproccess_N\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            mem_name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;shared\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003eidx\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e02\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            shm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e shared_memory\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSharedMemory(name\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003emem_name)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            frame_sh \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003endarray(shape\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;shape\u0026#34;\u003c/span\u003e], dtype\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dtype\u0026#34;\u003c/span\u003e], buffer\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eshm\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebuf)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            frame_sh[:] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e frame[:]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# 推論開始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            pre_flags[idx]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eset()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003emultiprocessingのSharedMemoryでは共有メモリを名前をつけて呼び出すことができます．便利！\nここでは，\u0026ldquo;shared1[番号]\u0026ldquo;という名前で共有メモリを確保している（後述）．カメラ台数✕カメラあたりのプロセス数分の共有メモリ領域が必要になるため，\nあるカメラのあるフレームに対する保存領域は，カメラ番号✕カメラ1台あたりのプロセス数＋フレーム番号%カメラ1台あたりのプロセス数で求めることができる．と同時にこの番号は推論プロセスを動作させるフラグ管理のインデックスとしても使う．\u003c/p\u003e","title":"リアルタイムの映像入力に対してFPSを落とさずに画像処理したい"},{"content":"ビデオカメラからOpenCVで映像取得（低品質） 麻雀自動採譜の実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．\nビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できるビデオキャプチャカードを導入． これでビデオカメラをWebカメラみたく使えるぞ！やった！！\nと思った矢先，\nなんか遅い気がする！！\nよくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，1秒遅延＋低FPS＋低画質という散々な結果に\nお前 Full HD で 30FPS 出るって言ってたじゃないか\u0026hellip;\u0026hellip;\nキャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．\nRTSP通信でカメラの映像を受信してみる\nこのときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった． 今回もPyAVを使えば解決するかもしれない．\nビデオカメラからPyAVで映像取得 PyAVでPCに有線接続されたカメラから映像を取得する．\nimport cv2 import av con_options = dict( video_size=\u0026#39;1920x1080\u0026#39;, vcodec=\u0026#39;mjpeg\u0026#39;, framerate=\u0026#39;30\u0026#39;, rtbufsize=\u0026#39;1\u0026#39;, ) device_name = \u0026#34;USB Video\u0026#34; con_def = dict( format=\u0026#39;dshow\u0026#39;, file=f\u0026#39;video={device_name}\u0026#39;, options=dict(con_options, video_device_number=\u0026#39;0\u0026#39;) ) container = av.open(**con_def) for frame in container.decode(video=0): frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) # cv2.imshow(\u0026#34;test_window\u0026#34;, frame) # cv2.waitKey(1) ちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．\nffmpeg -list_devices true -f dshow -i dummy PyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．\nこれで遅延ほぼなし，30FPSでの映像受信ができた．\n","permalink":"http://localhost:1313/blog/posts/20231225_getframebypyav/","summary":"\u003ch2 id=\"ビデオカメラからopencvで映像取得低品質\"\u003eビデオカメラからOpenCVで映像取得（低品質）\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003eの実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．\u003c/p\u003e\n\u003cp\u003eビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できる\u003ca href=\"https://amzn.asia/d/jclJASJ\"\u003eビデオキャプチャカード\u003c/a\u003eを導入．\nこれでビデオカメラをWebカメラみたく使えるぞ！やった！！\u003c/p\u003e\n\u003cp\u003eと思った矢先，\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eなんか遅い気がする！！\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eよくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，\u003cstrong\u003e1秒遅延＋低FPS＋低画質\u003c/strong\u003eという散々な結果に\u003c/p\u003e\n\u003cp\u003eお前 Full HD で 30FPS 出るって言ってたじゃないか\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003eキャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/blog/2023_12_01_getframertsp/\"\u003eRTSP通信でカメラの映像を受信してみる\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこのときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった．\n今回も\u003ca href=\"https://pyav.org/docs/develop/index.html\" target=\"_blank\"\u003ePyAV\u003c/a\u003eを使えば解決するかもしれない．\u003c/p\u003e\n\u003ch2 id=\"ビデオカメラからpyavで映像取得\"\u003eビデオカメラからPyAVで映像取得\u003c/h2\u003e\n\u003cp\u003ePyAVでPCに有線接続されたカメラから映像を取得する．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e cv2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e av\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econ_options  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  dict(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    video_size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1920x1080\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    vcodec\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;mjpeg\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    framerate\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;30\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    rtbufsize\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edevice_name  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;USB Video\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econ_def  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  dict(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;dshow\u0026#39;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    file\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;video=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003edevice_name\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    options\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003edict(con_options, video_device_number\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;0\u0026#39;\u003c/span\u003e) )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econtainer \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e av\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eopen(\u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003econ_def)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e frame \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e container\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edecode(video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto_ndarray(format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;bgr24\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# cv2.imshow(\u0026#34;test_window\u0026#34;, frame)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# cv2.waitKey(1)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-console\" data-lang=\"console\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003effmpeg -list_devices true -f dshow -i dummy\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．\u003c/p\u003e","title":"映像入力の遅延をPyAVで解決してみる"},{"content":" multiprocessing × YOLOv8の詳細解説 Denoを利用した基本的なWebサイト作成 Deno Deploy設定 ","permalink":"http://localhost:1313/blog/posts/20231212_scheduled_update/","summary":"\u003cul\u003e\n\u003cli\u003emultiprocessing × YOLOv8の詳細解説\u003c/li\u003e\n\u003cli\u003eDenoを利用した基本的なWebサイト作成\u003c/li\u003e\n\u003cli\u003eDeno Deploy設定\u003c/li\u003e\n\u003c/ul\u003e","title":"今後の更新予定"},{"content":"RTSPでフレーム取得（OpenCV） RTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．\nimport cv2 url = \u0026#34;rtsp://username:password@ipaddress\u0026#34; # Tapo C200は末尾に画質を指定 url = \u0026#34;rtsp://username:password@ipaddress/stream1\u0026#34; cap = cv2.VideoCapture(url) while True: ret, frame = cap.read() pass ただこれだとTapo C200では1秒程度遅延が発生してしまう． 麻雀自動採譜 の実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．\nRTSPでフレーム取得（PyAV） RTSP映像の受信にはFFmpegのPythonバインディングである PyAV を利用した方法もあり、以下で映像受信が可能でした．\nimport av\rurl = \u0026#34;rtsp://username:password@ipaddress\u0026#34;\rcontainer = av.open(url)\rfor frame in container.decode(video=0):\rframe = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;)\rpass PyAVを利用した場合は遅延が大幅に減少した．やったね．\nRTSPで一定時間毎にフレーム取得（PyAV） 麻雀自動採譜 牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚でtime.sleep(n)を使用したところ以下のエラーが複数出現．\nmax delay reached. need to consume packet\rRTP: missed 6066 packets\rRTP: PT=60: bad cseq a657 expected=8ea5\rmax delay reached. need to consume packet sleepで無理に処理を中断させたため接続が不安定になったのかも． 通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．\nimport av interval = 10 # 10秒ごとに撮影 url = \u0026#34;rtsp://username:password@ipaddress\u0026#34; container = av.open(url) counter = 0 for frame in container.decode(video=0): if frame is not None and frame.time//interval\u0026gt;counter: counter+=1 frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) cv2.imwrite(f\u0026#39;save_{counter:04}.png\u0026#39;, frame) PyAVのcontainerから呼び出したフレームは撮影開始を0としたときの撮影時刻をtimeで取得できる． 撮影開始後数フレームはNoneが返ることがあるため注意． これで安定したn秒毎の撮影が実行できる．\n","permalink":"http://localhost:1313/blog/posts/20231201_getframertsp/","summary":"\u003ch2 id=\"rtspでフレーム取得opencv\"\u003eRTSPでフレーム取得（OpenCV）\u003c/h2\u003e\n\u003cp\u003eRTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e cv2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rtsp://username:password@ipaddress\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Tapo C200は末尾に画質を指定\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rtsp://username:password@ipaddress/stream1\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cv2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eVideoCapture(url)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ret, frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eただこれだとTapo C200では\u003cstrong\u003e1秒程度遅延\u003c/strong\u003eが発生してしまう．\n\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003e\nの実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．\u003c/p\u003e\n\u003ch2 id=\"rtspでフレーム取得pyav\"\u003eRTSPでフレーム取得（PyAV）\u003c/h2\u003e\n\u003cp\u003eRTSP映像の受信にはFFmpegのPythonバインディングである\n\u003ca href=\"https://pyav.org/docs/develop/index.html\" target=\"_blank\"\u003ePyAV\u003c/a\u003e\nを利用した方法もあり、以下で映像受信が可能でした．\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-python:\" data-lang=\"python:\"\u003eimport av\r\n\r\nurl = \u0026#34;rtsp://username:password@ipaddress\u0026#34;\r\ncontainer = av.open(url)\r\nfor frame in container.decode(video=0):\r\n    frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;)\r\n    pass\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ePyAVを利用した場合は遅延が大幅に減少した．やったね．\u003c/p\u003e\n\u003ch2 id=\"rtspで一定時間毎にフレーム取得pyav\"\u003eRTSPで一定時間毎にフレーム取得（PyAV）\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003e\n牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚で\u003ccode\u003etime.sleep(n)\u003c/code\u003eを使用したところ以下のエラーが複数出現．\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-raw:\" data-lang=\"raw:\"\u003emax delay reached. need to consume packet\r\nRTP: missed 6066 packets\r\nRTP: PT=60: bad cseq a657 expected=8ea5\r\nmax delay reached. need to consume packet\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003esleepで無理に処理を中断させたため接続が不安定になったのかも．\n通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．\u003c/p\u003e","title":"RTSP通信でカメラの映像を受信してみる"},{"content":"実装過程あれこれ１ これまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の Tapo C200 からフレームを受信することができるようになりました． 早速各カメラに割り当てたプロセス毎に適当に学習させておいた YOLOv8 による牌推論を実行し，結果を描画してみました．わくわく！\nはい，映像出力が不穏な感じに． Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．\nプロセス間でまったく映像出力のタイミングがあっていない フレーム間隔がまちまち 映像の乱れが酷い FPSが低い（Tapo入力時15→10弱） 特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね\u0026hellip;\u0026hellip;\nフレーム出力がばらつく理由を勝手に予想．こんな感じだろうか\n出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える． こんなときこそマルチプロセス処理の出番でしょ↓\n（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）\n既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．\n映像受信プロセス ×4（カメラの台数） 推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当） 推論結果統合プロセス ×4（カメラの台数） さあやるぞ！\n実装過程あれこれ２ ここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．\nbase64→内部UDPが一瞬頭を過るが無視して 公式ドキュメント を漁ってみます．\nドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか． Value, Array, Queue, Pipe, RawArray, Manager, shared_memory\u0026hellip;\u0026hellip;\n多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．\n試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに． フラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．\nそんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります． Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r概ね同じタイミングでフレームが描画されているように見えますね．\nFPSはいずれのカメラも15程度であり，入力時のFPSを保持したままリアルタイムで推論が実行できました． ちなみに，\nmultiprocessingではなくthreadingを使ってYOLOv8のモデルをスレッド間で共有する方法があるが，提供元である UltralyticsのMedium記事 では推奨方法， 公式ドキュメント では非推奨方法として紹介されていたため今回は見送り． 試験的に使用してるYOLOv8モデルは，現環境（GeForce RTX 3070）で学習可能な最大サイズのモデル（YOLOv8l）に対して複数の公開データセットを統合した5万枚強の牌画像を学習させたものである．統合作業時にデータセット間でラベルのズレが発生したため，検出結果に誤りがある．ラベルが混合した状態のデータセットではあるが安定した検出ができているため，データセットの整理によって更に高精度な検出の実現が期待できる． 背景部分の予期せぬ牌検出は信頼度で棄却できるため問題なし． 検証動画は双方画面録画の仕様上FPSが低下している． ソースコード 複数台のカメラのフレーム入力に対してYOLOv8による推論を並列実行するPythonプログラム ソースコード解説はこちらからどうぞ．\nimport time import msvcrt from multiprocessing import Process, Event, shared_memory import numpy as np import av import cv2 from ultralytics import YOLO # RTSPカメラ設定情報 info_list = [[\u0026#34;toncamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;nancamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;shacamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;peicamera\u0026#34;, \u0026#34;*\u0026#34;]] password = \u0026#34;*\u0026#34; # 1:1280x720, 2:640x360 mode = 1 # 推論プロセス数指定 N = 3 def rec_cam(cam_id, username, ipaddress, pre_flags, stop_flag, frame_info): url = f\u0026#34;rtsp://{username}:{password}@{ipaddress}:554/stream{mode}\u0026#34; container = av.open(url) counter = 0 start = time.time() for frame in container.decode(video=0): counter+=1 frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) # 共有メモリにフレームを書き込み idx = (cam_id)*N+counter%N mem_name = f\u0026#39;shared{idx+1:02}\u0026#39; shm = shared_memory.SharedMemory(name=mem_name) frame_sh = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) frame_sh[:] = frame[:] # 推論開始 pre_flags[idx].set() if stop_flag.is_set(): t = time.time() - start print(f\u0026#34;{username}, TIME:{round(t, 3)}, FRAME:{counter}, FPS:{round(counter/t, 3)}\u0026#34;) break container.close() cv2.destroyAllWindows() return def finish_monitor(stop_flag): while True: if msvcrt.kbhit() and msvcrt.getch() == b\u0026#39;q\u0026#39;: print(\u0026#34;Finished\u0026#34;) stop_flag.set() break return def predict_frame(pre_flag, pre_fin_flag, mem_name, stop_flag, username, frame_info): model = YOLO(\u0026#39;best.pt\u0026#39;) while True: if pre_flag.is_set(): # 推論 pre_flag.clear() shm = shared_memory.SharedMemory(name=mem_name) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # YOLOv8による推論実行 result = model(frame)[0] frame = result.plot() # 結果統合用共有メモリにのせる shm_result = shared_memory.SharedMemory(name=username) frame_new = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm_result.buf) frame_new[:]=frame[:] # 結果統合用 pre_fin_flag.set() if stop_flag.is_set(): break def view_frame(pre_fin_flag, username, stop_flag, frame_info): while True: if pre_fin_flag.is_set(): pre_fin_flag.clear() shm = shared_memory.SharedMemory(name=username) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # Full-HDだと大抵画面が埋まっちゃう # frame = cv2.resize(frame, None, None, 0.6, 0.6) cv2.imshow(username, frame) cv2.waitKey(1) if stop_flag.is_set(): break if __name__ == \u0026#39;__main__\u0026#39;: # メモリ確保情報 url = f\u0026#34;rtsp://{info_list[0][0]}:{password}@{info_list[0][1]}:554/stream{mode}\u0026#34; container = av.open(url) for i, frame in enumerate(container.decode(video=0)): frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) if frame is not None: break container.close() frame_info = {\u0026#34;nbytes\u0026#34; : frame.nbytes, \u0026#34;shape\u0026#34; : frame.shape, \u0026#34;dtype\u0026#34; : frame.dtype} # 停止プロセス stop_flag = Event() p = Process(target=finish_monitor, args=(stop_flag,)) p.start() # 表示プロセス群（カメラ台数分） view_processes = [] pre_fin_flags = [] mem_space = [] for i in range(4): pre_fin_flag = Event() pre_fin_flags.append(pre_fin_flag) username = info_list[i][0] # メモリ確保 今回は画像用 shm = shared_memory.SharedMemory(create=True, size=frame_info[\u0026#34;nbytes\u0026#34;], name = username) mem_space.append(shm) p = Process(target=view_frame, args=(pre_fin_flag, username, stop_flag, frame_info)) view_processes.append(p) p.start() # 推論プロセス群（カメラ台数×推論プロセス数） pre_processes = [] pre_flags = [] mem_names = [] for i in range(N*len(info_list)): pre_flag = Event() pre_flags.append(pre_flag) mem_name = f\u0026#39;shared{i+1:02}\u0026#39; mem_names.append(mem_name) # メモリ確保 shm = shared_memory.SharedMemory(create=True, size=frame_info[\u0026#34;nbytes\u0026#34;], name = mem_name) mem_space.append(shm) # frame_sh = np.ndarray(shape=frame_shape, dtype=frame_dtype, buffer=shm.buf) username = info_list[i//N][0] pre_fin_flag = pre_fin_flags[i//N] p = Process(target=predict_frame, args=(pre_flag, pre_fin_flag, mem_name, stop_flag, username, frame_info)) pre_processes.append(p) p.start() # 撮影プロセス群（カメラ台数） rec_processes = [] for cam_id, info in enumerate(info_list): p = Process(target=rec_cam, args=(cam_id, info[0], info[1], pre_flags, stop_flag, frame_info,)) rec_processes.append(p) p.start() p.join() for p in view_processes: p.join() for p in pre_processes: p.join() for p in rec_processes: p.join() # 共有メモリのリソース開放、削除 for mem_name in mem_names + [info[0] for info in info_list[:4]]: shm = shared_memory.SharedMemory(name = mem_name) shm.close() shm.unlink() ","permalink":"http://localhost:1313/blog/posts/20231201_multiprocessingforyolov8/","summary":"\u003ch2 id=\"実装過程あれこれ１\"\u003e実装過程あれこれ１\u003c/h2\u003e\n\u003cp\u003eこれまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の\n\u003ca href=\"https://www.tp-link.com/jp/smart-home/tapo/tapo-c200/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eTapo C200\u003c/a\u003e\nからフレームを受信することができるようになりました．\n\u003cbr\u003e\n早速各カメラに割り当てたプロセス毎に適当に学習させておいた\n\u003ca href=\"https://docs.ultralytics.com/ja/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eYOLOv8\u003c/a\u003e\nによる牌推論を実行し，結果を描画してみました．わくわく！\u003c/p\u003e\n\u003cp\u003eはい，映像出力が不穏な感じに．\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20231201_multiprocessingforyolov8/before.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20231201_multiprocessingforyolov8/before.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\u003c/p\u003e\n\u003cp\u003e卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプロセス間でまったく映像出力のタイミングがあっていない\u003c/li\u003e\n\u003cli\u003eフレーム間隔がまちまち\u003c/li\u003e\n\u003cli\u003e映像の乱れが酷い\u003c/li\u003e\n\u003cli\u003eFPSが低い（Tapo入力時15→10弱）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね\u0026hellip;\u0026hellip;\u003cbr\u003e\u003c/p\u003e\n\u003cp\u003eフレーム出力がばらつく理由を勝手に予想．こんな感じだろうか\u003c/p\u003e\n\u003cfigure\u003e\r\n    \u003cimg loading=\"lazy\" src=\"multiprocess_fig1.jpg\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える．\n\u003cbr\u003e\u003cbr\u003e\n\u003cspan style=\"font-size: 200%; color: red;\"\u003e\nこんなときこそマルチプロセス処理の出番でしょ↓\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\r\n    \u003cimg loading=\"lazy\" src=\"multiprocess_fig2.jpg\"/\u003e \r\n\u003c/figure\u003e\r\n\n（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）\u003c/p\u003e\n\u003cp\u003e既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e映像受信プロセス ×4（カメラの台数）\u003c/li\u003e\n\u003cli\u003e推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当）\u003c/li\u003e\n\u003cli\u003e推論結果統合プロセス ×4（カメラの台数）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eさあやるぞ！\u003c/p\u003e\n\u003ch2 id=\"実装過程あれこれ２\"\u003e実装過程あれこれ２\u003c/h2\u003e\n\u003cp\u003eここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．\u003cbr\u003e\nbase64→内部UDPが一瞬頭を過るが無視して\n\u003ca href=\"https://docs.python.org/ja/3.10/library/multiprocessing.html\" target=\"_blank\"\u003e公式ドキュメント\u003c/a\u003e\nを漁ってみます．\u003c/p\u003e\n\u003cp\u003eドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか．\nValue, Array, Queue, Pipe, RawArray, Manager, shared_memory\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．\u003c/p\u003e\n\u003cp\u003e試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに．\nフラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．\u003c/p\u003e\n\u003cp\u003eそんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります．\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20231201_multiprocessingforyolov8/after.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20231201_multiprocessingforyolov8/after.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\u003c/p\u003e","title":"複数カメラの入力に対しYOLOv8で並列推論する"},{"content":"ライブラリとは？ ライブラリとは，再利用可能なソフトウェア部品のことで，特定の機能をまとめたコードの再集合のことを指します．簡単に言えば，他の人が作ってくれた機能を自身のプログラムで使える便利なコード集のことです． Pythonには標準ライブラリと外部ライブラリの2種類があります．\n標準ライブラリ 標準ライブラリはPythonをインストールしたときに標準的に入っているライブラリのことです．通常のプログラム作成時に頻繁に必要となる機能をまとめたものが多くあります．例えば，\n乱数を作成する random 時刻を取得できる datetime, time ファイルシステムへのアクセスを実現する os, shutil 基礎的な数学関数 math HTMLサポート html URLを使ってインターネットにアクセスできる urllib スマートフォンで言うところのカメラ，アルバム，時計などのデフォルトアプリみたいなものです．これらはPythonが動く環境であれば勝手に入っているため，import ライブラリ名と記述すればすぐに利用することができます．助かる！！\n# サイコロの出目を出力 import random dice_roll = random.randint(1, 6) print(\u0026#34;サイコロの出目:\u0026#34;, dice_roll) 外部ライブラリ 外部ライブラリはPythonにはもともと組み込まれておらず，自身で導入するタイプのライブラリのことです．実装したいプログラムの機能に応じて適切に選択する必要があります．例えば，\n効率的な多次元配列の数値計算を実現する Numpy 画像処理の機能を提供する OpenCV グラフを簡単に描画できる Matplotlib 様々なアルゴリズムでの機械学習を可能とする scikit-learn Webアプリの開発に役立つ Django スマートフォンで言うところのゲームやチャットアプリなどのストアアプリみたいなものです．これらは自身でインストール作業をする必要がありますが，それさえ済ませば高機能な実装をいとも容易く実現できます！とても便利！！ 有志のプログラマが開発して公開してくれているため，感謝の気持ちを忘れずに使いましょう（Pythonもそうだけど）．\n# サイコロの出目を出力 import numpy as np dice_roll = np.random.randint(1, 7) print(\u0026#34;サイコロの出目:\u0026#34;, dice_roll) ライブラリを確認してみよう では，ここからは実際に手を動かしながら解説を進めます． まずはスタートメニューから Anaconda Prompt を開いてください．\nこれはコマンドプロンプトと呼ばれる画面で，ここにコマンドと呼ばれる命令文を入力することでコンピュータを操作できます． 試しに，ここにtreeと打ち込んでEnterキーを押してみてください．\n\u0026hellip;\u0026hellip;何かツリー構造の記述がたくさん出てきたかと思います．これはディレクトリ構造をツリー形式で表示するコマンドです（あんまり使わないけど）！ なかなか表示が終わらない場合はCtrl+Cキーでキャンセルしてください．\nではコマンドを使って，今使っているPythonで利用可能な外部ライブラリの一覧を見てみましょう！ Anaconda Promptにconda listと打ち込んでEnterキーを押してみてください．\nインストール済みライブラリとバージョンの一覧が表示されました．既にたくさん入っているようです！\nよくよく見てみると，数値計算ができるNumpyや表データを扱えるPandasなど，データサイエンスに役立つライブラリがたくさんあります． 実はAnacondaは科学計算のためのPython配布形式（ディストリビューション）の一つであり，既に役立つライブラリが揃えられているんです．\nこの状態，一見プログラムを書くのに便利な環境であるように見えるんですが実はそうでないケースがあります．\n実はライブラリは，機能を実現するために別のライブラリを参照するケースがあります．例えば，\n画像処理を実現するOpenCVには，画像を配列として扱うためにNumpyが必要 グラフを描画するMatplotlibには，画像描画のためにPillowが必要 機械学習を実現するscikit-learnには，高度な数値計算をするためにScipyが必要 またライブラリはバージョンによって提供される機能が変化することがあります．そのため，同じ実行環境に特定のバージョンを指定して参照ライブラリを導入しておく必要があるのです．\nこれを，ライブラリの依存関係と言います．この依存関係が原因で複数のライブラリを導入するときに問題が発生する可能性があるんです．\n例えば，ライブラリA-C及びB-Cの依存関係があるとき，AとBが共存するためにはCのバージョンが整合している必要があります．\nこうしたライブラリバージョンの試行錯誤を行う場合，先程表示させた複雑に依存関係が絡み合ったライブラリ群を扱うのはなかなか骨が折れます．\nまた，不要なライブラリは更に混乱を引き起こすので必要となる機能のための最小構成で開発環境を整える必要があります．\nそこで役立つのが，仮想環境です！！！（やっと本題）\n仮想環境 仮想環境とは，プログラムを動かすときに仮に作って利用する動作環境のことです． 簡単に言うと「依存関係の解決が面倒だから，まっさらな世界を一から創造すればいいじゃん！！」です．\nでは，実際に作ってみましょう．\n先ほど開いたAnaconda Promptにconda create -n test python=3.12と打ち込んで実行（Proceed([y]/n)?にはyを入力して処理を続行） conda activate testと打ち込んで実行 ここでは，testという名前の仮想環境を作成してその環境に切り替える，という作業をしてもらいました． 最新行の左側が(test)と表示されていれば成功です！\n先ほどまでは(base)と表示されていたはずです．Anacondaではデフォルトでbase環境があり，そこにたくさんのライブラリがありました． ではconda listで現在のtest環境のライブラリ構成を見てみましょう！\nbase環境と比べてライブラリの数がかなり少なくなったはずです．ここに表示されているライブラリはPython3.12に付随する必要最低限のライブラリです． このきれいな環境に最小限の適切なライブラリを導入することで，快適なコーディングライフを送ることができます！！\n番外編：Conda仮想環境の管理 複数の仮想環境を扱うときに便利なコマンドをまとめておきます！\n仮想環境の作成（環境名がtest，Python3.12の場合）\nconda create -n test python=3.12 仮想環境への移行\nconda activate test 仮想環境から離脱\nconda deactivate 仮想環境の削除\nconda remove -n test --all 仮想環境一覧の表示\nconda info -e ","permalink":"http://localhost:1313/blog/posts/20250506_condavenv/","summary":"\u003ch2 id=\"ライブラリとは\"\u003eライブラリとは？\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eライブラリ\u003c/strong\u003eとは，再利用可能なソフトウェア部品のことで，特定の機能をまとめたコードの再集合のことを指します．簡単に言えば，他の人が作ってくれた機能を自身のプログラムで使える便利なコード集のことです．\nPythonには\u003cstrong\u003e標準ライブラリ\u003c/strong\u003eと\u003cstrong\u003e外部ライブラリ\u003c/strong\u003eの2種類があります．\u003c/p\u003e\n\u003ch3 id=\"標準ライブラリ\"\u003e標準ライブラリ\u003c/h3\u003e\n\u003cp\u003e標準ライブラリはPythonをインストールしたときに標準的に入っているライブラリのことです．通常のプログラム作成時に頻繁に必要となる機能をまとめたものが多くあります．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e乱数を作成する random\u003c/li\u003e\n\u003cli\u003e時刻を取得できる datetime, time\u003c/li\u003e\n\u003cli\u003eファイルシステムへのアクセスを実現する os, shutil\u003c/li\u003e\n\u003cli\u003e基礎的な数学関数 math\u003c/li\u003e\n\u003cli\u003eHTMLサポート html\u003c/li\u003e\n\u003cli\u003eURLを使ってインターネットにアクセスできる urllib\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスマートフォンで言うところのカメラ，アルバム，時計などのデフォルトアプリみたいなものです．これらはPythonが動く環境であれば勝手に入っているため，\u003ccode\u003eimport ライブラリ名\u003c/code\u003eと記述すればすぐに利用することができます．助かる！！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# サイコロの出目を出力\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e random\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edice_roll \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e random\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandint(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;サイコロの出目:\u0026#34;\u003c/span\u003e, dice_roll)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"外部ライブラリ\"\u003e外部ライブラリ\u003c/h3\u003e\n\u003cp\u003e外部ライブラリはPythonにはもともと組み込まれておらず，自身で導入するタイプのライブラリのことです．実装したいプログラムの機能に応じて適切に選択する必要があります．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e効率的な多次元配列の数値計算を実現する Numpy\u003c/li\u003e\n\u003cli\u003e画像処理の機能を提供する OpenCV\u003c/li\u003e\n\u003cli\u003eグラフを簡単に描画できる Matplotlib\u003c/li\u003e\n\u003cli\u003e様々なアルゴリズムでの機械学習を可能とする scikit-learn\u003c/li\u003e\n\u003cli\u003eWebアプリの開発に役立つ Django\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスマートフォンで言うところのゲームやチャットアプリなどのストアアプリみたいなものです．これらは自身でインストール作業をする必要がありますが，それさえ済ませば高機能な実装をいとも容易く実現できます！とても便利！！\n有志のプログラマが開発して公開してくれているため，感謝の気持ちを忘れずに使いましょう（Pythonもそうだけど）．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# サイコロの出目を出力\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e np\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edice_roll \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandom\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandint(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;サイコロの出目:\u0026#34;\u003c/span\u003e, dice_roll)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"ライブラリを確認してみよう\"\u003eライブラリを確認してみよう\u003c/h2\u003e\n\u003cp\u003eでは，ここからは実際に手を動かしながら解説を進めます．\nまずはスタートメニューから \u003cstrong\u003eAnaconda Prompt\u003c/strong\u003e を開いてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"img_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"console_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこれは\u003cstrong\u003eコマンドプロンプト\u003c/strong\u003eと呼ばれる画面で，ここに\u003cstrong\u003eコマンド\u003c/strong\u003eと呼ばれる命令文を入力することでコンピュータを操作できます．\n試しに，ここに\u003ccode\u003etree\u003c/code\u003eと打ち込んでEnterキーを押してみてください．\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;何かツリー構造の記述がたくさん出てきたかと思います．これはディレクトリ構造をツリー形式で表示するコマンドです（あんまり使わないけど）！\nなかなか表示が終わらない場合はCtrl+Cキーでキャンセルしてください．\u003c/p\u003e\n\u003cp\u003eではコマンドを使って，今使っているPythonで利用可能な外部ライブラリの一覧を見てみましょう！\nAnaconda Promptに\u003ccode\u003econda list\u003c/code\u003eと打ち込んでEnterキーを押してみてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"modules.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eインストール済みライブラリとバージョンの一覧が表示されました．既にたくさん入っているようです！\u003c/p\u003e\n\u003cp\u003eよくよく見てみると，数値計算ができるNumpyや表データを扱えるPandasなど，データサイエンスに役立つライブラリがたくさんあります．\n実はAnacondaは科学計算のためのPython配布形式（ディストリビューション）の一つであり，既に役立つライブラリが揃えられているんです．\u003c/p\u003e\n\u003cp\u003eこの状態，一見プログラムを書くのに便利な環境であるように見えるんですが実はそうでないケースがあります．\u003c/p\u003e","title":"【入門1】Python仮想環境"},{"content":"JupyterLab+Visual Studio Codeを使ったPython実行 Visual Studio CodeにJupyter拡張機能をインストール 空のファイルを作成し，拡張子を.ipynbとする ＋コードボタンからコードブロックを作成し，Pythonプログラムを記述 右上カーネルの選択から実行したいPython環境を選択し コードブロック左側の実行ボタン▷を押下 ポップアップが出てきたらインストールをクリックしてipykernelをインストール （pip install ipykernelを当該環境で実行してインストールも出来ます） YOLOのセットアップ 今回は最新のYOLOを簡単に扱うことができるUltralyticsパッケージを利用します．\nconda create --name yolo-env python=3.11を実行して仮想環境を作成 conda activate yolo-envで仮想環境を有効化 pip install ultralyticsでUltralyticsパッケージをインストール ここまで済んだら，テストコードを動かしてみましょう！\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolo11n.pt\u0026#34;) results = model(\u0026#34;https://ultralytics.com/images/bus.jpg\u0026#34;) results[0].show() バスを背景にした画像の物体検出が表示されたはずです！\n作業ディレクトリを開くと，yolo11n.ptというファイルがダウンロードされています．これが，YOLOのモデルファイルです．これはYOLOのデフォルトの訓練済みモデルで，COCO Datasetに収録された80種類の物体を識別することが出来ます．\n","permalink":"http://localhost:1313/blog/posts/20250506_yolojupyter/","summary":"\u003ch2 id=\"jupyterlabvisual-studio-codeを使ったpython実行\"\u003eJupyterLab+Visual Studio Codeを使ったPython実行\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eVisual Studio CodeにJupyter拡張機能をインストール\u003c/li\u003e\n\u003cli\u003e空のファイルを作成し，拡張子を\u003ccode\u003e.ipynb\u003c/code\u003eとする\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e＋コード\u003c/code\u003eボタンからコードブロックを作成し，Pythonプログラムを記述\u003c/li\u003e\n\u003cli\u003e右上\u003ccode\u003eカーネルの選択\u003c/code\u003eから実行したいPython環境を選択し\u003c/li\u003e\n\u003cli\u003eコードブロック左側の実行ボタン\u003ccode\u003e▷\u003c/code\u003eを押下\u003c/li\u003e\n\u003cli\u003eポップアップが出てきたら\u003ccode\u003eインストール\u003c/code\u003eをクリックして\u003ccode\u003eipykernel\u003c/code\u003eをインストール\n（\u003ccode\u003epip install ipykernel\u003c/code\u003eを当該環境で実行してインストールも出来ます）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"vsjupyter.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"yoloのセットアップ\"\u003eYOLOのセットアップ\u003c/h2\u003e\n\u003cp\u003e今回は最新のYOLOを簡単に扱うことができるUltralyticsパッケージを利用します．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003econda create --name yolo-env python=3.11\u003c/code\u003eを実行して仮想環境を作成\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate yolo-env\u003c/code\u003eで仮想環境を有効化\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install ultralytics\u003c/code\u003eでUltralyticsパッケージをインストール\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eここまで済んだら，テストコードを動かしてみましょう！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e ultralytics \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e YOLO\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emodel \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e YOLO(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;yolo11n.pt\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresults \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://ultralytics.com/images/bus.jpg\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresults[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eバスを背景にした画像の物体検出が表示されたはずです！\u003c/p\u003e\n\u003cp\u003e作業ディレクトリを開くと，\u003ccode\u003eyolo11n.pt\u003c/code\u003eというファイルがダウンロードされています．これが，YOLOのモデルファイルです．これはYOLOのデフォルトの訓練済みモデルで，COCO Datasetに収録された80種類の物体を識別することが出来ます．\u003c/p\u003e","title":"【入門2】YOLO，Jupyterのセットアップ"},{"content":"アノテーションとは？ 機械学習の分類のひとつである教師あり学習では，まず入力データと出力データの組を複数用意し，それらを使って機械学習モデルを訓練します．\n入力データと出力データの組は，タスクによって様々です．例えば，\n入力：画像データ，出力：それぞれが犬or猫 入力：家の情報（間取り，立地，築年数など），出力：家の価格 入力：これまでの気温，出力：明日の気温 入力：画像データ，出力：各物体の座標 どんなタスクを前提とする場合でも，これら入力データと出力データの組を作る作業が必要となります．これがアノテーションです．\n環境構築 今回はYOLOによる画像物体認識を目標とします．そこで，アノテーションで作るべきデータは以下になります．\n入力：画像データ，出力：各物体の座標 頑張れば用意した画像の座標を数えてアノテーションできそうですが，今回はより簡単に作業を進めるためにLabelmeというツールを使います．GUIが整備されたソフトウェアですが，Pythonライブラリとして無償で提供されています！（ありがたい！！）\nAnaconda Promptを開く conda create -n annot_env python=3.12で仮想環境を作成 conda activate annot_envで仮想環境を有効化 pip install --upgrade labelmeと入力しLabelmeをインストール これでインストールは完了です．そのままannot_env環境内でlabelmeコマンドを実行してみてください！\nこの画面が表示されたら成功です！\nいざ，アノテーション 画像読み込み 今回は画像から麻雀牌を物体認識するタスクに挑戦してみます！サンプル画像は以下からダウンロード，解凍してください．\nサンプル画像 Zipファイルの解凍が済んだらLabelmeの画面左側にあるOpen Dirから，画像のディレクトリを開いてください．\nこれでアノテーションの準備は完了です！\n範囲選択，ラベル付け アノテーションの流れは以下になります．\n画像を右クリック，Create Rectangleをクリック 物体の範囲を四角形で囲んで選択，間違えたらEscキーで戻る 出てきたポップアップの上部Enter object labelにラベルを入力してOKをクリック 2と3を物体の数だけひたすら繰り返す 画像内のアノテーションが終わったら画面左部Saveから画像と同名のJSONファイルを保存 ちょっとやってみますね～\nはい，できました．ラベルは牌の種類（萬子筒子索子字牌）の略称です．\n\u0026hellip;\u0026hellip;\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\nめんどくさい\n機械学習では一般的にデータの数が大きいほど性能が上がりやすいとされています．つまり，（要件の要求レベルにもよりますが）精度を上げるためにはたくさんアノテーションする必要があるんです．\nそこで，特定のタスクの機械学習を考える場合はアノテーションに取り掛かる前に，データセットが利用可能なライセンスで公開されていないかどうか調べるところからスタートします．例えばYOLOの場合，Roboflow，VisualData，Google Open Images Datasetなどが画像データセットを公開している他，タスクに応じて論文と併せて公開されているデータセットも多数あります．\nまた今回はアノテーション入門ということで手作業での方法を紹介していますが，画像認識AIによる半自動ツールもあるので興味があれば調べてみてください．\n今回は，なんと，特別に，アノテーション済みファイルを用意してあります！\nアノテーション結果（JSON） ZIPファイルをダウンロード＆解凍したら，datasetsという名前のディレクトリを作成し，先程の画像ファイルとJSONファイルを以下の構造で配置してください．（image_1.jsonは自作のファイルで構いません！）\n.\r└── datasets/\r├── image_1.jpg\r├── image_1.json\r├── image_2.jpg\r├── image_2.json\r├── 〜省略〜\r├── image_10.jpg\r└── image_10.json データセットフォーマットの変換 では，学習させるデータの中身を一度見てみましょう！image_1.jsonを開いてみます．\nJSONはデータ記述形式のひとつで，キーと値を並べて書くのが特徴です．Pythonの辞書型そっくりですね． ラベルや座標がファイルの中に複数記述されていることがわかります．\n少しスクロールすると，よくわからない文字列が大量に記述されています．実はこれ，Base64と呼ばれるエンコード形式で画像そのものが書きつけられているんです．これにより，このJSONファイル単体で入力データを表現することが出来ます．\nさて，ひととおり学習データの中身を見てもらいましたが，実はこのままではYOLOの学習が出来ないんです．\nYOLOに学習させるためには，画像のアノテーションを以下の形式で記述する必要があります．\n画像ファイル名.txt\nlabel_id X Y Width Height 例えば，image_1.jsonをこの形式で記述すると以下のようになります．\nimage_1.txt\n4 0.171569 0.375817 0.142157 0.183007\r6 0.286765 0.382353 0.124183 0.183007\r0 0.401144 0.398693 0.127451 0.185185\r5 0.517157 0.408497 0.117647 0.178649\r10 0.638889 0.418301 0.125817 0.189542\r7 0.767157 0.436819 0.127451 0.174292 YOLOのデータセットでは，ラベルの番号，X座標，Y座標，幅，高さを列挙して記述します．\nこれを手作業で変換するのはかなり面倒です．\nそこで今回は，Labelmeで保存したJSONファイルをYOLOのデータセット形式に変換するツール，labelme2yoloを使います．condaのannot_env環境下で以下の手順で変換します．\npip install labelme2yoloでlabelme2yoloをインストール datasetディレクトリの一つ上の階層で以下のコマンドを実行 labelme2yolo --json_dir ./datasets/ --val_size 0.10 --output_format \u0026quot;bbox\u0026quot; これが済んだら，datasetsディレクトリの中にYOLODatasetが生成されているはずです．構造は以下のようになっています．\n.\r└── YOLODatasets/\r├── images/\r│ ├── train/\r│ │ ├── image_1.jpg\r│ │ └── ～省略～\r│ └── val/\r│ └── image_7.jpg\r├── labels/\r│ ├── train/\r│ │ ├── image_1.txt\r│ │ └── ～省略～\r│ └── val/\r│ └── image_7.txt\r└── dataset.yaml 先ほどのデータセット構造より複雑になっていますね．画像とラベルが別のフォルダに格納され，更にその中で訓練用(train)と評価用(val)に分かれています．\nlabelsディレクトリのテキストファイルを見てみましょう．YOLOのフォーマットに変換され，ラベルIDや座標が記録されているはずです．\ndataset.yamlの中身を見てみましょう．\nデータセットまでの絶対パス，訓練評価用ディレクトリのパス，ラベルIDとラベル名の対応が記載されています．YOLOの学習時にはこのdataset.yamlを利用してデータセットを読み込みます．\nYOLOの学習 それでは，いよいよ作ったデータセットを使ってYOLOの学習をしてみましょう！！\nまず，テスト用の画像を用意します．以下のリンクから画像ファイルimage_11.jpgをダウンロードしてdataset.yamlと同階層に配置します．\nテスト画像image_11.jpg dataset.yamlと同階層に.ipynbファイルを作成して，以下のPythonプログラムを実行してみましょう．\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolo11n.pt\u0026#34;) model.train(data=\u0026#39;dataset.yaml\u0026#39;, epochs=300, verbose=False) results = model(\u0026#34;image_11.jpg\u0026#34;) results[0].show() CPUのみの実行環境ではかなり時間がかかるので気長に待ちましょう～\n学習が終了すると，image_11.jpgに対する推論結果が表示されます．\n未検出や誤検出がちらほらありますが，部分的に検出が成功していることがわかります．データセットサイズやエポック数を変えることで性能の改善が期待できます．\nまた，データセットディレクトリのruns内を見ると，weightディレクトリにbest.ptが保存されていることがわかります． これは今回学習したモデルファイルで，実際に物体検出をシステム開発に組み込むときに使用します．\n","permalink":"http://localhost:1313/blog/posts/20250506_yoloannotation/","summary":"\u003ch2 id=\"アノテーションとは\"\u003eアノテーションとは？\u003c/h2\u003e\n\u003cp\u003e機械学習の分類のひとつである\u003cstrong\u003e教師あり学習\u003c/strong\u003eでは，まず入力データと出力データの組を複数用意し，それらを使って機械学習モデルを訓練します．\u003c/p\u003e\n\u003cp\u003e入力データと出力データの組は，タスクによって様々です．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e入力：画像データ，出力：それぞれが犬or猫\u003c/li\u003e\n\u003cli\u003e入力：家の情報（間取り，立地，築年数など），出力：家の価格\u003c/li\u003e\n\u003cli\u003e入力：これまでの気温，出力：明日の気温\u003c/li\u003e\n\u003cli\u003e入力：画像データ，出力：各物体の座標\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eどんなタスクを前提とする場合でも，これら\u003cstrong\u003e入力データと出力データの組を作る作業\u003c/strong\u003eが必要となります．これが\u003cstrong\u003eアノテーション\u003c/strong\u003eです．\u003c/p\u003e\n\u003ch2 id=\"環境構築\"\u003e環境構築\u003c/h2\u003e\n\u003cp\u003e今回はYOLOによる画像物体認識を目標とします．そこで，アノテーションで作るべきデータは以下になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e入力：画像データ，出力：各物体の座標\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e頑張れば用意した画像の座標を数えてアノテーションできそうですが，今回はより簡単に作業を進めるために\u003cstrong\u003eLabelme\u003c/strong\u003eというツールを使います．GUIが整備されたソフトウェアですが，Pythonライブラリとして無償で提供されています！（ありがたい！！）\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAnaconda Promptを開く\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda create -n annot_env python=3.12\u003c/code\u003eで仮想環境を作成\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate annot_env\u003c/code\u003eで仮想環境を有効化\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install --upgrade labelme\u003c/code\u003eと入力しLabelmeをインストール\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eこれでインストールは完了です．そのまま\u003ccode\u003eannot_env\u003c/code\u003e環境内で\u003ccode\u003elabelme\u003c/code\u003eコマンドを実行してみてください！\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"labelme.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこの画面が表示されたら成功です！\u003c/p\u003e\n\u003ch2 id=\"いざアノテーション\"\u003eいざ，アノテーション\u003c/h2\u003e\n\u003ch3 id=\"画像読み込み\"\u003e画像読み込み\u003c/h3\u003e\n\u003cp\u003e今回は画像から麻雀牌を物体認識するタスクに挑戦してみます！サンプル画像は以下からダウンロード，解凍してください．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hakoshi-normal/yolo_sample/raw/refs/heads/main/images/images.zip\"\u003eサンプル画像\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eZipファイルの解凍が済んだらLabelmeの画面左側にある\u003ccode\u003eOpen Dir\u003c/code\u003eから，画像のディレクトリを開いてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"labelme_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこれでアノテーションの準備は完了です！\u003c/p\u003e\n\u003ch3 id=\"範囲選択ラベル付け\"\u003e範囲選択，ラベル付け\u003c/h3\u003e\n\u003cp\u003eアノテーションの流れは以下になります．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e画像を右クリック，\u003ccode\u003eCreate Rectangle\u003c/code\u003eをクリック\u003c/li\u003e\n\u003cli\u003e物体の範囲を四角形で囲んで選択，間違えたら\u003ccode\u003eEsc\u003c/code\u003eキーで戻る\u003c/li\u003e\n\u003cli\u003e出てきたポップアップの上部\u003ccode\u003eEnter object label\u003c/code\u003eにラベルを入力して\u003ccode\u003eOK\u003c/code\u003eをクリック\u003c/li\u003e\n\u003cli\u003e2と3を物体の数だけひたすら繰り返す\u003c/li\u003e\n\u003cli\u003e画像内のアノテーションが終わったら画面左部\u003ccode\u003eSave\u003c/code\u003eから画像と同名のJSONファイルを保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eちょっとやってみますね～\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"annotated.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eはい，できました．ラベルは牌の種類（萬子筒子索子字牌）の略称です．\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 150%; color: red;\"\u003eめんどくさい\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e機械学習では一般的にデータの数が大きいほど性能が上がりやすいとされています．つまり，（要件の要求レベルにもよりますが）精度を上げるためにはたくさんアノテーションする必要があるんです．\u003c/p\u003e\n\u003cp\u003eそこで，特定のタスクの機械学習を考える場合はアノテーションに取り掛かる前に，データセットが利用可能なライセンスで公開されていないかどうか調べるところからスタートします．例えばYOLOの場合，\u003ca href=\"https://universe.roboflow.com/\"\u003eRoboflow\u003c/a\u003e，\u003ca href=\"https://visualdata.io/discovery\"\u003eVisualData\u003c/a\u003e，\u003ca href=\"https://storage.googleapis.com/openimages/web/index.html\"\u003eGoogle Open Images Dataset\u003c/a\u003eなどが画像データセットを公開している他，タスクに応じて論文と併せて公開されているデータセットも多数あります．\u003c/p\u003e\n\u003cp\u003eまた今回はアノテーション入門ということで手作業での方法を紹介していますが，画像認識AIによる半自動ツールもあるので興味があれば調べてみてください．\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e今回は，なんと，特別に，アノテーション済みファイルを用意してあります！\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hakoshi-normal/yolo_sample/raw/refs/heads/main/images/labels.zip\"\u003eアノテーション結果（JSON）\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eZIPファイルをダウンロード＆解凍したら，\u003ccode\u003edatasets\u003c/code\u003eという名前のディレクトリを作成し，先程の画像ファイルとJSONファイルを以下の構造で配置してください．（\u003ccode\u003eimage_1.json\u003c/code\u003eは自作のファイルで構いません！）\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-planetext\" data-lang=\"planetext\"\u003e.\r\n└── datasets/\r\n    ├── image_1.jpg\r\n    ├── image_1.json\r\n    ├── image_2.jpg\r\n    ├── image_2.json\r\n    ├── 〜省略〜\r\n    ├── image_10.jpg\r\n    └── image_10.json\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"データセットフォーマットの変換\"\u003eデータセットフォーマットの変換\u003c/h3\u003e\n\u003cp\u003eでは，学習させるデータの中身を一度見てみましょう！\u003ccode\u003eimage_1.json\u003c/code\u003eを開いてみます．\u003c/p\u003e","title":"【入門3】YOLOアノテーションと学習"},{"content":"はじめに 自動採譜プロジェクトの息抜き的な記事です．\nせっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして， ところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．\nカメラ切り替えトリガー カメラの切り替えトリガーを「牌山にプレーヤーが触れたタイミング」と設定しました．\n自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．\nツモ位置の検出 まず，天カメの映像から卓上のツモ位置を特定する必要があります．\n牌山の形を見てみましょう．\n対局開始前はこんな感じ．\n対局開始直後はこんな感じ．\n対局中はこんな感じ．\n対局開始前は4つ牌山がありますが，対局開始以降は牌山が必ず3つ以下になります王牌の扱い上，4つのときもあります．\n牌山が3つのとき\n牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．\n牌山が4つのとき\n一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．\n実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．\n牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．\n実装概要 元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます． cv2.inRange()を使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．\n二値化した画像に対し，cv2.findContours()を使用し，牌山となりうる矩形を検出します．\n手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます． その後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．\nツモプレイヤーの検出 ツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します． MediaPipe のハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します． 今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．\nプレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．\nオートスイッチャー実装！！！ OBSを使用して配信するため，カメラ切り替えには obs-websocket-py を使用しました．こちらの サンプル がそのまま動作して助かりました．\nGUI実装にはHTML/JSでの記述が可能な Eel を使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．\n実際の動作の様子がこちら．\nYour browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．\nあと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．\n","permalink":"http://localhost:1313/blog/posts/20240210_mahjongautocamswitcher/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e自動採譜プロジェクト\u003c/a\u003eの息抜き的な記事です．\u003c/p\u003e\n\u003cp\u003eせっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして，\nところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．\u003c/p\u003e\n\u003ch2 id=\"カメラ切り替えトリガー\"\u003eカメラ切り替えトリガー\u003c/h2\u003e\n\u003cp\u003eカメラの切り替えトリガーを「\u003cstrong\u003e牌山にプレーヤーが触れたタイミング\u003c/strong\u003e」と設定しました．\u003c/p\u003e\n\u003cp\u003e自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．\u003c/p\u003e\n\u003ch2 id=\"ツモ位置の検出\"\u003eツモ位置の検出\u003c/h2\u003e\n\u003cp\u003eまず，天カメの映像から卓上のツモ位置を特定する必要があります．\u003cbr\u003e牌山の形を見てみましょう．\u003c/p\u003e\n\u003cp\u003e対局開始前はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_0.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局開始直後はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局中はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_2.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局開始前は4つ牌山がありますが，対局開始以降は牌山が\u003cdel\u003e必ず3つ以下になります\u003c/del\u003e王牌の扱い上，4つのときもあります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e牌山が3つのとき\u003c/p\u003e\n\u003cp\u003e牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e牌山が4つのとき\u003c/p\u003e\n\u003cp\u003e一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"fig1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．\u003c/p\u003e\n\u003cp\u003e牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．\u003c/p\u003e\n\u003ch3 id=\"実装概要\"\u003e実装概要\u003c/h3\u003e\n\u003cp\u003e元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます．\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"original_image.png\"/\u003e \r\n\u003c/figure\u003e\r\n\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecv2.inRange()\u003c/code\u003eを使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"mask_hai.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e二値化した画像に対し，\u003ccode\u003ecv2.findContours()\u003c/code\u003eを使用し，牌山となりうる矩形を検出します．\u003c/p\u003e\n\u003cp\u003e手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます．\nその後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumohai.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"ツモプレイヤーの検出\"\u003eツモプレイヤーの検出\u003c/h2\u003e\n\u003cp\u003eツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します．\n\u003ca href=\"https://developers.google.com/mediapipe/solutions/vision/hand_landmarker\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eMediaPipe\u003c/a\u003e\nのハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します．\n今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumo1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eプレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumo2.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"オートスイッチャー実装\"\u003eオートスイッチャー実装！！！\u003c/h2\u003e\n\u003cp\u003eOBSを使用して配信するため，カメラ切り替えには\n\u003ca href=\"https://github.com/Elektordi/obs-websocket-py/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eobs-websocket-py\u003c/a\u003e\nを使用しました．こちらの\n\u003ca href=\"https://github.com/Elektordi/obs-websocket-py/blob/master/samples/switch_scenes.py\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eサンプル\u003c/a\u003e\nがそのまま動作して助かりました．\u003c/p\u003e\n\u003cp\u003eGUI実装にはHTML/JSでの記述が可能な\n\u003ca href=\"https://github.com/python-eel/Eel\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eEel\u003c/a\u003e\nを使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．\u003c/p\u003e\n\u003cp\u003e実際の動作の様子がこちら．\u003c/p\u003e\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\n\u003cp\u003e現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．\u003c/p\u003e\n\u003cp\u003eあと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．\u003c/p\u003e","title":"麻雀放送対局用のオートスイッチャーを作りたい"},{"content":"はじめに 複数カメラの入力に対しYOLOv8で並列推論するのソースコード解説です． 解説のため，一部表現を変えている部分があります．\n大まかな流れは以下の通り．\n共有メモリ確保 フレーム入力プロセス カメラからフレームを取得 フレームを共有メモリ1に保存 推論プロセス 共有フレームを呼び出し 推論 推論結果を描画したフレームを共有メモリ2に保存 フレーム表示プロセス 共有フレームを呼び出し フレームの画面描画 プロセス間の遷移はmultiprocessingモジュールのEvent関数をフラグとして利用します． カメラの台数が増えた場合はプロセス数を増やせば対応できます．\n事前準備 設定情報 cam_N = 4 # カメラ台数 proccess_N = 3 # カメラ1台あたりのプロセス数 frame_info = {\u0026#34;nbytes\u0026#34; : frame.nbytes, \u0026#34;shape\u0026#34; : frame.shape, \u0026#34;dtype\u0026#34; : frame.dtype} フレームのサイズや型はメモリ確保，呼び出し時に必要になります．\n共有メモリの確保 shm = shared_memory.SharedMemory(create=True, size=frame.nbytes, name = mem_name) 共有メモリの確保はこの一文でできます．めっちゃ便利ですね． ただし，mem_nameを動的に作成して複数の共有メモリを確保する場合，注意事項があります． shared_memory.SharedMemoryの戻り値を格納する変数（上記コードではshm）を設定しなかった場合や上書きした場合，それ以前の共有メモリ領域は参照できなくなります．その場合はshmを格納するリストを用意しておくとアクセス可能になります．\nプロセスの立ち上げ p = Process(target=関数名, args=(引数,)) p.start() # プロセス開始 p.join() # プロセス終了 複数プロセスを立ち上げる場合は p をリストに入れておくと管理が楽です．\nフレーム入力プロセス def rec_cam(cam_id, pre_flags, frame_info): cap = cv2.VideoCapture(cam_id) while True: _, frame = cap.read() idx = (cam_id)*proccess_N+counter%proccess_N mem_name = f\u0026#39;shared{idx+1:02}\u0026#39; shm = shared_memory.SharedMemory(name=mem_name) frame_sh = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) frame_sh[:] = frame[:] # 推論開始 pre_flags[idx].set() cap.close() multiprocessingのSharedMemoryでは共有メモリを名前をつけて呼び出すことができます．便利！ ここでは，\u0026ldquo;shared1[番号]\u0026ldquo;という名前で共有メモリを確保している（後述）．カメラ台数✕カメラあたりのプロセス数分の共有メモリ領域が必要になるため， あるカメラのあるフレームに対する保存領域は，カメラ番号✕カメラ1台あたりのプロセス数＋フレーム番号%カメラ1台あたりのプロセス数で求めることができる．と同時にこの番号は推論プロセスを動作させるフラグ管理のインデックスとしても使う．\n推論プロセス def predict_frame(pre_flag, view_flag, mem_name, cam_id, frame_info): while True: if pre_flag.is_set(): # 推論開始 pre_flag.clear() shm = shared_memory.SharedMemory(name=mem_name) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # 重い推論処理がここに入る（frameに結果を反映する） # 結果統合用共有メモリにのせる shm_result = shared_memory.SharedMemory(name=f\u0026#39;shared2{cam_id}\u0026#39;) frame_new = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm_result.buf) frame_new[:]=frame[:] # 結果表示 view_flag.set() カメラ台数✕カメラあたりのプロセス数分の推論プロセスを立ち上げます． 推論開始用のフラグや共有メモリの呼び出し名は予め与えた状態でプロセスを立ち上げる．フレーム入力プロセスでの推論開始用フラグの有効化を待ち受けています． 処理済みのフレーを格納する共有メモリ領域名は\u0026rsquo;shared2[番号]\u0026lsquo;としており，カメラの台数分事前に確保してあります． 検出結果を描画する必要がない場合は，フレーム表示プロセスを介する必要がないため，別プロセスに推論結果を渡したり，UDP通信などで端末間の結果送信が実装できると思います．\nフレーム表示プロセス def view_frame(view_flag, cam_id, frame_info): while True: if view_flag.is_set(): view_flag.clear() shm = shared_memory.SharedMemory(name=f\u0026#39;shared2{cam_id}\u0026#39;) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) cv2.imshow(f\u0026#39;window_{cam_id}\u0026#39;, frame) cv2.waitKey(1) 推論プロセスでのフレーム表示用フラグの有効化を待ち受けています．\nおわりに 今回は，以前実装した並列画像処理プログラムの解説でした．コードブロックを断片的に示しただけなので，具体的な実装コードを知りたい方はソースコードから参照できます．\n","permalink":"http://localhost:1313/blog/posts/20231226_multiprocessingforrealtimevideo/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/2023_12_01_multiprocessingforyolov8/\" target=\"_blank\"\u003e複数カメラの入力に対しYOLOv8で並列推論する\u003c/a\u003eのソースコード解説です．\n解説のため，一部表現を変えている部分があります．\u003c/p\u003e\n\u003cp\u003e大まかな流れは以下の通り．\u003c/p\u003e\n\u003col start=\"0\"\u003e\n\u003cli\u003e共有メモリ確保\u003c/li\u003e\n\u003cli\u003eフレーム入力プロセス\n\u003col\u003e\n\u003cli\u003eカメラからフレームを取得\u003c/li\u003e\n\u003cli\u003eフレームを共有メモリ1に保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e推論プロセス\n\u003col\u003e\n\u003cli\u003e共有フレームを呼び出し\u003c/li\u003e\n\u003cli\u003e推論\u003c/li\u003e\n\u003cli\u003e推論結果を描画したフレームを共有メモリ2に保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eフレーム表示プロセス\n\u003col\u003e\n\u003cli\u003e共有フレームを呼び出し\u003c/li\u003e\n\u003cli\u003eフレームの画面描画\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eプロセス間の遷移はmultiprocessingモジュールのEvent関数をフラグとして利用します．\nカメラの台数が増えた場合はプロセス数を増やせば対応できます．\u003c/p\u003e\n\u003ch2 id=\"事前準備\"\u003e事前準備\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e設定情報\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecam_N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# カメラ台数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eproccess_N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# カメラ1台あたりのプロセス数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eframe_info \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;nbytes\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enbytes,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;shape\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshape,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dtype\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edtype}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eフレームのサイズや型はメモリ確保，呼び出し時に必要になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e共有メモリの確保\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eshm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e shared_memory\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSharedMemory(create\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enbytes, name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e mem_name)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e共有メモリの確保はこの一文でできます．めっちゃ便利ですね．\nただし，mem_nameを動的に作成して複数の共有メモリを確保する場合，注意事項があります．\nshared_memory.SharedMemoryの戻り値を格納する変数（上記コードではshm）を設定しなかった場合や上書きした場合，それ以前の共有メモリ領域は参照できなくなります．その場合はshmを格納するリストを用意しておくとアクセス可能になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプロセスの立ち上げ\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Process(target\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e関数名, args\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e(引数,))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estart() \u003cspan style=\"color:#75715e\"\u003e# プロセス開始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ejoin() \u003cspan style=\"color:#75715e\"\u003e# プロセス終了\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e複数プロセスを立ち上げる場合は p をリストに入れておくと管理が楽です．\u003c/p\u003e\n\u003ch2 id=\"フレーム入力プロセス\"\u003eフレーム入力プロセス\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erec_cam\u003c/span\u003e(cam_id, pre_flags, frame_info):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cv2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eVideoCapture(cam_id)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            _, frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            idx \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (cam_id)\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003eproccess_N\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003ecounter\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003eproccess_N\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            mem_name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;shared\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003eidx\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e02\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            shm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e shared_memory\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSharedMemory(name\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003emem_name)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            frame_sh \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003endarray(shape\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;shape\u0026#34;\u003c/span\u003e], dtype\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dtype\u0026#34;\u003c/span\u003e], buffer\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eshm\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebuf)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            frame_sh[:] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e frame[:]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# 推論開始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            pre_flags[idx]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eset()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003emultiprocessingのSharedMemoryでは共有メモリを名前をつけて呼び出すことができます．便利！\nここでは，\u0026ldquo;shared1[番号]\u0026ldquo;という名前で共有メモリを確保している（後述）．カメラ台数✕カメラあたりのプロセス数分の共有メモリ領域が必要になるため，\nあるカメラのあるフレームに対する保存領域は，カメラ番号✕カメラ1台あたりのプロセス数＋フレーム番号%カメラ1台あたりのプロセス数で求めることができる．と同時にこの番号は推論プロセスを動作させるフラグ管理のインデックスとしても使う．\u003c/p\u003e","title":"リアルタイムの映像入力に対してFPSを落とさずに画像処理したい"},{"content":"ビデオカメラからOpenCVで映像取得（低品質） 麻雀自動採譜の実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．\nビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できるビデオキャプチャカードを導入． これでビデオカメラをWebカメラみたく使えるぞ！やった！！\nと思った矢先，\nなんか遅い気がする！！\nよくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，1秒遅延＋低FPS＋低画質という散々な結果に\nお前 Full HD で 30FPS 出るって言ってたじゃないか\u0026hellip;\u0026hellip;\nキャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．\nRTSP通信でカメラの映像を受信してみる\nこのときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった． 今回もPyAVを使えば解決するかもしれない．\nビデオカメラからPyAVで映像取得 PyAVでPCに有線接続されたカメラから映像を取得する．\nimport cv2 import av con_options = dict( video_size=\u0026#39;1920x1080\u0026#39;, vcodec=\u0026#39;mjpeg\u0026#39;, framerate=\u0026#39;30\u0026#39;, rtbufsize=\u0026#39;1\u0026#39;, ) device_name = \u0026#34;USB Video\u0026#34; con_def = dict( format=\u0026#39;dshow\u0026#39;, file=f\u0026#39;video={device_name}\u0026#39;, options=dict(con_options, video_device_number=\u0026#39;0\u0026#39;) ) container = av.open(**con_def) for frame in container.decode(video=0): frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) # cv2.imshow(\u0026#34;test_window\u0026#34;, frame) # cv2.waitKey(1) ちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．\nffmpeg -list_devices true -f dshow -i dummy PyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．\nこれで遅延ほぼなし，30FPSでの映像受信ができた．\n","permalink":"http://localhost:1313/blog/posts/20231225_getframebypyav/","summary":"\u003ch2 id=\"ビデオカメラからopencvで映像取得低品質\"\u003eビデオカメラからOpenCVで映像取得（低品質）\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003eの実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．\u003c/p\u003e\n\u003cp\u003eビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できる\u003ca href=\"https://amzn.asia/d/jclJASJ\"\u003eビデオキャプチャカード\u003c/a\u003eを導入．\nこれでビデオカメラをWebカメラみたく使えるぞ！やった！！\u003c/p\u003e\n\u003cp\u003eと思った矢先，\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eなんか遅い気がする！！\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eよくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，\u003cstrong\u003e1秒遅延＋低FPS＋低画質\u003c/strong\u003eという散々な結果に\u003c/p\u003e\n\u003cp\u003eお前 Full HD で 30FPS 出るって言ってたじゃないか\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003eキャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/blog/2023_12_01_getframertsp/\"\u003eRTSP通信でカメラの映像を受信してみる\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこのときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった．\n今回も\u003ca href=\"https://pyav.org/docs/develop/index.html\" target=\"_blank\"\u003ePyAV\u003c/a\u003eを使えば解決するかもしれない．\u003c/p\u003e\n\u003ch2 id=\"ビデオカメラからpyavで映像取得\"\u003eビデオカメラからPyAVで映像取得\u003c/h2\u003e\n\u003cp\u003ePyAVでPCに有線接続されたカメラから映像を取得する．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e cv2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e av\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econ_options  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  dict(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    video_size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1920x1080\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    vcodec\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;mjpeg\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    framerate\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;30\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    rtbufsize\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edevice_name  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;USB Video\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econ_def  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  dict(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;dshow\u0026#39;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    file\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;video=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003edevice_name\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    options\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003edict(con_options, video_device_number\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;0\u0026#39;\u003c/span\u003e) )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econtainer \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e av\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eopen(\u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003econ_def)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e frame \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e container\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edecode(video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto_ndarray(format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;bgr24\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# cv2.imshow(\u0026#34;test_window\u0026#34;, frame)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# cv2.waitKey(1)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-console\" data-lang=\"console\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003effmpeg -list_devices true -f dshow -i dummy\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．\u003c/p\u003e","title":"映像入力の遅延をPyAVで解決してみる"},{"content":" multiprocessing × YOLOv8の詳細解説 Denoを利用した基本的なWebサイト作成 Deno Deploy設定 ","permalink":"http://localhost:1313/blog/posts/20231212_scheduled_update/","summary":"\u003cul\u003e\n\u003cli\u003emultiprocessing × YOLOv8の詳細解説\u003c/li\u003e\n\u003cli\u003eDenoを利用した基本的なWebサイト作成\u003c/li\u003e\n\u003cli\u003eDeno Deploy設定\u003c/li\u003e\n\u003c/ul\u003e","title":"今後の更新予定"},{"content":"RTSPでフレーム取得（OpenCV） RTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．\nimport cv2 url = \u0026#34;rtsp://username:password@ipaddress\u0026#34; # Tapo C200は末尾に画質を指定 url = \u0026#34;rtsp://username:password@ipaddress/stream1\u0026#34; cap = cv2.VideoCapture(url) while True: ret, frame = cap.read() pass ただこれだとTapo C200では1秒程度遅延が発生してしまう． 麻雀自動採譜 の実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．\nRTSPでフレーム取得（PyAV） RTSP映像の受信にはFFmpegのPythonバインディングである PyAV を利用した方法もあり、以下で映像受信が可能でした．\nimport av\rurl = \u0026#34;rtsp://username:password@ipaddress\u0026#34;\rcontainer = av.open(url)\rfor frame in container.decode(video=0):\rframe = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;)\rpass PyAVを利用した場合は遅延が大幅に減少した．やったね．\nRTSPで一定時間毎にフレーム取得（PyAV） 麻雀自動採譜 牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚でtime.sleep(n)を使用したところ以下のエラーが複数出現．\nmax delay reached. need to consume packet\rRTP: missed 6066 packets\rRTP: PT=60: bad cseq a657 expected=8ea5\rmax delay reached. need to consume packet sleepで無理に処理を中断させたため接続が不安定になったのかも． 通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．\nimport av interval = 10 # 10秒ごとに撮影 url = \u0026#34;rtsp://username:password@ipaddress\u0026#34; container = av.open(url) counter = 0 for frame in container.decode(video=0): if frame is not None and frame.time//interval\u0026gt;counter: counter+=1 frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) cv2.imwrite(f\u0026#39;save_{counter:04}.png\u0026#39;, frame) PyAVのcontainerから呼び出したフレームは撮影開始を0としたときの撮影時刻をtimeで取得できる． 撮影開始後数フレームはNoneが返ることがあるため注意． これで安定したn秒毎の撮影が実行できる．\n","permalink":"http://localhost:1313/blog/posts/20231201_getframertsp/","summary":"\u003ch2 id=\"rtspでフレーム取得opencv\"\u003eRTSPでフレーム取得（OpenCV）\u003c/h2\u003e\n\u003cp\u003eRTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e cv2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rtsp://username:password@ipaddress\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Tapo C200は末尾に画質を指定\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rtsp://username:password@ipaddress/stream1\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cv2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eVideoCapture(url)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ret, frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eただこれだとTapo C200では\u003cstrong\u003e1秒程度遅延\u003c/strong\u003eが発生してしまう．\n\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003e\nの実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．\u003c/p\u003e\n\u003ch2 id=\"rtspでフレーム取得pyav\"\u003eRTSPでフレーム取得（PyAV）\u003c/h2\u003e\n\u003cp\u003eRTSP映像の受信にはFFmpegのPythonバインディングである\n\u003ca href=\"https://pyav.org/docs/develop/index.html\" target=\"_blank\"\u003ePyAV\u003c/a\u003e\nを利用した方法もあり、以下で映像受信が可能でした．\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-python:\" data-lang=\"python:\"\u003eimport av\r\n\r\nurl = \u0026#34;rtsp://username:password@ipaddress\u0026#34;\r\ncontainer = av.open(url)\r\nfor frame in container.decode(video=0):\r\n    frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;)\r\n    pass\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ePyAVを利用した場合は遅延が大幅に減少した．やったね．\u003c/p\u003e\n\u003ch2 id=\"rtspで一定時間毎にフレーム取得pyav\"\u003eRTSPで一定時間毎にフレーム取得（PyAV）\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003e\n牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚で\u003ccode\u003etime.sleep(n)\u003c/code\u003eを使用したところ以下のエラーが複数出現．\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-raw:\" data-lang=\"raw:\"\u003emax delay reached. need to consume packet\r\nRTP: missed 6066 packets\r\nRTP: PT=60: bad cseq a657 expected=8ea5\r\nmax delay reached. need to consume packet\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003esleepで無理に処理を中断させたため接続が不安定になったのかも．\n通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．\u003c/p\u003e","title":"RTSP通信でカメラの映像を受信してみる"},{"content":"実装過程あれこれ１ これまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の Tapo C200 からフレームを受信することができるようになりました． 早速各カメラに割り当てたプロセス毎に適当に学習させておいた YOLOv8 による牌推論を実行し，結果を描画してみました．わくわく！\nはい，映像出力が不穏な感じに． Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．\nプロセス間でまったく映像出力のタイミングがあっていない フレーム間隔がまちまち 映像の乱れが酷い FPSが低い（Tapo入力時15→10弱） 特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね\u0026hellip;\u0026hellip;\nフレーム出力がばらつく理由を勝手に予想．こんな感じだろうか\n出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える． こんなときこそマルチプロセス処理の出番でしょ↓\n（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）\n既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．\n映像受信プロセス ×4（カメラの台数） 推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当） 推論結果統合プロセス ×4（カメラの台数） さあやるぞ！\n実装過程あれこれ２ ここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．\nbase64→内部UDPが一瞬頭を過るが無視して 公式ドキュメント を漁ってみます．\nドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか． Value, Array, Queue, Pipe, RawArray, Manager, shared_memory\u0026hellip;\u0026hellip;\n多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．\n試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに． フラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．\nそんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります． Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r概ね同じタイミングでフレームが描画されているように見えますね．\nFPSはいずれのカメラも15程度であり，入力時のFPSを保持したままリアルタイムで推論が実行できました． ちなみに，\nmultiprocessingではなくthreadingを使ってYOLOv8のモデルをスレッド間で共有する方法があるが，提供元である UltralyticsのMedium記事 では推奨方法， 公式ドキュメント では非推奨方法として紹介されていたため今回は見送り． 試験的に使用してるYOLOv8モデルは，現環境（GeForce RTX 3070）で学習可能な最大サイズのモデル（YOLOv8l）に対して複数の公開データセットを統合した5万枚強の牌画像を学習させたものである．統合作業時にデータセット間でラベルのズレが発生したため，検出結果に誤りがある．ラベルが混合した状態のデータセットではあるが安定した検出ができているため，データセットの整理によって更に高精度な検出の実現が期待できる． 背景部分の予期せぬ牌検出は信頼度で棄却できるため問題なし． 検証動画は双方画面録画の仕様上FPSが低下している． ソースコード 複数台のカメラのフレーム入力に対してYOLOv8による推論を並列実行するPythonプログラム ソースコード解説はこちらからどうぞ．\nimport time import msvcrt from multiprocessing import Process, Event, shared_memory import numpy as np import av import cv2 from ultralytics import YOLO # RTSPカメラ設定情報 info_list = [[\u0026#34;toncamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;nancamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;shacamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;peicamera\u0026#34;, \u0026#34;*\u0026#34;]] password = \u0026#34;*\u0026#34; # 1:1280x720, 2:640x360 mode = 1 # 推論プロセス数指定 N = 3 def rec_cam(cam_id, username, ipaddress, pre_flags, stop_flag, frame_info): url = f\u0026#34;rtsp://{username}:{password}@{ipaddress}:554/stream{mode}\u0026#34; container = av.open(url) counter = 0 start = time.time() for frame in container.decode(video=0): counter+=1 frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) # 共有メモリにフレームを書き込み idx = (cam_id)*N+counter%N mem_name = f\u0026#39;shared{idx+1:02}\u0026#39; shm = shared_memory.SharedMemory(name=mem_name) frame_sh = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) frame_sh[:] = frame[:] # 推論開始 pre_flags[idx].set() if stop_flag.is_set(): t = time.time() - start print(f\u0026#34;{username}, TIME:{round(t, 3)}, FRAME:{counter}, FPS:{round(counter/t, 3)}\u0026#34;) break container.close() cv2.destroyAllWindows() return def finish_monitor(stop_flag): while True: if msvcrt.kbhit() and msvcrt.getch() == b\u0026#39;q\u0026#39;: print(\u0026#34;Finished\u0026#34;) stop_flag.set() break return def predict_frame(pre_flag, pre_fin_flag, mem_name, stop_flag, username, frame_info): model = YOLO(\u0026#39;best.pt\u0026#39;) while True: if pre_flag.is_set(): # 推論 pre_flag.clear() shm = shared_memory.SharedMemory(name=mem_name) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # YOLOv8による推論実行 result = model(frame)[0] frame = result.plot() # 結果統合用共有メモリにのせる shm_result = shared_memory.SharedMemory(name=username) frame_new = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm_result.buf) frame_new[:]=frame[:] # 結果統合用 pre_fin_flag.set() if stop_flag.is_set(): break def view_frame(pre_fin_flag, username, stop_flag, frame_info): while True: if pre_fin_flag.is_set(): pre_fin_flag.clear() shm = shared_memory.SharedMemory(name=username) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # Full-HDだと大抵画面が埋まっちゃう # frame = cv2.resize(frame, None, None, 0.6, 0.6) cv2.imshow(username, frame) cv2.waitKey(1) if stop_flag.is_set(): break if __name__ == \u0026#39;__main__\u0026#39;: # メモリ確保情報 url = f\u0026#34;rtsp://{info_list[0][0]}:{password}@{info_list[0][1]}:554/stream{mode}\u0026#34; container = av.open(url) for i, frame in enumerate(container.decode(video=0)): frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) if frame is not None: break container.close() frame_info = {\u0026#34;nbytes\u0026#34; : frame.nbytes, \u0026#34;shape\u0026#34; : frame.shape, \u0026#34;dtype\u0026#34; : frame.dtype} # 停止プロセス stop_flag = Event() p = Process(target=finish_monitor, args=(stop_flag,)) p.start() # 表示プロセス群（カメラ台数分） view_processes = [] pre_fin_flags = [] mem_space = [] for i in range(4): pre_fin_flag = Event() pre_fin_flags.append(pre_fin_flag) username = info_list[i][0] # メモリ確保 今回は画像用 shm = shared_memory.SharedMemory(create=True, size=frame_info[\u0026#34;nbytes\u0026#34;], name = username) mem_space.append(shm) p = Process(target=view_frame, args=(pre_fin_flag, username, stop_flag, frame_info)) view_processes.append(p) p.start() # 推論プロセス群（カメラ台数×推論プロセス数） pre_processes = [] pre_flags = [] mem_names = [] for i in range(N*len(info_list)): pre_flag = Event() pre_flags.append(pre_flag) mem_name = f\u0026#39;shared{i+1:02}\u0026#39; mem_names.append(mem_name) # メモリ確保 shm = shared_memory.SharedMemory(create=True, size=frame_info[\u0026#34;nbytes\u0026#34;], name = mem_name) mem_space.append(shm) # frame_sh = np.ndarray(shape=frame_shape, dtype=frame_dtype, buffer=shm.buf) username = info_list[i//N][0] pre_fin_flag = pre_fin_flags[i//N] p = Process(target=predict_frame, args=(pre_flag, pre_fin_flag, mem_name, stop_flag, username, frame_info)) pre_processes.append(p) p.start() # 撮影プロセス群（カメラ台数） rec_processes = [] for cam_id, info in enumerate(info_list): p = Process(target=rec_cam, args=(cam_id, info[0], info[1], pre_flags, stop_flag, frame_info,)) rec_processes.append(p) p.start() p.join() for p in view_processes: p.join() for p in pre_processes: p.join() for p in rec_processes: p.join() # 共有メモリのリソース開放、削除 for mem_name in mem_names + [info[0] for info in info_list[:4]]: shm = shared_memory.SharedMemory(name = mem_name) shm.close() shm.unlink() ","permalink":"http://localhost:1313/blog/posts/20231201_multiprocessingforyolov8/","summary":"\u003ch2 id=\"実装過程あれこれ１\"\u003e実装過程あれこれ１\u003c/h2\u003e\n\u003cp\u003eこれまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の\n\u003ca href=\"https://www.tp-link.com/jp/smart-home/tapo/tapo-c200/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eTapo C200\u003c/a\u003e\nからフレームを受信することができるようになりました．\n\u003cbr\u003e\n早速各カメラに割り当てたプロセス毎に適当に学習させておいた\n\u003ca href=\"https://docs.ultralytics.com/ja/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eYOLOv8\u003c/a\u003e\nによる牌推論を実行し，結果を描画してみました．わくわく！\u003c/p\u003e\n\u003cp\u003eはい，映像出力が不穏な感じに．\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20231201_multiprocessingforyolov8/before.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20231201_multiprocessingforyolov8/before.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\u003c/p\u003e\n\u003cp\u003e卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプロセス間でまったく映像出力のタイミングがあっていない\u003c/li\u003e\n\u003cli\u003eフレーム間隔がまちまち\u003c/li\u003e\n\u003cli\u003e映像の乱れが酷い\u003c/li\u003e\n\u003cli\u003eFPSが低い（Tapo入力時15→10弱）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね\u0026hellip;\u0026hellip;\u003cbr\u003e\u003c/p\u003e\n\u003cp\u003eフレーム出力がばらつく理由を勝手に予想．こんな感じだろうか\u003c/p\u003e\n\u003cfigure\u003e\r\n    \u003cimg loading=\"lazy\" src=\"multiprocess_fig1.jpg\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える．\n\u003cbr\u003e\u003cbr\u003e\n\u003cspan style=\"font-size: 200%; color: red;\"\u003e\nこんなときこそマルチプロセス処理の出番でしょ↓\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\r\n    \u003cimg loading=\"lazy\" src=\"multiprocess_fig2.jpg\"/\u003e \r\n\u003c/figure\u003e\r\n\n（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）\u003c/p\u003e\n\u003cp\u003e既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e映像受信プロセス ×4（カメラの台数）\u003c/li\u003e\n\u003cli\u003e推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当）\u003c/li\u003e\n\u003cli\u003e推論結果統合プロセス ×4（カメラの台数）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eさあやるぞ！\u003c/p\u003e\n\u003ch2 id=\"実装過程あれこれ２\"\u003e実装過程あれこれ２\u003c/h2\u003e\n\u003cp\u003eここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．\u003cbr\u003e\nbase64→内部UDPが一瞬頭を過るが無視して\n\u003ca href=\"https://docs.python.org/ja/3.10/library/multiprocessing.html\" target=\"_blank\"\u003e公式ドキュメント\u003c/a\u003e\nを漁ってみます．\u003c/p\u003e\n\u003cp\u003eドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか．\nValue, Array, Queue, Pipe, RawArray, Manager, shared_memory\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．\u003c/p\u003e\n\u003cp\u003e試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに．\nフラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．\u003c/p\u003e\n\u003cp\u003eそんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります．\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20231201_multiprocessingforyolov8/after.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20231201_multiprocessingforyolov8/after.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\u003c/p\u003e","title":"複数カメラの入力に対しYOLOv8で並列推論する"},{"content":"ライブラリとは？ ライブラリとは，再利用可能なソフトウェア部品のことで，特定の機能をまとめたコードの再集合のことを指します．簡単に言えば，他の人が作ってくれた機能を自身のプログラムで使える便利なコード集のことです． Pythonには標準ライブラリと外部ライブラリの2種類があります．\n標準ライブラリ 標準ライブラリはPythonをインストールしたときに標準的に入っているライブラリのことです．通常のプログラム作成時に頻繁に必要となる機能をまとめたものが多くあります．例えば，\n乱数を作成する random 時刻を取得できる datetime, time ファイルシステムへのアクセスを実現する os, shutil 基礎的な数学関数 math HTMLサポート html URLを使ってインターネットにアクセスできる urllib スマートフォンで言うところのカメラ，アルバム，時計などのデフォルトアプリみたいなものです．これらはPythonが動く環境であれば勝手に入っているため，import ライブラリ名と記述すればすぐに利用することができます．助かる！！\n# サイコロの出目を出力 import random dice_roll = random.randint(1, 6) print(\u0026#34;サイコロの出目:\u0026#34;, dice_roll) 外部ライブラリ 外部ライブラリはPythonにはもともと組み込まれておらず，自身で導入するタイプのライブラリのことです．実装したいプログラムの機能に応じて適切に選択する必要があります．例えば，\n効率的な多次元配列の数値計算を実現する Numpy 画像処理の機能を提供する OpenCV グラフを簡単に描画できる Matplotlib 様々なアルゴリズムでの機械学習を可能とする scikit-learn Webアプリの開発に役立つ Django スマートフォンで言うところのゲームやチャットアプリなどのストアアプリみたいなものです．これらは自身でインストール作業をする必要がありますが，それさえ済ませば高機能な実装をいとも容易く実現できます！とても便利！！ 有志のプログラマが開発して公開してくれているため，感謝の気持ちを忘れずに使いましょう（Pythonもそうだけど）．\n# サイコロの出目を出力 import numpy as np dice_roll = np.random.randint(1, 7) print(\u0026#34;サイコロの出目:\u0026#34;, dice_roll) ライブラリを確認してみよう では，ここからは実際に手を動かしながら解説を進めます． まずはスタートメニューから Anaconda Prompt を開いてください．\nこれはコマンドプロンプトと呼ばれる画面で，ここにコマンドと呼ばれる命令文を入力することでコンピュータを操作できます． 試しに，ここにtreeと打ち込んでEnterキーを押してみてください．\n\u0026hellip;\u0026hellip;何かツリー構造の記述がたくさん出てきたかと思います．これはディレクトリ構造をツリー形式で表示するコマンドです（あんまり使わないけど）！ なかなか表示が終わらない場合はCtrl+Cキーでキャンセルしてください．\nではコマンドを使って，今使っているPythonで利用可能な外部ライブラリの一覧を見てみましょう！ Anaconda Promptにconda listと打ち込んでEnterキーを押してみてください．\nインストール済みライブラリとバージョンの一覧が表示されました．既にたくさん入っているようです！\nよくよく見てみると，数値計算ができるNumpyや表データを扱えるPandasなど，データサイエンスに役立つライブラリがたくさんあります． 実はAnacondaは科学計算のためのPython配布形式（ディストリビューション）の一つであり，既に役立つライブラリが揃えられているんです．\nこの状態，一見プログラムを書くのに便利な環境であるように見えるんですが実はそうでないケースがあります．\n実はライブラリは，機能を実現するために別のライブラリを参照するケースがあります．例えば，\n画像処理を実現するOpenCVには，画像を配列として扱うためにNumpyが必要 グラフを描画するMatplotlibには，画像描画のためにPillowが必要 機械学習を実現するscikit-learnには，高度な数値計算をするためにScipyが必要 またライブラリはバージョンによって提供される機能が変化することがあります．そのため，同じ実行環境に特定のバージョンを指定して参照ライブラリを導入しておく必要があるのです．\nこれを，ライブラリの依存関係と言います．この依存関係が原因で複数のライブラリを導入するときに問題が発生する可能性があるんです．\n例えば，ライブラリA-C及びB-Cの依存関係があるとき，AとBが共存するためにはCのバージョンが整合している必要があります．\nこうしたライブラリバージョンの試行錯誤を行う場合，先程表示させた複雑に依存関係が絡み合ったライブラリ群を扱うのはなかなか骨が折れます．\nまた，不要なライブラリは更に混乱を引き起こすので必要となる機能のための最小構成で開発環境を整える必要があります．\nそこで役立つのが，仮想環境です！！！（やっと本題）\n仮想環境 仮想環境とは，プログラムを動かすときに仮に作って利用する動作環境のことです． 簡単に言うと「依存関係の解決が面倒だから，まっさらな世界を一から創造すればいいじゃん！！」です．\nでは，実際に作ってみましょう．\n先ほど開いたAnaconda Promptにconda create -n test python=3.12と打ち込んで実行（Proceed([y]/n)?にはyを入力して処理を続行） conda activate testと打ち込んで実行 ここでは，testという名前の仮想環境を作成してその環境に切り替える，という作業をしてもらいました． 最新行の左側が(test)と表示されていれば成功です！\n先ほどまでは(base)と表示されていたはずです．Anacondaではデフォルトでbase環境があり，そこにたくさんのライブラリがありました． ではconda listで現在のtest環境のライブラリ構成を見てみましょう！\nbase環境と比べてライブラリの数がかなり少なくなったはずです．ここに表示されているライブラリはPython3.12に付随する必要最低限のライブラリです． このきれいな環境に最小限の適切なライブラリを導入することで，快適なコーディングライフを送ることができます！！\n番外編：Conda仮想環境の管理 複数の仮想環境を扱うときに便利なコマンドをまとめておきます！\n仮想環境の作成（環境名がtest，Python3.12の場合）\nconda create -n test python=3.12 仮想環境への移行\nconda activate test 仮想環境から離脱\nconda deactivate 仮想環境の削除\nconda remove -n test --all 仮想環境一覧の表示\nconda info -e ","permalink":"http://localhost:1313/blog/posts/20250506_condavenv/","summary":"\u003ch2 id=\"ライブラリとは\"\u003eライブラリとは？\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eライブラリ\u003c/strong\u003eとは，再利用可能なソフトウェア部品のことで，特定の機能をまとめたコードの再集合のことを指します．簡単に言えば，他の人が作ってくれた機能を自身のプログラムで使える便利なコード集のことです．\nPythonには\u003cstrong\u003e標準ライブラリ\u003c/strong\u003eと\u003cstrong\u003e外部ライブラリ\u003c/strong\u003eの2種類があります．\u003c/p\u003e\n\u003ch3 id=\"標準ライブラリ\"\u003e標準ライブラリ\u003c/h3\u003e\n\u003cp\u003e標準ライブラリはPythonをインストールしたときに標準的に入っているライブラリのことです．通常のプログラム作成時に頻繁に必要となる機能をまとめたものが多くあります．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e乱数を作成する random\u003c/li\u003e\n\u003cli\u003e時刻を取得できる datetime, time\u003c/li\u003e\n\u003cli\u003eファイルシステムへのアクセスを実現する os, shutil\u003c/li\u003e\n\u003cli\u003e基礎的な数学関数 math\u003c/li\u003e\n\u003cli\u003eHTMLサポート html\u003c/li\u003e\n\u003cli\u003eURLを使ってインターネットにアクセスできる urllib\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスマートフォンで言うところのカメラ，アルバム，時計などのデフォルトアプリみたいなものです．これらはPythonが動く環境であれば勝手に入っているため，\u003ccode\u003eimport ライブラリ名\u003c/code\u003eと記述すればすぐに利用することができます．助かる！！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# サイコロの出目を出力\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e random\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edice_roll \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e random\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandint(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;サイコロの出目:\u0026#34;\u003c/span\u003e, dice_roll)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"外部ライブラリ\"\u003e外部ライブラリ\u003c/h3\u003e\n\u003cp\u003e外部ライブラリはPythonにはもともと組み込まれておらず，自身で導入するタイプのライブラリのことです．実装したいプログラムの機能に応じて適切に選択する必要があります．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e効率的な多次元配列の数値計算を実現する Numpy\u003c/li\u003e\n\u003cli\u003e画像処理の機能を提供する OpenCV\u003c/li\u003e\n\u003cli\u003eグラフを簡単に描画できる Matplotlib\u003c/li\u003e\n\u003cli\u003e様々なアルゴリズムでの機械学習を可能とする scikit-learn\u003c/li\u003e\n\u003cli\u003eWebアプリの開発に役立つ Django\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスマートフォンで言うところのゲームやチャットアプリなどのストアアプリみたいなものです．これらは自身でインストール作業をする必要がありますが，それさえ済ませば高機能な実装をいとも容易く実現できます！とても便利！！\n有志のプログラマが開発して公開してくれているため，感謝の気持ちを忘れずに使いましょう（Pythonもそうだけど）．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# サイコロの出目を出力\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e np\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edice_roll \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandom\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandint(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;サイコロの出目:\u0026#34;\u003c/span\u003e, dice_roll)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"ライブラリを確認してみよう\"\u003eライブラリを確認してみよう\u003c/h2\u003e\n\u003cp\u003eでは，ここからは実際に手を動かしながら解説を進めます．\nまずはスタートメニューから \u003cstrong\u003eAnaconda Prompt\u003c/strong\u003e を開いてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"img_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"console_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこれは\u003cstrong\u003eコマンドプロンプト\u003c/strong\u003eと呼ばれる画面で，ここに\u003cstrong\u003eコマンド\u003c/strong\u003eと呼ばれる命令文を入力することでコンピュータを操作できます．\n試しに，ここに\u003ccode\u003etree\u003c/code\u003eと打ち込んでEnterキーを押してみてください．\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;何かツリー構造の記述がたくさん出てきたかと思います．これはディレクトリ構造をツリー形式で表示するコマンドです（あんまり使わないけど）！\nなかなか表示が終わらない場合はCtrl+Cキーでキャンセルしてください．\u003c/p\u003e\n\u003cp\u003eではコマンドを使って，今使っているPythonで利用可能な外部ライブラリの一覧を見てみましょう！\nAnaconda Promptに\u003ccode\u003econda list\u003c/code\u003eと打ち込んでEnterキーを押してみてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"modules.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eインストール済みライブラリとバージョンの一覧が表示されました．既にたくさん入っているようです！\u003c/p\u003e\n\u003cp\u003eよくよく見てみると，数値計算ができるNumpyや表データを扱えるPandasなど，データサイエンスに役立つライブラリがたくさんあります．\n実はAnacondaは科学計算のためのPython配布形式（ディストリビューション）の一つであり，既に役立つライブラリが揃えられているんです．\u003c/p\u003e\n\u003cp\u003eこの状態，一見プログラムを書くのに便利な環境であるように見えるんですが実はそうでないケースがあります．\u003c/p\u003e","title":"【入門1】Python仮想環境"},{"content":"JupyterLab+Visual Studio Codeを使ったPython実行 Visual Studio CodeにJupyter拡張機能をインストール 空のファイルを作成し，拡張子を.ipynbとする ＋コードボタンからコードブロックを作成し，Pythonプログラムを記述 右上カーネルの選択から実行したいPython環境を選択し コードブロック左側の実行ボタン▷を押下 ポップアップが出てきたらインストールをクリックしてipykernelをインストール （pip install ipykernelを当該環境で実行してインストールも出来ます） YOLOのセットアップ 今回は最新のYOLOを簡単に扱うことができるUltralyticsパッケージを利用します．\nconda create --name yolo-env python=3.11を実行して仮想環境を作成 conda activate yolo-envで仮想環境を有効化 pip install ultralyticsでUltralyticsパッケージをインストール ここまで済んだら，テストコードを動かしてみましょう！\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolo11n.pt\u0026#34;) results = model(\u0026#34;https://ultralytics.com/images/bus.jpg\u0026#34;) results[0].show() バスを背景にした画像の物体検出が表示されたはずです！\n作業ディレクトリを開くと，yolo11n.ptというファイルがダウンロードされています．これが，YOLOのモデルファイルです．これはYOLOのデフォルトの訓練済みモデルで，COCO Datasetに収録された80種類の物体を識別することが出来ます．\n","permalink":"http://localhost:1313/blog/posts/20250506_yolojupyter/","summary":"\u003ch2 id=\"jupyterlabvisual-studio-codeを使ったpython実行\"\u003eJupyterLab+Visual Studio Codeを使ったPython実行\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eVisual Studio CodeにJupyter拡張機能をインストール\u003c/li\u003e\n\u003cli\u003e空のファイルを作成し，拡張子を\u003ccode\u003e.ipynb\u003c/code\u003eとする\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e＋コード\u003c/code\u003eボタンからコードブロックを作成し，Pythonプログラムを記述\u003c/li\u003e\n\u003cli\u003e右上\u003ccode\u003eカーネルの選択\u003c/code\u003eから実行したいPython環境を選択し\u003c/li\u003e\n\u003cli\u003eコードブロック左側の実行ボタン\u003ccode\u003e▷\u003c/code\u003eを押下\u003c/li\u003e\n\u003cli\u003eポップアップが出てきたら\u003ccode\u003eインストール\u003c/code\u003eをクリックして\u003ccode\u003eipykernel\u003c/code\u003eをインストール\n（\u003ccode\u003epip install ipykernel\u003c/code\u003eを当該環境で実行してインストールも出来ます）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"vsjupyter.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"yoloのセットアップ\"\u003eYOLOのセットアップ\u003c/h2\u003e\n\u003cp\u003e今回は最新のYOLOを簡単に扱うことができるUltralyticsパッケージを利用します．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003econda create --name yolo-env python=3.11\u003c/code\u003eを実行して仮想環境を作成\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate yolo-env\u003c/code\u003eで仮想環境を有効化\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install ultralytics\u003c/code\u003eでUltralyticsパッケージをインストール\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eここまで済んだら，テストコードを動かしてみましょう！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e ultralytics \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e YOLO\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emodel \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e YOLO(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;yolo11n.pt\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresults \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://ultralytics.com/images/bus.jpg\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresults[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eバスを背景にした画像の物体検出が表示されたはずです！\u003c/p\u003e\n\u003cp\u003e作業ディレクトリを開くと，\u003ccode\u003eyolo11n.pt\u003c/code\u003eというファイルがダウンロードされています．これが，YOLOのモデルファイルです．これはYOLOのデフォルトの訓練済みモデルで，COCO Datasetに収録された80種類の物体を識別することが出来ます．\u003c/p\u003e","title":"【入門2】YOLO，Jupyterのセットアップ"},{"content":"アノテーションとは？ 機械学習の分類のひとつである教師あり学習では，まず入力データと出力データの組を複数用意し，それらを使って機械学習モデルを訓練します．\n入力データと出力データの組は，タスクによって様々です．例えば，\n入力：画像データ，出力：それぞれが犬or猫 入力：家の情報（間取り，立地，築年数など），出力：家の価格 入力：これまでの気温，出力：明日の気温 入力：画像データ，出力：各物体の座標 どんなタスクを前提とする場合でも，これら入力データと出力データの組を作る作業が必要となります．これがアノテーションです．\n環境構築 今回はYOLOによる画像物体認識を目標とします．そこで，アノテーションで作るべきデータは以下になります．\n入力：画像データ，出力：各物体の座標 頑張れば用意した画像の座標を数えてアノテーションできそうですが，今回はより簡単に作業を進めるためにLabelmeというツールを使います．GUIが整備されたソフトウェアですが，Pythonライブラリとして無償で提供されています！（ありがたい！！）\nAnaconda Promptを開く conda create -n annot_env python=3.12で仮想環境を作成 conda activate annot_envで仮想環境を有効化 pip install --upgrade labelmeと入力しLabelmeをインストール これでインストールは完了です．そのままannot_env環境内でlabelmeコマンドを実行してみてください！\nこの画面が表示されたら成功です！\nいざ，アノテーション 画像読み込み 今回は画像から麻雀牌を物体認識するタスクに挑戦してみます！サンプル画像は以下からダウンロード，解凍してください．\nサンプル画像 Zipファイルの解凍が済んだらLabelmeの画面左側にあるOpen Dirから，画像のディレクトリを開いてください．\nこれでアノテーションの準備は完了です！\n範囲選択，ラベル付け アノテーションの流れは以下になります．\n画像を右クリック，Create Rectangleをクリック 物体の範囲を四角形で囲んで選択，間違えたらEscキーで戻る 出てきたポップアップの上部Enter object labelにラベルを入力してOKをクリック 2と3を物体の数だけひたすら繰り返す 画像内のアノテーションが終わったら画面左部Saveから画像と同名のJSONファイルを保存 ちょっとやってみますね～\nはい，できました．ラベルは牌の種類（萬子筒子索子字牌）の略称です．\n\u0026hellip;\u0026hellip;\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\nめんどくさい\n機械学習では一般的にデータの数が大きいほど性能が上がりやすいとされています．つまり，（要件の要求レベルにもよりますが）精度を上げるためにはたくさんアノテーションする必要があるんです．\nそこで，特定のタスクの機械学習を考える場合はアノテーションに取り掛かる前に，データセットが利用可能なライセンスで公開されていないかどうか調べるところからスタートします．例えばYOLOの場合，Roboflow，VisualData，Google Open Images Datasetなどが画像データセットを公開している他，タスクに応じて論文と併せて公開されているデータセットも多数あります．\nまた今回はアノテーション入門ということで手作業での方法を紹介していますが，画像認識AIによる半自動ツールもあるので興味があれば調べてみてください．\n今回は，なんと，特別に，アノテーション済みファイルを用意してあります！\nアノテーション結果（JSON） ZIPファイルをダウンロード＆解凍したら，datasetsという名前のディレクトリを作成し，先程の画像ファイルとJSONファイルを以下の構造で配置してください．（image_1.jsonは自作のファイルで構いません！）\n.\r└── datasets/\r├── image_1.jpg\r├── image_1.json\r├── image_2.jpg\r├── image_2.json\r├── 〜省略〜\r├── image_10.jpg\r└── image_10.json データセットフォーマットの変換 では，学習させるデータの中身を一度見てみましょう！image_1.jsonを開いてみます．\nJSONはデータ記述形式のひとつで，キーと値を並べて書くのが特徴です．Pythonの辞書型そっくりですね． ラベルや座標がファイルの中に複数記述されていることがわかります．\n少しスクロールすると，よくわからない文字列が大量に記述されています．実はこれ，Base64と呼ばれるエンコード形式で画像そのものが書きつけられているんです．これにより，このJSONファイル単体で入力データを表現することが出来ます．\nさて，ひととおり学習データの中身を見てもらいましたが，実はこのままではYOLOの学習が出来ないんです．\nYOLOに学習させるためには，画像のアノテーションを以下の形式で記述する必要があります．\n画像ファイル名.txt\nlabel_id X Y Width Height 例えば，image_1.jsonをこの形式で記述すると以下のようになります．\nimage_1.txt\n4 0.171569 0.375817 0.142157 0.183007\r6 0.286765 0.382353 0.124183 0.183007\r0 0.401144 0.398693 0.127451 0.185185\r5 0.517157 0.408497 0.117647 0.178649\r10 0.638889 0.418301 0.125817 0.189542\r7 0.767157 0.436819 0.127451 0.174292 YOLOのデータセットでは，ラベルの番号，X座標，Y座標，幅，高さを列挙して記述します．\nこれを手作業で変換するのはかなり面倒です．\nそこで今回は，Labelmeで保存したJSONファイルをYOLOのデータセット形式に変換するツール，labelme2yoloを使います．condaのannot_env環境下で以下の手順で変換します．\npip install labelme2yoloでlabelme2yoloをインストール datasetディレクトリの一つ上の階層で以下のコマンドを実行 labelme2yolo --json_dir ./datasets/ --val_size 0.10 --output_format \u0026quot;bbox\u0026quot; これが済んだら，datasetsディレクトリの中にYOLODatasetが生成されているはずです．構造は以下のようになっています．\n.\r└── YOLODatasets/\r├── images/\r│ ├── train/\r│ │ ├── image_1.jpg\r│ │ └── ～省略～\r│ └── val/\r│ └── image_7.jpg\r├── labels/\r│ ├── train/\r│ │ ├── image_1.txt\r│ │ └── ～省略～\r│ └── val/\r│ └── image_7.txt\r└── dataset.yaml 先ほどのデータセット構造より複雑になっていますね．画像とラベルが別のフォルダに格納され，更にその中で訓練用(train)と評価用(val)に分かれています．\nlabelsディレクトリのテキストファイルを見てみましょう．YOLOのフォーマットに変換され，ラベルIDや座標が記録されているはずです．\ndataset.yamlの中身を見てみましょう．\nデータセットまでの絶対パス，訓練評価用ディレクトリのパス，ラベルIDとラベル名の対応が記載されています．YOLOの学習時にはこのdataset.yamlを利用してデータセットを読み込みます．\nYOLOの学習 それでは，いよいよ作ったデータセットを使ってYOLOの学習をしてみましょう！！\nまず，テスト用の画像を用意します．以下のリンクから画像ファイルimage_11.jpgをダウンロードしてdataset.yamlと同階層に配置します．\nテスト画像image_11.jpg dataset.yamlと同階層に.ipynbファイルを作成して，以下のPythonプログラムを実行してみましょう．\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolo11n.pt\u0026#34;) model.train(data=\u0026#39;dataset.yaml\u0026#39;, epochs=300, verbose=False) results = model(\u0026#34;image_11.jpg\u0026#34;) results[0].show() CPUのみの実行環境ではかなり時間がかかるので気長に待ちましょう～\n学習が終了すると，image_11.jpgに対する推論結果が表示されます．\n未検出や誤検出がちらほらありますが，部分的に検出が成功していることがわかります．データセットサイズやエポック数を変えることで性能の改善が期待できます．\nまた，データセットディレクトリのruns内を見ると，weightディレクトリにbest.ptが保存されていることがわかります． これは今回学習したモデルファイルで，実際に物体検出をシステム開発に組み込むときに使用します．\n","permalink":"http://localhost:1313/blog/posts/20250506_yoloannotation/","summary":"\u003ch2 id=\"アノテーションとは\"\u003eアノテーションとは？\u003c/h2\u003e\n\u003cp\u003e機械学習の分類のひとつである\u003cstrong\u003e教師あり学習\u003c/strong\u003eでは，まず入力データと出力データの組を複数用意し，それらを使って機械学習モデルを訓練します．\u003c/p\u003e\n\u003cp\u003e入力データと出力データの組は，タスクによって様々です．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e入力：画像データ，出力：それぞれが犬or猫\u003c/li\u003e\n\u003cli\u003e入力：家の情報（間取り，立地，築年数など），出力：家の価格\u003c/li\u003e\n\u003cli\u003e入力：これまでの気温，出力：明日の気温\u003c/li\u003e\n\u003cli\u003e入力：画像データ，出力：各物体の座標\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eどんなタスクを前提とする場合でも，これら\u003cstrong\u003e入力データと出力データの組を作る作業\u003c/strong\u003eが必要となります．これが\u003cstrong\u003eアノテーション\u003c/strong\u003eです．\u003c/p\u003e\n\u003ch2 id=\"環境構築\"\u003e環境構築\u003c/h2\u003e\n\u003cp\u003e今回はYOLOによる画像物体認識を目標とします．そこで，アノテーションで作るべきデータは以下になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e入力：画像データ，出力：各物体の座標\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e頑張れば用意した画像の座標を数えてアノテーションできそうですが，今回はより簡単に作業を進めるために\u003cstrong\u003eLabelme\u003c/strong\u003eというツールを使います．GUIが整備されたソフトウェアですが，Pythonライブラリとして無償で提供されています！（ありがたい！！）\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAnaconda Promptを開く\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda create -n annot_env python=3.12\u003c/code\u003eで仮想環境を作成\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate annot_env\u003c/code\u003eで仮想環境を有効化\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install --upgrade labelme\u003c/code\u003eと入力しLabelmeをインストール\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eこれでインストールは完了です．そのまま\u003ccode\u003eannot_env\u003c/code\u003e環境内で\u003ccode\u003elabelme\u003c/code\u003eコマンドを実行してみてください！\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"labelme.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこの画面が表示されたら成功です！\u003c/p\u003e\n\u003ch2 id=\"いざアノテーション\"\u003eいざ，アノテーション\u003c/h2\u003e\n\u003ch3 id=\"画像読み込み\"\u003e画像読み込み\u003c/h3\u003e\n\u003cp\u003e今回は画像から麻雀牌を物体認識するタスクに挑戦してみます！サンプル画像は以下からダウンロード，解凍してください．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hakoshi-normal/yolo_sample/raw/refs/heads/main/images/images.zip\"\u003eサンプル画像\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eZipファイルの解凍が済んだらLabelmeの画面左側にある\u003ccode\u003eOpen Dir\u003c/code\u003eから，画像のディレクトリを開いてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"labelme_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこれでアノテーションの準備は完了です！\u003c/p\u003e\n\u003ch3 id=\"範囲選択ラベル付け\"\u003e範囲選択，ラベル付け\u003c/h3\u003e\n\u003cp\u003eアノテーションの流れは以下になります．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e画像を右クリック，\u003ccode\u003eCreate Rectangle\u003c/code\u003eをクリック\u003c/li\u003e\n\u003cli\u003e物体の範囲を四角形で囲んで選択，間違えたら\u003ccode\u003eEsc\u003c/code\u003eキーで戻る\u003c/li\u003e\n\u003cli\u003e出てきたポップアップの上部\u003ccode\u003eEnter object label\u003c/code\u003eにラベルを入力して\u003ccode\u003eOK\u003c/code\u003eをクリック\u003c/li\u003e\n\u003cli\u003e2と3を物体の数だけひたすら繰り返す\u003c/li\u003e\n\u003cli\u003e画像内のアノテーションが終わったら画面左部\u003ccode\u003eSave\u003c/code\u003eから画像と同名のJSONファイルを保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eちょっとやってみますね～\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"annotated.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eはい，できました．ラベルは牌の種類（萬子筒子索子字牌）の略称です．\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 150%; color: red;\"\u003eめんどくさい\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e機械学習では一般的にデータの数が大きいほど性能が上がりやすいとされています．つまり，（要件の要求レベルにもよりますが）精度を上げるためにはたくさんアノテーションする必要があるんです．\u003c/p\u003e\n\u003cp\u003eそこで，特定のタスクの機械学習を考える場合はアノテーションに取り掛かる前に，データセットが利用可能なライセンスで公開されていないかどうか調べるところからスタートします．例えばYOLOの場合，\u003ca href=\"https://universe.roboflow.com/\"\u003eRoboflow\u003c/a\u003e，\u003ca href=\"https://visualdata.io/discovery\"\u003eVisualData\u003c/a\u003e，\u003ca href=\"https://storage.googleapis.com/openimages/web/index.html\"\u003eGoogle Open Images Dataset\u003c/a\u003eなどが画像データセットを公開している他，タスクに応じて論文と併せて公開されているデータセットも多数あります．\u003c/p\u003e\n\u003cp\u003eまた今回はアノテーション入門ということで手作業での方法を紹介していますが，画像認識AIによる半自動ツールもあるので興味があれば調べてみてください．\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e今回は，なんと，特別に，アノテーション済みファイルを用意してあります！\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hakoshi-normal/yolo_sample/raw/refs/heads/main/images/labels.zip\"\u003eアノテーション結果（JSON）\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eZIPファイルをダウンロード＆解凍したら，\u003ccode\u003edatasets\u003c/code\u003eという名前のディレクトリを作成し，先程の画像ファイルとJSONファイルを以下の構造で配置してください．（\u003ccode\u003eimage_1.json\u003c/code\u003eは自作のファイルで構いません！）\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-planetext\" data-lang=\"planetext\"\u003e.\r\n└── datasets/\r\n    ├── image_1.jpg\r\n    ├── image_1.json\r\n    ├── image_2.jpg\r\n    ├── image_2.json\r\n    ├── 〜省略〜\r\n    ├── image_10.jpg\r\n    └── image_10.json\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"データセットフォーマットの変換\"\u003eデータセットフォーマットの変換\u003c/h3\u003e\n\u003cp\u003eでは，学習させるデータの中身を一度見てみましょう！\u003ccode\u003eimage_1.json\u003c/code\u003eを開いてみます．\u003c/p\u003e","title":"【入門3】YOLOアノテーションと学習"},{"content":"はじめに 自動採譜プロジェクトの息抜き的な記事です．\nせっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして， ところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．\nカメラ切り替えトリガー カメラの切り替えトリガーを「牌山にプレーヤーが触れたタイミング」と設定しました．\n自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．\nツモ位置の検出 まず，天カメの映像から卓上のツモ位置を特定する必要があります．\n牌山の形を見てみましょう．\n対局開始前はこんな感じ．\n対局開始直後はこんな感じ．\n対局中はこんな感じ．\n対局開始前は4つ牌山がありますが，対局開始以降は牌山が必ず3つ以下になります王牌の扱い上，4つのときもあります．\n牌山が3つのとき\n牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．\n牌山が4つのとき\n一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．\n実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．\n牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．\n実装概要 元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます． cv2.inRange()を使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．\n二値化した画像に対し，cv2.findContours()を使用し，牌山となりうる矩形を検出します．\n手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます． その後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．\nツモプレイヤーの検出 ツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します． MediaPipe のハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します． 今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．\nプレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．\nオートスイッチャー実装！！！ OBSを使用して配信するため，カメラ切り替えには obs-websocket-py を使用しました．こちらの サンプル がそのまま動作して助かりました．\nGUI実装にはHTML/JSでの記述が可能な Eel を使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．\n実際の動作の様子がこちら．\nYour browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．\nあと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．\n","permalink":"http://localhost:1313/blog/posts/20240210_mahjongautocamswitcher/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e自動採譜プロジェクト\u003c/a\u003eの息抜き的な記事です．\u003c/p\u003e\n\u003cp\u003eせっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして，\nところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．\u003c/p\u003e\n\u003ch2 id=\"カメラ切り替えトリガー\"\u003eカメラ切り替えトリガー\u003c/h2\u003e\n\u003cp\u003eカメラの切り替えトリガーを「\u003cstrong\u003e牌山にプレーヤーが触れたタイミング\u003c/strong\u003e」と設定しました．\u003c/p\u003e\n\u003cp\u003e自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．\u003c/p\u003e\n\u003ch2 id=\"ツモ位置の検出\"\u003eツモ位置の検出\u003c/h2\u003e\n\u003cp\u003eまず，天カメの映像から卓上のツモ位置を特定する必要があります．\u003cbr\u003e牌山の形を見てみましょう．\u003c/p\u003e\n\u003cp\u003e対局開始前はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_0.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局開始直後はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局中はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_2.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局開始前は4つ牌山がありますが，対局開始以降は牌山が\u003cdel\u003e必ず3つ以下になります\u003c/del\u003e王牌の扱い上，4つのときもあります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e牌山が3つのとき\u003c/p\u003e\n\u003cp\u003e牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e牌山が4つのとき\u003c/p\u003e\n\u003cp\u003e一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"fig1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．\u003c/p\u003e\n\u003cp\u003e牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．\u003c/p\u003e\n\u003ch3 id=\"実装概要\"\u003e実装概要\u003c/h3\u003e\n\u003cp\u003e元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます．\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"original_image.png\"/\u003e \r\n\u003c/figure\u003e\r\n\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecv2.inRange()\u003c/code\u003eを使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"mask_hai.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e二値化した画像に対し，\u003ccode\u003ecv2.findContours()\u003c/code\u003eを使用し，牌山となりうる矩形を検出します．\u003c/p\u003e\n\u003cp\u003e手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます．\nその後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumohai.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"ツモプレイヤーの検出\"\u003eツモプレイヤーの検出\u003c/h2\u003e\n\u003cp\u003eツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します．\n\u003ca href=\"https://developers.google.com/mediapipe/solutions/vision/hand_landmarker\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eMediaPipe\u003c/a\u003e\nのハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します．\n今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumo1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eプレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumo2.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"オートスイッチャー実装\"\u003eオートスイッチャー実装！！！\u003c/h2\u003e\n\u003cp\u003eOBSを使用して配信するため，カメラ切り替えには\n\u003ca href=\"https://github.com/Elektordi/obs-websocket-py/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eobs-websocket-py\u003c/a\u003e\nを使用しました．こちらの\n\u003ca href=\"https://github.com/Elektordi/obs-websocket-py/blob/master/samples/switch_scenes.py\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eサンプル\u003c/a\u003e\nがそのまま動作して助かりました．\u003c/p\u003e\n\u003cp\u003eGUI実装にはHTML/JSでの記述が可能な\n\u003ca href=\"https://github.com/python-eel/Eel\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eEel\u003c/a\u003e\nを使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．\u003c/p\u003e\n\u003cp\u003e実際の動作の様子がこちら．\u003c/p\u003e\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\n\u003cp\u003e現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．\u003c/p\u003e\n\u003cp\u003eあと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．\u003c/p\u003e","title":"麻雀放送対局用のオートスイッチャーを作りたい"},{"content":"はじめに 複数カメラの入力に対しYOLOv8で並列推論するのソースコード解説です． 解説のため，一部表現を変えている部分があります．\n大まかな流れは以下の通り．\n共有メモリ確保 フレーム入力プロセス カメラからフレームを取得 フレームを共有メモリ1に保存 推論プロセス 共有フレームを呼び出し 推論 推論結果を描画したフレームを共有メモリ2に保存 フレーム表示プロセス 共有フレームを呼び出し フレームの画面描画 プロセス間の遷移はmultiprocessingモジュールのEvent関数をフラグとして利用します． カメラの台数が増えた場合はプロセス数を増やせば対応できます．\n事前準備 設定情報 cam_N = 4 # カメラ台数 proccess_N = 3 # カメラ1台あたりのプロセス数 frame_info = {\u0026#34;nbytes\u0026#34; : frame.nbytes, \u0026#34;shape\u0026#34; : frame.shape, \u0026#34;dtype\u0026#34; : frame.dtype} フレームのサイズや型はメモリ確保，呼び出し時に必要になります．\n共有メモリの確保 shm = shared_memory.SharedMemory(create=True, size=frame.nbytes, name = mem_name) 共有メモリの確保はこの一文でできます．めっちゃ便利ですね． ただし，mem_nameを動的に作成して複数の共有メモリを確保する場合，注意事項があります． shared_memory.SharedMemoryの戻り値を格納する変数（上記コードではshm）を設定しなかった場合や上書きした場合，それ以前の共有メモリ領域は参照できなくなります．その場合はshmを格納するリストを用意しておくとアクセス可能になります．\nプロセスの立ち上げ p = Process(target=関数名, args=(引数,)) p.start() # プロセス開始 p.join() # プロセス終了 複数プロセスを立ち上げる場合は p をリストに入れておくと管理が楽です．\nフレーム入力プロセス def rec_cam(cam_id, pre_flags, frame_info): cap = cv2.VideoCapture(cam_id) while True: _, frame = cap.read() idx = (cam_id)*proccess_N+counter%proccess_N mem_name = f\u0026#39;shared{idx+1:02}\u0026#39; shm = shared_memory.SharedMemory(name=mem_name) frame_sh = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) frame_sh[:] = frame[:] # 推論開始 pre_flags[idx].set() cap.close() multiprocessingのSharedMemoryでは共有メモリを名前をつけて呼び出すことができます．便利！ ここでは，\u0026ldquo;shared1[番号]\u0026ldquo;という名前で共有メモリを確保している（後述）．カメラ台数✕カメラあたりのプロセス数分の共有メモリ領域が必要になるため， あるカメラのあるフレームに対する保存領域は，カメラ番号✕カメラ1台あたりのプロセス数＋フレーム番号%カメラ1台あたりのプロセス数で求めることができる．と同時にこの番号は推論プロセスを動作させるフラグ管理のインデックスとしても使う．\n推論プロセス def predict_frame(pre_flag, view_flag, mem_name, cam_id, frame_info): while True: if pre_flag.is_set(): # 推論開始 pre_flag.clear() shm = shared_memory.SharedMemory(name=mem_name) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # 重い推論処理がここに入る（frameに結果を反映する） # 結果統合用共有メモリにのせる shm_result = shared_memory.SharedMemory(name=f\u0026#39;shared2{cam_id}\u0026#39;) frame_new = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm_result.buf) frame_new[:]=frame[:] # 結果表示 view_flag.set() カメラ台数✕カメラあたりのプロセス数分の推論プロセスを立ち上げます． 推論開始用のフラグや共有メモリの呼び出し名は予め与えた状態でプロセスを立ち上げる．フレーム入力プロセスでの推論開始用フラグの有効化を待ち受けています． 処理済みのフレーを格納する共有メモリ領域名は\u0026rsquo;shared2[番号]\u0026lsquo;としており，カメラの台数分事前に確保してあります． 検出結果を描画する必要がない場合は，フレーム表示プロセスを介する必要がないため，別プロセスに推論結果を渡したり，UDP通信などで端末間の結果送信が実装できると思います．\nフレーム表示プロセス def view_frame(view_flag, cam_id, frame_info): while True: if view_flag.is_set(): view_flag.clear() shm = shared_memory.SharedMemory(name=f\u0026#39;shared2{cam_id}\u0026#39;) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) cv2.imshow(f\u0026#39;window_{cam_id}\u0026#39;, frame) cv2.waitKey(1) 推論プロセスでのフレーム表示用フラグの有効化を待ち受けています．\nおわりに 今回は，以前実装した並列画像処理プログラムの解説でした．コードブロックを断片的に示しただけなので，具体的な実装コードを知りたい方はソースコードから参照できます．\n","permalink":"http://localhost:1313/blog/posts/20231226_multiprocessingforrealtimevideo/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/2023_12_01_multiprocessingforyolov8/\" target=\"_blank\"\u003e複数カメラの入力に対しYOLOv8で並列推論する\u003c/a\u003eのソースコード解説です．\n解説のため，一部表現を変えている部分があります．\u003c/p\u003e\n\u003cp\u003e大まかな流れは以下の通り．\u003c/p\u003e\n\u003col start=\"0\"\u003e\n\u003cli\u003e共有メモリ確保\u003c/li\u003e\n\u003cli\u003eフレーム入力プロセス\n\u003col\u003e\n\u003cli\u003eカメラからフレームを取得\u003c/li\u003e\n\u003cli\u003eフレームを共有メモリ1に保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e推論プロセス\n\u003col\u003e\n\u003cli\u003e共有フレームを呼び出し\u003c/li\u003e\n\u003cli\u003e推論\u003c/li\u003e\n\u003cli\u003e推論結果を描画したフレームを共有メモリ2に保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eフレーム表示プロセス\n\u003col\u003e\n\u003cli\u003e共有フレームを呼び出し\u003c/li\u003e\n\u003cli\u003eフレームの画面描画\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eプロセス間の遷移はmultiprocessingモジュールのEvent関数をフラグとして利用します．\nカメラの台数が増えた場合はプロセス数を増やせば対応できます．\u003c/p\u003e\n\u003ch2 id=\"事前準備\"\u003e事前準備\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e設定情報\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecam_N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# カメラ台数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eproccess_N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# カメラ1台あたりのプロセス数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eframe_info \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;nbytes\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enbytes,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;shape\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshape,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dtype\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edtype}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eフレームのサイズや型はメモリ確保，呼び出し時に必要になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e共有メモリの確保\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eshm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e shared_memory\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSharedMemory(create\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enbytes, name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e mem_name)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e共有メモリの確保はこの一文でできます．めっちゃ便利ですね．\nただし，mem_nameを動的に作成して複数の共有メモリを確保する場合，注意事項があります．\nshared_memory.SharedMemoryの戻り値を格納する変数（上記コードではshm）を設定しなかった場合や上書きした場合，それ以前の共有メモリ領域は参照できなくなります．その場合はshmを格納するリストを用意しておくとアクセス可能になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプロセスの立ち上げ\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Process(target\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e関数名, args\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e(引数,))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estart() \u003cspan style=\"color:#75715e\"\u003e# プロセス開始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ejoin() \u003cspan style=\"color:#75715e\"\u003e# プロセス終了\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e複数プロセスを立ち上げる場合は p をリストに入れておくと管理が楽です．\u003c/p\u003e\n\u003ch2 id=\"フレーム入力プロセス\"\u003eフレーム入力プロセス\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erec_cam\u003c/span\u003e(cam_id, pre_flags, frame_info):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cv2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eVideoCapture(cam_id)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            _, frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            idx \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (cam_id)\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003eproccess_N\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003ecounter\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003eproccess_N\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            mem_name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;shared\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003eidx\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e02\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            shm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e shared_memory\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSharedMemory(name\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003emem_name)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            frame_sh \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003endarray(shape\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;shape\u0026#34;\u003c/span\u003e], dtype\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dtype\u0026#34;\u003c/span\u003e], buffer\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eshm\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebuf)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            frame_sh[:] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e frame[:]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# 推論開始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            pre_flags[idx]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eset()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003emultiprocessingのSharedMemoryでは共有メモリを名前をつけて呼び出すことができます．便利！\nここでは，\u0026ldquo;shared1[番号]\u0026ldquo;という名前で共有メモリを確保している（後述）．カメラ台数✕カメラあたりのプロセス数分の共有メモリ領域が必要になるため，\nあるカメラのあるフレームに対する保存領域は，カメラ番号✕カメラ1台あたりのプロセス数＋フレーム番号%カメラ1台あたりのプロセス数で求めることができる．と同時にこの番号は推論プロセスを動作させるフラグ管理のインデックスとしても使う．\u003c/p\u003e","title":"リアルタイムの映像入力に対してFPSを落とさずに画像処理したい"},{"content":"ビデオカメラからOpenCVで映像取得（低品質） 麻雀自動採譜の実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．\nビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できるビデオキャプチャカードを導入． これでビデオカメラをWebカメラみたく使えるぞ！やった！！\nと思った矢先，\nなんか遅い気がする！！\nよくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，1秒遅延＋低FPS＋低画質という散々な結果に\nお前 Full HD で 30FPS 出るって言ってたじゃないか\u0026hellip;\u0026hellip;\nキャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．\nRTSP通信でカメラの映像を受信してみる\nこのときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった． 今回もPyAVを使えば解決するかもしれない．\nビデオカメラからPyAVで映像取得 PyAVでPCに有線接続されたカメラから映像を取得する．\nimport cv2 import av con_options = dict( video_size=\u0026#39;1920x1080\u0026#39;, vcodec=\u0026#39;mjpeg\u0026#39;, framerate=\u0026#39;30\u0026#39;, rtbufsize=\u0026#39;1\u0026#39;, ) device_name = \u0026#34;USB Video\u0026#34; con_def = dict( format=\u0026#39;dshow\u0026#39;, file=f\u0026#39;video={device_name}\u0026#39;, options=dict(con_options, video_device_number=\u0026#39;0\u0026#39;) ) container = av.open(**con_def) for frame in container.decode(video=0): frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) # cv2.imshow(\u0026#34;test_window\u0026#34;, frame) # cv2.waitKey(1) ちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．\nffmpeg -list_devices true -f dshow -i dummy PyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．\nこれで遅延ほぼなし，30FPSでの映像受信ができた．\n","permalink":"http://localhost:1313/blog/posts/20231225_getframebypyav/","summary":"\u003ch2 id=\"ビデオカメラからopencvで映像取得低品質\"\u003eビデオカメラからOpenCVで映像取得（低品質）\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003eの実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．\u003c/p\u003e\n\u003cp\u003eビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できる\u003ca href=\"https://amzn.asia/d/jclJASJ\"\u003eビデオキャプチャカード\u003c/a\u003eを導入．\nこれでビデオカメラをWebカメラみたく使えるぞ！やった！！\u003c/p\u003e\n\u003cp\u003eと思った矢先，\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eなんか遅い気がする！！\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eよくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，\u003cstrong\u003e1秒遅延＋低FPS＋低画質\u003c/strong\u003eという散々な結果に\u003c/p\u003e\n\u003cp\u003eお前 Full HD で 30FPS 出るって言ってたじゃないか\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003eキャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/blog/2023_12_01_getframertsp/\"\u003eRTSP通信でカメラの映像を受信してみる\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこのときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった．\n今回も\u003ca href=\"https://pyav.org/docs/develop/index.html\" target=\"_blank\"\u003ePyAV\u003c/a\u003eを使えば解決するかもしれない．\u003c/p\u003e\n\u003ch2 id=\"ビデオカメラからpyavで映像取得\"\u003eビデオカメラからPyAVで映像取得\u003c/h2\u003e\n\u003cp\u003ePyAVでPCに有線接続されたカメラから映像を取得する．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e cv2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e av\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econ_options  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  dict(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    video_size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1920x1080\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    vcodec\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;mjpeg\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    framerate\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;30\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    rtbufsize\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edevice_name  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;USB Video\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econ_def  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  dict(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;dshow\u0026#39;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    file\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;video=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003edevice_name\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    options\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003edict(con_options, video_device_number\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;0\u0026#39;\u003c/span\u003e) )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econtainer \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e av\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eopen(\u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003econ_def)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e frame \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e container\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edecode(video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto_ndarray(format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;bgr24\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# cv2.imshow(\u0026#34;test_window\u0026#34;, frame)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# cv2.waitKey(1)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-console\" data-lang=\"console\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003effmpeg -list_devices true -f dshow -i dummy\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．\u003c/p\u003e","title":"映像入力の遅延をPyAVで解決してみる"},{"content":" multiprocessing × YOLOv8の詳細解説 Denoを利用した基本的なWebサイト作成 Deno Deploy設定 ","permalink":"http://localhost:1313/blog/posts/20231212_scheduled_update/","summary":"\u003cul\u003e\n\u003cli\u003emultiprocessing × YOLOv8の詳細解説\u003c/li\u003e\n\u003cli\u003eDenoを利用した基本的なWebサイト作成\u003c/li\u003e\n\u003cli\u003eDeno Deploy設定\u003c/li\u003e\n\u003c/ul\u003e","title":"今後の更新予定"},{"content":"RTSPでフレーム取得（OpenCV） RTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．\nimport cv2 url = \u0026#34;rtsp://username:password@ipaddress\u0026#34; # Tapo C200は末尾に画質を指定 url = \u0026#34;rtsp://username:password@ipaddress/stream1\u0026#34; cap = cv2.VideoCapture(url) while True: ret, frame = cap.read() pass ただこれだとTapo C200では1秒程度遅延が発生してしまう． 麻雀自動採譜 の実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．\nRTSPでフレーム取得（PyAV） RTSP映像の受信にはFFmpegのPythonバインディングである PyAV を利用した方法もあり、以下で映像受信が可能でした．\nimport av\rurl = \u0026#34;rtsp://username:password@ipaddress\u0026#34;\rcontainer = av.open(url)\rfor frame in container.decode(video=0):\rframe = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;)\rpass PyAVを利用した場合は遅延が大幅に減少した．やったね．\nRTSPで一定時間毎にフレーム取得（PyAV） 麻雀自動採譜 牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚でtime.sleep(n)を使用したところ以下のエラーが複数出現．\nmax delay reached. need to consume packet\rRTP: missed 6066 packets\rRTP: PT=60: bad cseq a657 expected=8ea5\rmax delay reached. need to consume packet sleepで無理に処理を中断させたため接続が不安定になったのかも． 通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．\nimport av interval = 10 # 10秒ごとに撮影 url = \u0026#34;rtsp://username:password@ipaddress\u0026#34; container = av.open(url) counter = 0 for frame in container.decode(video=0): if frame is not None and frame.time//interval\u0026gt;counter: counter+=1 frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) cv2.imwrite(f\u0026#39;save_{counter:04}.png\u0026#39;, frame) PyAVのcontainerから呼び出したフレームは撮影開始を0としたときの撮影時刻をtimeで取得できる． 撮影開始後数フレームはNoneが返ることがあるため注意． これで安定したn秒毎の撮影が実行できる．\n","permalink":"http://localhost:1313/blog/posts/20231201_getframertsp/","summary":"\u003ch2 id=\"rtspでフレーム取得opencv\"\u003eRTSPでフレーム取得（OpenCV）\u003c/h2\u003e\n\u003cp\u003eRTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e cv2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rtsp://username:password@ipaddress\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Tapo C200は末尾に画質を指定\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rtsp://username:password@ipaddress/stream1\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cv2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eVideoCapture(url)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ret, frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eただこれだとTapo C200では\u003cstrong\u003e1秒程度遅延\u003c/strong\u003eが発生してしまう．\n\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003e\nの実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．\u003c/p\u003e\n\u003ch2 id=\"rtspでフレーム取得pyav\"\u003eRTSPでフレーム取得（PyAV）\u003c/h2\u003e\n\u003cp\u003eRTSP映像の受信にはFFmpegのPythonバインディングである\n\u003ca href=\"https://pyav.org/docs/develop/index.html\" target=\"_blank\"\u003ePyAV\u003c/a\u003e\nを利用した方法もあり、以下で映像受信が可能でした．\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-python:\" data-lang=\"python:\"\u003eimport av\r\n\r\nurl = \u0026#34;rtsp://username:password@ipaddress\u0026#34;\r\ncontainer = av.open(url)\r\nfor frame in container.decode(video=0):\r\n    frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;)\r\n    pass\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ePyAVを利用した場合は遅延が大幅に減少した．やったね．\u003c/p\u003e\n\u003ch2 id=\"rtspで一定時間毎にフレーム取得pyav\"\u003eRTSPで一定時間毎にフレーム取得（PyAV）\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003e\n牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚で\u003ccode\u003etime.sleep(n)\u003c/code\u003eを使用したところ以下のエラーが複数出現．\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-raw:\" data-lang=\"raw:\"\u003emax delay reached. need to consume packet\r\nRTP: missed 6066 packets\r\nRTP: PT=60: bad cseq a657 expected=8ea5\r\nmax delay reached. need to consume packet\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003esleepで無理に処理を中断させたため接続が不安定になったのかも．\n通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．\u003c/p\u003e","title":"RTSP通信でカメラの映像を受信してみる"},{"content":"実装過程あれこれ１ これまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の Tapo C200 からフレームを受信することができるようになりました． 早速各カメラに割り当てたプロセス毎に適当に学習させておいた YOLOv8 による牌推論を実行し，結果を描画してみました．わくわく！\nはい，映像出力が不穏な感じに． Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．\nプロセス間でまったく映像出力のタイミングがあっていない フレーム間隔がまちまち 映像の乱れが酷い FPSが低い（Tapo入力時15→10弱） 特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね\u0026hellip;\u0026hellip;\nフレーム出力がばらつく理由を勝手に予想．こんな感じだろうか\n出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える． こんなときこそマルチプロセス処理の出番でしょ↓\n（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）\n既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．\n映像受信プロセス ×4（カメラの台数） 推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当） 推論結果統合プロセス ×4（カメラの台数） さあやるぞ！\n実装過程あれこれ２ ここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．\nbase64→内部UDPが一瞬頭を過るが無視して 公式ドキュメント を漁ってみます．\nドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか． Value, Array, Queue, Pipe, RawArray, Manager, shared_memory\u0026hellip;\u0026hellip;\n多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．\n試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに． フラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．\nそんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります． Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r概ね同じタイミングでフレームが描画されているように見えますね．\nFPSはいずれのカメラも15程度であり，入力時のFPSを保持したままリアルタイムで推論が実行できました． ちなみに，\nmultiprocessingではなくthreadingを使ってYOLOv8のモデルをスレッド間で共有する方法があるが，提供元である UltralyticsのMedium記事 では推奨方法， 公式ドキュメント では非推奨方法として紹介されていたため今回は見送り． 試験的に使用してるYOLOv8モデルは，現環境（GeForce RTX 3070）で学習可能な最大サイズのモデル（YOLOv8l）に対して複数の公開データセットを統合した5万枚強の牌画像を学習させたものである．統合作業時にデータセット間でラベルのズレが発生したため，検出結果に誤りがある．ラベルが混合した状態のデータセットではあるが安定した検出ができているため，データセットの整理によって更に高精度な検出の実現が期待できる． 背景部分の予期せぬ牌検出は信頼度で棄却できるため問題なし． 検証動画は双方画面録画の仕様上FPSが低下している． ソースコード 複数台のカメラのフレーム入力に対してYOLOv8による推論を並列実行するPythonプログラム ソースコード解説はこちらからどうぞ．\nimport time import msvcrt from multiprocessing import Process, Event, shared_memory import numpy as np import av import cv2 from ultralytics import YOLO # RTSPカメラ設定情報 info_list = [[\u0026#34;toncamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;nancamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;shacamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;peicamera\u0026#34;, \u0026#34;*\u0026#34;]] password = \u0026#34;*\u0026#34; # 1:1280x720, 2:640x360 mode = 1 # 推論プロセス数指定 N = 3 def rec_cam(cam_id, username, ipaddress, pre_flags, stop_flag, frame_info): url = f\u0026#34;rtsp://{username}:{password}@{ipaddress}:554/stream{mode}\u0026#34; container = av.open(url) counter = 0 start = time.time() for frame in container.decode(video=0): counter+=1 frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) # 共有メモリにフレームを書き込み idx = (cam_id)*N+counter%N mem_name = f\u0026#39;shared{idx+1:02}\u0026#39; shm = shared_memory.SharedMemory(name=mem_name) frame_sh = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) frame_sh[:] = frame[:] # 推論開始 pre_flags[idx].set() if stop_flag.is_set(): t = time.time() - start print(f\u0026#34;{username}, TIME:{round(t, 3)}, FRAME:{counter}, FPS:{round(counter/t, 3)}\u0026#34;) break container.close() cv2.destroyAllWindows() return def finish_monitor(stop_flag): while True: if msvcrt.kbhit() and msvcrt.getch() == b\u0026#39;q\u0026#39;: print(\u0026#34;Finished\u0026#34;) stop_flag.set() break return def predict_frame(pre_flag, pre_fin_flag, mem_name, stop_flag, username, frame_info): model = YOLO(\u0026#39;best.pt\u0026#39;) while True: if pre_flag.is_set(): # 推論 pre_flag.clear() shm = shared_memory.SharedMemory(name=mem_name) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # YOLOv8による推論実行 result = model(frame)[0] frame = result.plot() # 結果統合用共有メモリにのせる shm_result = shared_memory.SharedMemory(name=username) frame_new = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm_result.buf) frame_new[:]=frame[:] # 結果統合用 pre_fin_flag.set() if stop_flag.is_set(): break def view_frame(pre_fin_flag, username, stop_flag, frame_info): while True: if pre_fin_flag.is_set(): pre_fin_flag.clear() shm = shared_memory.SharedMemory(name=username) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # Full-HDだと大抵画面が埋まっちゃう # frame = cv2.resize(frame, None, None, 0.6, 0.6) cv2.imshow(username, frame) cv2.waitKey(1) if stop_flag.is_set(): break if __name__ == \u0026#39;__main__\u0026#39;: # メモリ確保情報 url = f\u0026#34;rtsp://{info_list[0][0]}:{password}@{info_list[0][1]}:554/stream{mode}\u0026#34; container = av.open(url) for i, frame in enumerate(container.decode(video=0)): frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) if frame is not None: break container.close() frame_info = {\u0026#34;nbytes\u0026#34; : frame.nbytes, \u0026#34;shape\u0026#34; : frame.shape, \u0026#34;dtype\u0026#34; : frame.dtype} # 停止プロセス stop_flag = Event() p = Process(target=finish_monitor, args=(stop_flag,)) p.start() # 表示プロセス群（カメラ台数分） view_processes = [] pre_fin_flags = [] mem_space = [] for i in range(4): pre_fin_flag = Event() pre_fin_flags.append(pre_fin_flag) username = info_list[i][0] # メモリ確保 今回は画像用 shm = shared_memory.SharedMemory(create=True, size=frame_info[\u0026#34;nbytes\u0026#34;], name = username) mem_space.append(shm) p = Process(target=view_frame, args=(pre_fin_flag, username, stop_flag, frame_info)) view_processes.append(p) p.start() # 推論プロセス群（カメラ台数×推論プロセス数） pre_processes = [] pre_flags = [] mem_names = [] for i in range(N*len(info_list)): pre_flag = Event() pre_flags.append(pre_flag) mem_name = f\u0026#39;shared{i+1:02}\u0026#39; mem_names.append(mem_name) # メモリ確保 shm = shared_memory.SharedMemory(create=True, size=frame_info[\u0026#34;nbytes\u0026#34;], name = mem_name) mem_space.append(shm) # frame_sh = np.ndarray(shape=frame_shape, dtype=frame_dtype, buffer=shm.buf) username = info_list[i//N][0] pre_fin_flag = pre_fin_flags[i//N] p = Process(target=predict_frame, args=(pre_flag, pre_fin_flag, mem_name, stop_flag, username, frame_info)) pre_processes.append(p) p.start() # 撮影プロセス群（カメラ台数） rec_processes = [] for cam_id, info in enumerate(info_list): p = Process(target=rec_cam, args=(cam_id, info[0], info[1], pre_flags, stop_flag, frame_info,)) rec_processes.append(p) p.start() p.join() for p in view_processes: p.join() for p in pre_processes: p.join() for p in rec_processes: p.join() # 共有メモリのリソース開放、削除 for mem_name in mem_names + [info[0] for info in info_list[:4]]: shm = shared_memory.SharedMemory(name = mem_name) shm.close() shm.unlink() ","permalink":"http://localhost:1313/blog/posts/20231201_multiprocessingforyolov8/","summary":"\u003ch2 id=\"実装過程あれこれ１\"\u003e実装過程あれこれ１\u003c/h2\u003e\n\u003cp\u003eこれまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の\n\u003ca href=\"https://www.tp-link.com/jp/smart-home/tapo/tapo-c200/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eTapo C200\u003c/a\u003e\nからフレームを受信することができるようになりました．\n\u003cbr\u003e\n早速各カメラに割り当てたプロセス毎に適当に学習させておいた\n\u003ca href=\"https://docs.ultralytics.com/ja/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eYOLOv8\u003c/a\u003e\nによる牌推論を実行し，結果を描画してみました．わくわく！\u003c/p\u003e\n\u003cp\u003eはい，映像出力が不穏な感じに．\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20231201_multiprocessingforyolov8/before.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20231201_multiprocessingforyolov8/before.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\u003c/p\u003e\n\u003cp\u003e卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプロセス間でまったく映像出力のタイミングがあっていない\u003c/li\u003e\n\u003cli\u003eフレーム間隔がまちまち\u003c/li\u003e\n\u003cli\u003e映像の乱れが酷い\u003c/li\u003e\n\u003cli\u003eFPSが低い（Tapo入力時15→10弱）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね\u0026hellip;\u0026hellip;\u003cbr\u003e\u003c/p\u003e\n\u003cp\u003eフレーム出力がばらつく理由を勝手に予想．こんな感じだろうか\u003c/p\u003e\n\u003cfigure\u003e\r\n    \u003cimg loading=\"lazy\" src=\"multiprocess_fig1.jpg\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える．\n\u003cbr\u003e\u003cbr\u003e\n\u003cspan style=\"font-size: 200%; color: red;\"\u003e\nこんなときこそマルチプロセス処理の出番でしょ↓\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\r\n    \u003cimg loading=\"lazy\" src=\"multiprocess_fig2.jpg\"/\u003e \r\n\u003c/figure\u003e\r\n\n（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）\u003c/p\u003e\n\u003cp\u003e既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e映像受信プロセス ×4（カメラの台数）\u003c/li\u003e\n\u003cli\u003e推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当）\u003c/li\u003e\n\u003cli\u003e推論結果統合プロセス ×4（カメラの台数）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eさあやるぞ！\u003c/p\u003e\n\u003ch2 id=\"実装過程あれこれ２\"\u003e実装過程あれこれ２\u003c/h2\u003e\n\u003cp\u003eここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．\u003cbr\u003e\nbase64→内部UDPが一瞬頭を過るが無視して\n\u003ca href=\"https://docs.python.org/ja/3.10/library/multiprocessing.html\" target=\"_blank\"\u003e公式ドキュメント\u003c/a\u003e\nを漁ってみます．\u003c/p\u003e\n\u003cp\u003eドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか．\nValue, Array, Queue, Pipe, RawArray, Manager, shared_memory\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．\u003c/p\u003e\n\u003cp\u003e試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに．\nフラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．\u003c/p\u003e\n\u003cp\u003eそんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります．\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20231201_multiprocessingforyolov8/after.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20231201_multiprocessingforyolov8/after.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\u003c/p\u003e","title":"複数カメラの入力に対しYOLOv8で並列推論する"},{"content":"ライブラリとは？ ライブラリとは，再利用可能なソフトウェア部品のことで，特定の機能をまとめたコードの再集合のことを指します．簡単に言えば，他の人が作ってくれた機能を自身のプログラムで使える便利なコード集のことです． Pythonには標準ライブラリと外部ライブラリの2種類があります．\n標準ライブラリ 標準ライブラリはPythonをインストールしたときに標準的に入っているライブラリのことです．通常のプログラム作成時に頻繁に必要となる機能をまとめたものが多くあります．例えば，\n乱数を作成する random 時刻を取得できる datetime, time ファイルシステムへのアクセスを実現する os, shutil 基礎的な数学関数 math HTMLサポート html URLを使ってインターネットにアクセスできる urllib スマートフォンで言うところのカメラ，アルバム，時計などのデフォルトアプリみたいなものです．これらはPythonが動く環境であれば勝手に入っているため，import ライブラリ名と記述すればすぐに利用することができます．助かる！！\n# サイコロの出目を出力 import random dice_roll = random.randint(1, 6) print(\u0026#34;サイコロの出目:\u0026#34;, dice_roll) 外部ライブラリ 外部ライブラリはPythonにはもともと組み込まれておらず，自身で導入するタイプのライブラリのことです．実装したいプログラムの機能に応じて適切に選択する必要があります．例えば，\n効率的な多次元配列の数値計算を実現する Numpy 画像処理の機能を提供する OpenCV グラフを簡単に描画できる Matplotlib 様々なアルゴリズムでの機械学習を可能とする scikit-learn Webアプリの開発に役立つ Django スマートフォンで言うところのゲームやチャットアプリなどのストアアプリみたいなものです．これらは自身でインストール作業をする必要がありますが，それさえ済ませば高機能な実装をいとも容易く実現できます！とても便利！！ 有志のプログラマが開発して公開してくれているため，感謝の気持ちを忘れずに使いましょう（Pythonもそうだけど）．\n# サイコロの出目を出力 import numpy as np dice_roll = np.random.randint(1, 7) print(\u0026#34;サイコロの出目:\u0026#34;, dice_roll) ライブラリを確認してみよう では，ここからは実際に手を動かしながら解説を進めます． まずはスタートメニューから Anaconda Prompt を開いてください．\nこれはコマンドプロンプトと呼ばれる画面で，ここにコマンドと呼ばれる命令文を入力することでコンピュータを操作できます． 試しに，ここにtreeと打ち込んでEnterキーを押してみてください．\n\u0026hellip;\u0026hellip;何かツリー構造の記述がたくさん出てきたかと思います．これはディレクトリ構造をツリー形式で表示するコマンドです（あんまり使わないけど）！ なかなか表示が終わらない場合はCtrl+Cキーでキャンセルしてください．\nではコマンドを使って，今使っているPythonで利用可能な外部ライブラリの一覧を見てみましょう！ Anaconda Promptにconda listと打ち込んでEnterキーを押してみてください．\nインストール済みライブラリとバージョンの一覧が表示されました．既にたくさん入っているようです！\nよくよく見てみると，数値計算ができるNumpyや表データを扱えるPandasなど，データサイエンスに役立つライブラリがたくさんあります． 実はAnacondaは科学計算のためのPython配布形式（ディストリビューション）の一つであり，既に役立つライブラリが揃えられているんです．\nこの状態，一見プログラムを書くのに便利な環境であるように見えるんですが実はそうでないケースがあります．\n実はライブラリは，機能を実現するために別のライブラリを参照するケースがあります．例えば，\n画像処理を実現するOpenCVには，画像を配列として扱うためにNumpyが必要 グラフを描画するMatplotlibには，画像描画のためにPillowが必要 機械学習を実現するscikit-learnには，高度な数値計算をするためにScipyが必要 またライブラリはバージョンによって提供される機能が変化することがあります．そのため，同じ実行環境に特定のバージョンを指定して参照ライブラリを導入しておく必要があるのです．\nこれを，ライブラリの依存関係と言います．この依存関係が原因で複数のライブラリを導入するときに問題が発生する可能性があるんです．\n例えば，ライブラリA-C及びB-Cの依存関係があるとき，AとBが共存するためにはCのバージョンが整合している必要があります．\nこうしたライブラリバージョンの試行錯誤を行う場合，先程表示させた複雑に依存関係が絡み合ったライブラリ群を扱うのはなかなか骨が折れます．\nまた，不要なライブラリは更に混乱を引き起こすので必要となる機能のための最小構成で開発環境を整える必要があります．\nそこで役立つのが，仮想環境です！！！（やっと本題）\n仮想環境 仮想環境とは，プログラムを動かすときに仮に作って利用する動作環境のことです． 簡単に言うと「依存関係の解決が面倒だから，まっさらな世界を一から創造すればいいじゃん！！」です．\nでは，実際に作ってみましょう．\n先ほど開いたAnaconda Promptにconda create -n test python=3.12と打ち込んで実行（Proceed([y]/n)?にはyを入力して処理を続行） conda activate testと打ち込んで実行 ここでは，testという名前の仮想環境を作成してその環境に切り替える，という作業をしてもらいました． 最新行の左側が(test)と表示されていれば成功です！\n先ほどまでは(base)と表示されていたはずです．Anacondaではデフォルトでbase環境があり，そこにたくさんのライブラリがありました． ではconda listで現在のtest環境のライブラリ構成を見てみましょう！\nbase環境と比べてライブラリの数がかなり少なくなったはずです．ここに表示されているライブラリはPython3.12に付随する必要最低限のライブラリです． このきれいな環境に最小限の適切なライブラリを導入することで，快適なコーディングライフを送ることができます！！\n番外編：Conda仮想環境の管理 複数の仮想環境を扱うときに便利なコマンドをまとめておきます！\n仮想環境の作成（環境名がtest，Python3.12の場合）\nconda create -n test python=3.12 仮想環境への移行\nconda activate test 仮想環境から離脱\nconda deactivate 仮想環境の削除\nconda remove -n test --all 仮想環境一覧の表示\nconda info -e ","permalink":"http://localhost:1313/blog/posts/20250506_condavenv/","summary":"\u003ch2 id=\"ライブラリとは\"\u003eライブラリとは？\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eライブラリ\u003c/strong\u003eとは，再利用可能なソフトウェア部品のことで，特定の機能をまとめたコードの再集合のことを指します．簡単に言えば，他の人が作ってくれた機能を自身のプログラムで使える便利なコード集のことです．\nPythonには\u003cstrong\u003e標準ライブラリ\u003c/strong\u003eと\u003cstrong\u003e外部ライブラリ\u003c/strong\u003eの2種類があります．\u003c/p\u003e\n\u003ch3 id=\"標準ライブラリ\"\u003e標準ライブラリ\u003c/h3\u003e\n\u003cp\u003e標準ライブラリはPythonをインストールしたときに標準的に入っているライブラリのことです．通常のプログラム作成時に頻繁に必要となる機能をまとめたものが多くあります．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e乱数を作成する random\u003c/li\u003e\n\u003cli\u003e時刻を取得できる datetime, time\u003c/li\u003e\n\u003cli\u003eファイルシステムへのアクセスを実現する os, shutil\u003c/li\u003e\n\u003cli\u003e基礎的な数学関数 math\u003c/li\u003e\n\u003cli\u003eHTMLサポート html\u003c/li\u003e\n\u003cli\u003eURLを使ってインターネットにアクセスできる urllib\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスマートフォンで言うところのカメラ，アルバム，時計などのデフォルトアプリみたいなものです．これらはPythonが動く環境であれば勝手に入っているため，\u003ccode\u003eimport ライブラリ名\u003c/code\u003eと記述すればすぐに利用することができます．助かる！！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# サイコロの出目を出力\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e random\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edice_roll \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e random\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandint(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;サイコロの出目:\u0026#34;\u003c/span\u003e, dice_roll)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"外部ライブラリ\"\u003e外部ライブラリ\u003c/h3\u003e\n\u003cp\u003e外部ライブラリはPythonにはもともと組み込まれておらず，自身で導入するタイプのライブラリのことです．実装したいプログラムの機能に応じて適切に選択する必要があります．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e効率的な多次元配列の数値計算を実現する Numpy\u003c/li\u003e\n\u003cli\u003e画像処理の機能を提供する OpenCV\u003c/li\u003e\n\u003cli\u003eグラフを簡単に描画できる Matplotlib\u003c/li\u003e\n\u003cli\u003e様々なアルゴリズムでの機械学習を可能とする scikit-learn\u003c/li\u003e\n\u003cli\u003eWebアプリの開発に役立つ Django\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスマートフォンで言うところのゲームやチャットアプリなどのストアアプリみたいなものです．これらは自身でインストール作業をする必要がありますが，それさえ済ませば高機能な実装をいとも容易く実現できます！とても便利！！\n有志のプログラマが開発して公開してくれているため，感謝の気持ちを忘れずに使いましょう（Pythonもそうだけど）．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# サイコロの出目を出力\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e np\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edice_roll \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandom\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandint(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;サイコロの出目:\u0026#34;\u003c/span\u003e, dice_roll)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"ライブラリを確認してみよう\"\u003eライブラリを確認してみよう\u003c/h2\u003e\n\u003cp\u003eでは，ここからは実際に手を動かしながら解説を進めます．\nまずはスタートメニューから \u003cstrong\u003eAnaconda Prompt\u003c/strong\u003e を開いてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"img_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"console_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこれは\u003cstrong\u003eコマンドプロンプト\u003c/strong\u003eと呼ばれる画面で，ここに\u003cstrong\u003eコマンド\u003c/strong\u003eと呼ばれる命令文を入力することでコンピュータを操作できます．\n試しに，ここに\u003ccode\u003etree\u003c/code\u003eと打ち込んでEnterキーを押してみてください．\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;何かツリー構造の記述がたくさん出てきたかと思います．これはディレクトリ構造をツリー形式で表示するコマンドです（あんまり使わないけど）！\nなかなか表示が終わらない場合はCtrl+Cキーでキャンセルしてください．\u003c/p\u003e\n\u003cp\u003eではコマンドを使って，今使っているPythonで利用可能な外部ライブラリの一覧を見てみましょう！\nAnaconda Promptに\u003ccode\u003econda list\u003c/code\u003eと打ち込んでEnterキーを押してみてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"modules.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eインストール済みライブラリとバージョンの一覧が表示されました．既にたくさん入っているようです！\u003c/p\u003e\n\u003cp\u003eよくよく見てみると，数値計算ができるNumpyや表データを扱えるPandasなど，データサイエンスに役立つライブラリがたくさんあります．\n実はAnacondaは科学計算のためのPython配布形式（ディストリビューション）の一つであり，既に役立つライブラリが揃えられているんです．\u003c/p\u003e\n\u003cp\u003eこの状態，一見プログラムを書くのに便利な環境であるように見えるんですが実はそうでないケースがあります．\u003c/p\u003e","title":"【入門1】Python仮想環境"},{"content":"JupyterLab+Visual Studio Codeを使ったPython実行 Visual Studio CodeにJupyter拡張機能をインストール 空のファイルを作成し，拡張子を.ipynbとする ＋コードボタンからコードブロックを作成し，Pythonプログラムを記述 右上カーネルの選択から実行したいPython環境を選択し コードブロック左側の実行ボタン▷を押下 ポップアップが出てきたらインストールをクリックしてipykernelをインストール （pip install ipykernelを当該環境で実行してインストールも出来ます） YOLOのセットアップ 今回は最新のYOLOを簡単に扱うことができるUltralyticsパッケージを利用します．\nconda create --name yolo-env python=3.11を実行して仮想環境を作成 conda activate yolo-envで仮想環境を有効化 pip install ultralyticsでUltralyticsパッケージをインストール ここまで済んだら，テストコードを動かしてみましょう！\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolo11n.pt\u0026#34;) results = model(\u0026#34;https://ultralytics.com/images/bus.jpg\u0026#34;) results[0].show() バスを背景にした画像の物体検出が表示されたはずです！\n作業ディレクトリを開くと，yolo11n.ptというファイルがダウンロードされています．これが，YOLOのモデルファイルです．これはYOLOのデフォルトの訓練済みモデルで，COCO Datasetに収録された80種類の物体を識別することが出来ます．\n参考 ","permalink":"http://localhost:1313/blog/posts/20250506_yolojupyter/","summary":"\u003ch2 id=\"jupyterlabvisual-studio-codeを使ったpython実行\"\u003eJupyterLab+Visual Studio Codeを使ったPython実行\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eVisual Studio CodeにJupyter拡張機能をインストール\u003c/li\u003e\n\u003cli\u003e空のファイルを作成し，拡張子を\u003ccode\u003e.ipynb\u003c/code\u003eとする\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e＋コード\u003c/code\u003eボタンからコードブロックを作成し，Pythonプログラムを記述\u003c/li\u003e\n\u003cli\u003e右上\u003ccode\u003eカーネルの選択\u003c/code\u003eから実行したいPython環境を選択し\u003c/li\u003e\n\u003cli\u003eコードブロック左側の実行ボタン\u003ccode\u003e▷\u003c/code\u003eを押下\u003c/li\u003e\n\u003cli\u003eポップアップが出てきたら\u003ccode\u003eインストール\u003c/code\u003eをクリックして\u003ccode\u003eipykernel\u003c/code\u003eをインストール\n（\u003ccode\u003epip install ipykernel\u003c/code\u003eを当該環境で実行してインストールも出来ます）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"vsjupyter.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"yoloのセットアップ\"\u003eYOLOのセットアップ\u003c/h2\u003e\n\u003cp\u003e今回は最新のYOLOを簡単に扱うことができるUltralyticsパッケージを利用します．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003econda create --name yolo-env python=3.11\u003c/code\u003eを実行して仮想環境を作成\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate yolo-env\u003c/code\u003eで仮想環境を有効化\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install ultralytics\u003c/code\u003eでUltralyticsパッケージをインストール\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eここまで済んだら，テストコードを動かしてみましょう！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e ultralytics \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e YOLO\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emodel \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e YOLO(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;yolo11n.pt\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresults \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://ultralytics.com/images/bus.jpg\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresults[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eバスを背景にした画像の物体検出が表示されたはずです！\u003c/p\u003e\n\u003cp\u003e作業ディレクトリを開くと，\u003ccode\u003eyolo11n.pt\u003c/code\u003eというファイルがダウンロードされています．これが，YOLOのモデルファイルです．これはYOLOのデフォルトの訓練済みモデルで，COCO Datasetに収録された80種類の物体を識別することが出来ます．\u003c/p\u003e\n\u003ch2 id=\"参考\"\u003e参考\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://docs.ultralytics.com/ja/quickstart/#understanding-settings\"\u003e\u003c/a\u003e\u003c/p\u003e","title":"【入門2】YOLO，Jupyterのセットアップ"},{"content":"アノテーションとは？ 機械学習の分類のひとつである教師あり学習では，まず入力データと出力データの組を複数用意し，それらを使って機械学習モデルを訓練します．\n入力データと出力データの組は，タスクによって様々です．例えば，\n入力：画像データ，出力：それぞれが犬or猫 入力：家の情報（間取り，立地，築年数など），出力：家の価格 入力：これまでの気温，出力：明日の気温 入力：画像データ，出力：各物体の座標 どんなタスクを前提とする場合でも，これら入力データと出力データの組を作る作業が必要となります．これがアノテーションです．\n環境構築 今回はYOLOによる画像物体認識を目標とします．そこで，アノテーションで作るべきデータは以下になります．\n入力：画像データ，出力：各物体の座標 頑張れば用意した画像の座標を数えてアノテーションできそうですが，今回はより簡単に作業を進めるためにLabelmeというツールを使います．GUIが整備されたソフトウェアですが，Pythonライブラリとして無償で提供されています！（ありがたい！！）\nAnaconda Promptを開く conda create -n annot_env python=3.12で仮想環境を作成 conda activate annot_envで仮想環境を有効化 pip install --upgrade labelmeと入力しLabelmeをインストール これでインストールは完了です．そのままannot_env環境内でlabelmeコマンドを実行してみてください！\nこの画面が表示されたら成功です！\nいざ，アノテーション 画像読み込み 今回は画像から麻雀牌を物体認識するタスクに挑戦してみます！サンプル画像は以下からダウンロード，解凍してください．\nサンプル画像 Zipファイルの解凍が済んだらLabelmeの画面左側にあるOpen Dirから，画像のディレクトリを開いてください．\nこれでアノテーションの準備は完了です！\n範囲選択，ラベル付け アノテーションの流れは以下になります．\n画像を右クリック，Create Rectangleをクリック 物体の範囲を四角形で囲んで選択，間違えたらEscキーで戻る 出てきたポップアップの上部Enter object labelにラベルを入力してOKをクリック 2と3を物体の数だけひたすら繰り返す 画像内のアノテーションが終わったら画面左部Saveから画像と同名のJSONファイルを保存 ちょっとやってみますね～\nはい，できました．ラベルは牌の種類（萬子筒子索子字牌）の略称です．\n\u0026hellip;\u0026hellip;\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\nめんどくさい\n機械学習では一般的にデータの数が大きいほど性能が上がりやすいとされています．つまり，（要件の要求レベルにもよりますが）精度を上げるためにはたくさんアノテーションする必要があるんです．\nそこで，特定のタスクの機械学習を考える場合はアノテーションに取り掛かる前に，データセットが利用可能なライセンスで公開されていないかどうか調べるところからスタートします．例えばYOLOの場合，Roboflow，VisualData，Google Open Images Datasetなどが画像データセットを公開している他，タスクに応じて論文と併せて公開されているデータセットも多数あります．\nまた今回はアノテーション入門ということで手作業での方法を紹介していますが，画像認識AIによる半自動ツールもあるので興味があれば調べてみてください．\n今回は，なんと，特別に，アノテーション済みファイルを用意してあります！\nアノテーション結果（JSON） ZIPファイルをダウンロード＆解凍したら，datasetsという名前のディレクトリを作成し，先程の画像ファイルとJSONファイルを以下の構造で配置してください．（image_1.jsonは自作のファイルで構いません！）\n.\r└── datasets/\r├── image_1.jpg\r├── image_1.json\r├── image_2.jpg\r├── image_2.json\r├── 〜省略〜\r├── image_10.jpg\r└── image_10.json データセットフォーマットの変換 では，学習させるデータの中身を一度見てみましょう！image_1.jsonを開いてみます．\nJSONはデータ記述形式のひとつで，キーと値を並べて書くのが特徴です．Pythonの辞書型そっくりですね． ラベルや座標がファイルの中に複数記述されていることがわかります．\n少しスクロールすると，よくわからない文字列が大量に記述されています．実はこれ，Base64と呼ばれるエンコード形式で画像そのものが書きつけられているんです．これにより，このJSONファイル単体で入力データを表現することが出来ます．\nさて，ひととおり学習データの中身を見てもらいましたが，実はこのままではYOLOの学習が出来ないんです．\nYOLOに学習させるためには，画像のアノテーションを以下の形式で記述する必要があります．\n画像ファイル名.txt\nlabel_id X Y Width Height 例えば，image_1.jsonをこの形式で記述すると以下のようになります．\nimage_1.txt\n4 0.171569 0.375817 0.142157 0.183007\r6 0.286765 0.382353 0.124183 0.183007\r0 0.401144 0.398693 0.127451 0.185185\r5 0.517157 0.408497 0.117647 0.178649\r10 0.638889 0.418301 0.125817 0.189542\r7 0.767157 0.436819 0.127451 0.174292 YOLOのデータセットでは，ラベルの番号，X座標，Y座標，幅，高さを列挙して記述します．\nこれを手作業で変換するのはかなり面倒です．\nそこで今回は，Labelmeで保存したJSONファイルをYOLOのデータセット形式に変換するツール，labelme2yoloを使います．condaのannot_env環境下で以下の手順で変換します．\npip install labelme2yoloでlabelme2yoloをインストール datasetディレクトリの一つ上の階層で以下のコマンドを実行 labelme2yolo --json_dir ./datasets/ --val_size 0.10 --output_format \u0026quot;bbox\u0026quot; これが済んだら，datasetsディレクトリの中にYOLODatasetが生成されているはずです．構造は以下のようになっています．\n.\r└── YOLODatasets/\r├── images/\r│ ├── train/\r│ │ ├── image_1.jpg\r│ │ └── ～省略～\r│ └── val/\r│ └── image_7.jpg\r├── labels/\r│ ├── train/\r│ │ ├── image_1.txt\r│ │ └── ～省略～\r│ └── val/\r│ └── image_7.txt\r└── dataset.yaml 先ほどのデータセット構造より複雑になっていますね．画像とラベルが別のフォルダに格納され，更にその中で訓練用(train)と評価用(val)に分かれています．\nlabelsディレクトリのテキストファイルを見てみましょう．YOLOのフォーマットに変換され，ラベルIDや座標が記録されているはずです．\ndataset.yamlの中身を見てみましょう．\nデータセットまでの絶対パス，訓練評価用ディレクトリのパス，ラベルIDとラベル名の対応が記載されています．YOLOの学習時にはこのdataset.yamlを利用してデータセットを読み込みます．\nYOLOの学習 それでは，いよいよ作ったデータセットを使ってYOLOの学習をしてみましょう！！\nまず，テスト用の画像を用意します．以下のリンクから画像ファイルimage_11.jpgをダウンロードしてdataset.yamlと同階層に配置します．\nテスト画像image_11.jpg dataset.yamlと同階層に.ipynbファイルを作成して，以下のPythonプログラムを実行してみましょう．\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolo11n.pt\u0026#34;) model.train(data=\u0026#39;dataset.yaml\u0026#39;, epochs=300, verbose=False) results = model(\u0026#34;image_11.jpg\u0026#34;) results[0].show() CPUのみの実行環境ではかなり時間がかかるので気長に待ちましょう～\n学習が終了すると，image_11.jpgに対する推論結果が表示されます．\n未検出や誤検出がちらほらありますが，部分的に検出が成功していることがわかります．データセットサイズやエポック数を変えることで性能の改善が期待できます．\nまた，データセットディレクトリのruns内を見ると，weightディレクトリにbest.ptが保存されていることがわかります． これは今回学習したモデルファイルで，実際に物体検出をシステム開発に組み込むときに使用します．\n","permalink":"http://localhost:1313/blog/posts/20250506_yoloannotation/","summary":"\u003ch2 id=\"アノテーションとは\"\u003eアノテーションとは？\u003c/h2\u003e\n\u003cp\u003e機械学習の分類のひとつである\u003cstrong\u003e教師あり学習\u003c/strong\u003eでは，まず入力データと出力データの組を複数用意し，それらを使って機械学習モデルを訓練します．\u003c/p\u003e\n\u003cp\u003e入力データと出力データの組は，タスクによって様々です．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e入力：画像データ，出力：それぞれが犬or猫\u003c/li\u003e\n\u003cli\u003e入力：家の情報（間取り，立地，築年数など），出力：家の価格\u003c/li\u003e\n\u003cli\u003e入力：これまでの気温，出力：明日の気温\u003c/li\u003e\n\u003cli\u003e入力：画像データ，出力：各物体の座標\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eどんなタスクを前提とする場合でも，これら\u003cstrong\u003e入力データと出力データの組を作る作業\u003c/strong\u003eが必要となります．これが\u003cstrong\u003eアノテーション\u003c/strong\u003eです．\u003c/p\u003e\n\u003ch2 id=\"環境構築\"\u003e環境構築\u003c/h2\u003e\n\u003cp\u003e今回はYOLOによる画像物体認識を目標とします．そこで，アノテーションで作るべきデータは以下になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e入力：画像データ，出力：各物体の座標\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e頑張れば用意した画像の座標を数えてアノテーションできそうですが，今回はより簡単に作業を進めるために\u003cstrong\u003eLabelme\u003c/strong\u003eというツールを使います．GUIが整備されたソフトウェアですが，Pythonライブラリとして無償で提供されています！（ありがたい！！）\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAnaconda Promptを開く\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda create -n annot_env python=3.12\u003c/code\u003eで仮想環境を作成\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate annot_env\u003c/code\u003eで仮想環境を有効化\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install --upgrade labelme\u003c/code\u003eと入力しLabelmeをインストール\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eこれでインストールは完了です．そのまま\u003ccode\u003eannot_env\u003c/code\u003e環境内で\u003ccode\u003elabelme\u003c/code\u003eコマンドを実行してみてください！\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"labelme.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこの画面が表示されたら成功です！\u003c/p\u003e\n\u003ch2 id=\"いざアノテーション\"\u003eいざ，アノテーション\u003c/h2\u003e\n\u003ch3 id=\"画像読み込み\"\u003e画像読み込み\u003c/h3\u003e\n\u003cp\u003e今回は画像から麻雀牌を物体認識するタスクに挑戦してみます！サンプル画像は以下からダウンロード，解凍してください．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hakoshi-normal/yolo_sample/raw/refs/heads/main/images/images.zip\"\u003eサンプル画像\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eZipファイルの解凍が済んだらLabelmeの画面左側にある\u003ccode\u003eOpen Dir\u003c/code\u003eから，画像のディレクトリを開いてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"labelme_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこれでアノテーションの準備は完了です！\u003c/p\u003e\n\u003ch3 id=\"範囲選択ラベル付け\"\u003e範囲選択，ラベル付け\u003c/h3\u003e\n\u003cp\u003eアノテーションの流れは以下になります．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e画像を右クリック，\u003ccode\u003eCreate Rectangle\u003c/code\u003eをクリック\u003c/li\u003e\n\u003cli\u003e物体の範囲を四角形で囲んで選択，間違えたら\u003ccode\u003eEsc\u003c/code\u003eキーで戻る\u003c/li\u003e\n\u003cli\u003e出てきたポップアップの上部\u003ccode\u003eEnter object label\u003c/code\u003eにラベルを入力して\u003ccode\u003eOK\u003c/code\u003eをクリック\u003c/li\u003e\n\u003cli\u003e2と3を物体の数だけひたすら繰り返す\u003c/li\u003e\n\u003cli\u003e画像内のアノテーションが終わったら画面左部\u003ccode\u003eSave\u003c/code\u003eから画像と同名のJSONファイルを保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eちょっとやってみますね～\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"annotated.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eはい，できました．ラベルは牌の種類（萬子筒子索子字牌）の略称です．\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 150%; color: red;\"\u003eめんどくさい\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e機械学習では一般的にデータの数が大きいほど性能が上がりやすいとされています．つまり，（要件の要求レベルにもよりますが）精度を上げるためにはたくさんアノテーションする必要があるんです．\u003c/p\u003e\n\u003cp\u003eそこで，特定のタスクの機械学習を考える場合はアノテーションに取り掛かる前に，データセットが利用可能なライセンスで公開されていないかどうか調べるところからスタートします．例えばYOLOの場合，\u003ca href=\"https://universe.roboflow.com/\"\u003eRoboflow\u003c/a\u003e，\u003ca href=\"https://visualdata.io/discovery\"\u003eVisualData\u003c/a\u003e，\u003ca href=\"https://storage.googleapis.com/openimages/web/index.html\"\u003eGoogle Open Images Dataset\u003c/a\u003eなどが画像データセットを公開している他，タスクに応じて論文と併せて公開されているデータセットも多数あります．\u003c/p\u003e\n\u003cp\u003eまた今回はアノテーション入門ということで手作業での方法を紹介していますが，画像認識AIによる半自動ツールもあるので興味があれば調べてみてください．\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e今回は，なんと，特別に，アノテーション済みファイルを用意してあります！\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hakoshi-normal/yolo_sample/raw/refs/heads/main/images/labels.zip\"\u003eアノテーション結果（JSON）\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eZIPファイルをダウンロード＆解凍したら，\u003ccode\u003edatasets\u003c/code\u003eという名前のディレクトリを作成し，先程の画像ファイルとJSONファイルを以下の構造で配置してください．（\u003ccode\u003eimage_1.json\u003c/code\u003eは自作のファイルで構いません！）\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-planetext\" data-lang=\"planetext\"\u003e.\r\n└── datasets/\r\n    ├── image_1.jpg\r\n    ├── image_1.json\r\n    ├── image_2.jpg\r\n    ├── image_2.json\r\n    ├── 〜省略〜\r\n    ├── image_10.jpg\r\n    └── image_10.json\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"データセットフォーマットの変換\"\u003eデータセットフォーマットの変換\u003c/h3\u003e\n\u003cp\u003eでは，学習させるデータの中身を一度見てみましょう！\u003ccode\u003eimage_1.json\u003c/code\u003eを開いてみます．\u003c/p\u003e","title":"【入門3】YOLOアノテーションと学習"},{"content":"はじめに 自動採譜プロジェクトの息抜き的な記事です．\nせっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして， ところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．\nカメラ切り替えトリガー カメラの切り替えトリガーを「牌山にプレーヤーが触れたタイミング」と設定しました．\n自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．\nツモ位置の検出 まず，天カメの映像から卓上のツモ位置を特定する必要があります．\n牌山の形を見てみましょう．\n対局開始前はこんな感じ．\n対局開始直後はこんな感じ．\n対局中はこんな感じ．\n対局開始前は4つ牌山がありますが，対局開始以降は牌山が必ず3つ以下になります王牌の扱い上，4つのときもあります．\n牌山が3つのとき\n牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．\n牌山が4つのとき\n一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．\n実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．\n牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．\n実装概要 元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます． cv2.inRange()を使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．\n二値化した画像に対し，cv2.findContours()を使用し，牌山となりうる矩形を検出します．\n手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます． その後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．\nツモプレイヤーの検出 ツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します． MediaPipe のハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します． 今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．\nプレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．\nオートスイッチャー実装！！！ OBSを使用して配信するため，カメラ切り替えには obs-websocket-py を使用しました．こちらの サンプル がそのまま動作して助かりました．\nGUI実装にはHTML/JSでの記述が可能な Eel を使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．\n実際の動作の様子がこちら．\nYour browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．\nあと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．\n","permalink":"http://localhost:1313/blog/posts/20240210_mahjongautocamswitcher/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e自動採譜プロジェクト\u003c/a\u003eの息抜き的な記事です．\u003c/p\u003e\n\u003cp\u003eせっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして，\nところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．\u003c/p\u003e\n\u003ch2 id=\"カメラ切り替えトリガー\"\u003eカメラ切り替えトリガー\u003c/h2\u003e\n\u003cp\u003eカメラの切り替えトリガーを「\u003cstrong\u003e牌山にプレーヤーが触れたタイミング\u003c/strong\u003e」と設定しました．\u003c/p\u003e\n\u003cp\u003e自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．\u003c/p\u003e\n\u003ch2 id=\"ツモ位置の検出\"\u003eツモ位置の検出\u003c/h2\u003e\n\u003cp\u003eまず，天カメの映像から卓上のツモ位置を特定する必要があります．\u003cbr\u003e牌山の形を見てみましょう．\u003c/p\u003e\n\u003cp\u003e対局開始前はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_0.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局開始直後はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局中はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_2.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局開始前は4つ牌山がありますが，対局開始以降は牌山が\u003cdel\u003e必ず3つ以下になります\u003c/del\u003e王牌の扱い上，4つのときもあります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e牌山が3つのとき\u003c/p\u003e\n\u003cp\u003e牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e牌山が4つのとき\u003c/p\u003e\n\u003cp\u003e一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"fig1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．\u003c/p\u003e\n\u003cp\u003e牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．\u003c/p\u003e\n\u003ch3 id=\"実装概要\"\u003e実装概要\u003c/h3\u003e\n\u003cp\u003e元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます．\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"original_image.png\"/\u003e \r\n\u003c/figure\u003e\r\n\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecv2.inRange()\u003c/code\u003eを使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"mask_hai.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e二値化した画像に対し，\u003ccode\u003ecv2.findContours()\u003c/code\u003eを使用し，牌山となりうる矩形を検出します．\u003c/p\u003e\n\u003cp\u003e手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます．\nその後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumohai.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"ツモプレイヤーの検出\"\u003eツモプレイヤーの検出\u003c/h2\u003e\n\u003cp\u003eツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します．\n\u003ca href=\"https://developers.google.com/mediapipe/solutions/vision/hand_landmarker\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eMediaPipe\u003c/a\u003e\nのハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します．\n今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumo1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eプレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumo2.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"オートスイッチャー実装\"\u003eオートスイッチャー実装！！！\u003c/h2\u003e\n\u003cp\u003eOBSを使用して配信するため，カメラ切り替えには\n\u003ca href=\"https://github.com/Elektordi/obs-websocket-py/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eobs-websocket-py\u003c/a\u003e\nを使用しました．こちらの\n\u003ca href=\"https://github.com/Elektordi/obs-websocket-py/blob/master/samples/switch_scenes.py\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eサンプル\u003c/a\u003e\nがそのまま動作して助かりました．\u003c/p\u003e\n\u003cp\u003eGUI実装にはHTML/JSでの記述が可能な\n\u003ca href=\"https://github.com/python-eel/Eel\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eEel\u003c/a\u003e\nを使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．\u003c/p\u003e\n\u003cp\u003e実際の動作の様子がこちら．\u003c/p\u003e\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\n\u003cp\u003e現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．\u003c/p\u003e\n\u003cp\u003eあと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．\u003c/p\u003e","title":"麻雀放送対局用のオートスイッチャーを作りたい"},{"content":"はじめに 複数カメラの入力に対しYOLOv8で並列推論するのソースコード解説です． 解説のため，一部表現を変えている部分があります．\n大まかな流れは以下の通り．\n共有メモリ確保 フレーム入力プロセス カメラからフレームを取得 フレームを共有メモリ1に保存 推論プロセス 共有フレームを呼び出し 推論 推論結果を描画したフレームを共有メモリ2に保存 フレーム表示プロセス 共有フレームを呼び出し フレームの画面描画 プロセス間の遷移はmultiprocessingモジュールのEvent関数をフラグとして利用します． カメラの台数が増えた場合はプロセス数を増やせば対応できます．\n事前準備 設定情報 cam_N = 4 # カメラ台数 proccess_N = 3 # カメラ1台あたりのプロセス数 frame_info = {\u0026#34;nbytes\u0026#34; : frame.nbytes, \u0026#34;shape\u0026#34; : frame.shape, \u0026#34;dtype\u0026#34; : frame.dtype} フレームのサイズや型はメモリ確保，呼び出し時に必要になります．\n共有メモリの確保 shm = shared_memory.SharedMemory(create=True, size=frame.nbytes, name = mem_name) 共有メモリの確保はこの一文でできます．めっちゃ便利ですね． ただし，mem_nameを動的に作成して複数の共有メモリを確保する場合，注意事項があります． shared_memory.SharedMemoryの戻り値を格納する変数（上記コードではshm）を設定しなかった場合や上書きした場合，それ以前の共有メモリ領域は参照できなくなります．その場合はshmを格納するリストを用意しておくとアクセス可能になります．\nプロセスの立ち上げ p = Process(target=関数名, args=(引数,)) p.start() # プロセス開始 p.join() # プロセス終了 複数プロセスを立ち上げる場合は p をリストに入れておくと管理が楽です．\nフレーム入力プロセス def rec_cam(cam_id, pre_flags, frame_info): cap = cv2.VideoCapture(cam_id) while True: _, frame = cap.read() idx = (cam_id)*proccess_N+counter%proccess_N mem_name = f\u0026#39;shared{idx+1:02}\u0026#39; shm = shared_memory.SharedMemory(name=mem_name) frame_sh = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) frame_sh[:] = frame[:] # 推論開始 pre_flags[idx].set() cap.close() multiprocessingのSharedMemoryでは共有メモリを名前をつけて呼び出すことができます．便利！ ここでは，\u0026ldquo;shared1[番号]\u0026ldquo;という名前で共有メモリを確保している（後述）．カメラ台数✕カメラあたりのプロセス数分の共有メモリ領域が必要になるため， あるカメラのあるフレームに対する保存領域は，カメラ番号✕カメラ1台あたりのプロセス数＋フレーム番号%カメラ1台あたりのプロセス数で求めることができる．と同時にこの番号は推論プロセスを動作させるフラグ管理のインデックスとしても使う．\n推論プロセス def predict_frame(pre_flag, view_flag, mem_name, cam_id, frame_info): while True: if pre_flag.is_set(): # 推論開始 pre_flag.clear() shm = shared_memory.SharedMemory(name=mem_name) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # 重い推論処理がここに入る（frameに結果を反映する） # 結果統合用共有メモリにのせる shm_result = shared_memory.SharedMemory(name=f\u0026#39;shared2{cam_id}\u0026#39;) frame_new = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm_result.buf) frame_new[:]=frame[:] # 結果表示 view_flag.set() カメラ台数✕カメラあたりのプロセス数分の推論プロセスを立ち上げます． 推論開始用のフラグや共有メモリの呼び出し名は予め与えた状態でプロセスを立ち上げる．フレーム入力プロセスでの推論開始用フラグの有効化を待ち受けています． 処理済みのフレーを格納する共有メモリ領域名は\u0026rsquo;shared2[番号]\u0026lsquo;としており，カメラの台数分事前に確保してあります． 検出結果を描画する必要がない場合は，フレーム表示プロセスを介する必要がないため，別プロセスに推論結果を渡したり，UDP通信などで端末間の結果送信が実装できると思います．\nフレーム表示プロセス def view_frame(view_flag, cam_id, frame_info): while True: if view_flag.is_set(): view_flag.clear() shm = shared_memory.SharedMemory(name=f\u0026#39;shared2{cam_id}\u0026#39;) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) cv2.imshow(f\u0026#39;window_{cam_id}\u0026#39;, frame) cv2.waitKey(1) 推論プロセスでのフレーム表示用フラグの有効化を待ち受けています．\nおわりに 今回は，以前実装した並列画像処理プログラムの解説でした．コードブロックを断片的に示しただけなので，具体的な実装コードを知りたい方はソースコードから参照できます．\n","permalink":"http://localhost:1313/blog/posts/20231226_multiprocessingforrealtimevideo/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/2023_12_01_multiprocessingforyolov8/\" target=\"_blank\"\u003e複数カメラの入力に対しYOLOv8で並列推論する\u003c/a\u003eのソースコード解説です．\n解説のため，一部表現を変えている部分があります．\u003c/p\u003e\n\u003cp\u003e大まかな流れは以下の通り．\u003c/p\u003e\n\u003col start=\"0\"\u003e\n\u003cli\u003e共有メモリ確保\u003c/li\u003e\n\u003cli\u003eフレーム入力プロセス\n\u003col\u003e\n\u003cli\u003eカメラからフレームを取得\u003c/li\u003e\n\u003cli\u003eフレームを共有メモリ1に保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e推論プロセス\n\u003col\u003e\n\u003cli\u003e共有フレームを呼び出し\u003c/li\u003e\n\u003cli\u003e推論\u003c/li\u003e\n\u003cli\u003e推論結果を描画したフレームを共有メモリ2に保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eフレーム表示プロセス\n\u003col\u003e\n\u003cli\u003e共有フレームを呼び出し\u003c/li\u003e\n\u003cli\u003eフレームの画面描画\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eプロセス間の遷移はmultiprocessingモジュールのEvent関数をフラグとして利用します．\nカメラの台数が増えた場合はプロセス数を増やせば対応できます．\u003c/p\u003e\n\u003ch2 id=\"事前準備\"\u003e事前準備\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e設定情報\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecam_N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# カメラ台数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eproccess_N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# カメラ1台あたりのプロセス数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eframe_info \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;nbytes\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enbytes,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;shape\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshape,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dtype\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edtype}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eフレームのサイズや型はメモリ確保，呼び出し時に必要になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e共有メモリの確保\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eshm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e shared_memory\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSharedMemory(create\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enbytes, name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e mem_name)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e共有メモリの確保はこの一文でできます．めっちゃ便利ですね．\nただし，mem_nameを動的に作成して複数の共有メモリを確保する場合，注意事項があります．\nshared_memory.SharedMemoryの戻り値を格納する変数（上記コードではshm）を設定しなかった場合や上書きした場合，それ以前の共有メモリ領域は参照できなくなります．その場合はshmを格納するリストを用意しておくとアクセス可能になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプロセスの立ち上げ\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Process(target\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e関数名, args\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e(引数,))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estart() \u003cspan style=\"color:#75715e\"\u003e# プロセス開始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ejoin() \u003cspan style=\"color:#75715e\"\u003e# プロセス終了\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e複数プロセスを立ち上げる場合は p をリストに入れておくと管理が楽です．\u003c/p\u003e\n\u003ch2 id=\"フレーム入力プロセス\"\u003eフレーム入力プロセス\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erec_cam\u003c/span\u003e(cam_id, pre_flags, frame_info):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cv2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eVideoCapture(cam_id)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            _, frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            idx \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (cam_id)\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003eproccess_N\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003ecounter\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003eproccess_N\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            mem_name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;shared\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003eidx\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e02\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            shm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e shared_memory\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSharedMemory(name\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003emem_name)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            frame_sh \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003endarray(shape\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;shape\u0026#34;\u003c/span\u003e], dtype\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dtype\u0026#34;\u003c/span\u003e], buffer\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eshm\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebuf)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            frame_sh[:] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e frame[:]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# 推論開始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            pre_flags[idx]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eset()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003emultiprocessingのSharedMemoryでは共有メモリを名前をつけて呼び出すことができます．便利！\nここでは，\u0026ldquo;shared1[番号]\u0026ldquo;という名前で共有メモリを確保している（後述）．カメラ台数✕カメラあたりのプロセス数分の共有メモリ領域が必要になるため，\nあるカメラのあるフレームに対する保存領域は，カメラ番号✕カメラ1台あたりのプロセス数＋フレーム番号%カメラ1台あたりのプロセス数で求めることができる．と同時にこの番号は推論プロセスを動作させるフラグ管理のインデックスとしても使う．\u003c/p\u003e","title":"リアルタイムの映像入力に対してFPSを落とさずに画像処理したい"},{"content":"ビデオカメラからOpenCVで映像取得（低品質） 麻雀自動採譜の実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．\nビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できるビデオキャプチャカードを導入． これでビデオカメラをWebカメラみたく使えるぞ！やった！！\nと思った矢先，\nなんか遅い気がする！！\nよくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，1秒遅延＋低FPS＋低画質という散々な結果に\nお前 Full HD で 30FPS 出るって言ってたじゃないか\u0026hellip;\u0026hellip;\nキャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．\nRTSP通信でカメラの映像を受信してみる\nこのときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった． 今回もPyAVを使えば解決するかもしれない．\nビデオカメラからPyAVで映像取得 PyAVでPCに有線接続されたカメラから映像を取得する．\nimport cv2 import av con_options = dict( video_size=\u0026#39;1920x1080\u0026#39;, vcodec=\u0026#39;mjpeg\u0026#39;, framerate=\u0026#39;30\u0026#39;, rtbufsize=\u0026#39;1\u0026#39;, ) device_name = \u0026#34;USB Video\u0026#34; con_def = dict( format=\u0026#39;dshow\u0026#39;, file=f\u0026#39;video={device_name}\u0026#39;, options=dict(con_options, video_device_number=\u0026#39;0\u0026#39;) ) container = av.open(**con_def) for frame in container.decode(video=0): frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) # cv2.imshow(\u0026#34;test_window\u0026#34;, frame) # cv2.waitKey(1) ちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．\nffmpeg -list_devices true -f dshow -i dummy PyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．\nこれで遅延ほぼなし，30FPSでの映像受信ができた．\n","permalink":"http://localhost:1313/blog/posts/20231225_getframebypyav/","summary":"\u003ch2 id=\"ビデオカメラからopencvで映像取得低品質\"\u003eビデオカメラからOpenCVで映像取得（低品質）\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003eの実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．\u003c/p\u003e\n\u003cp\u003eビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できる\u003ca href=\"https://amzn.asia/d/jclJASJ\"\u003eビデオキャプチャカード\u003c/a\u003eを導入．\nこれでビデオカメラをWebカメラみたく使えるぞ！やった！！\u003c/p\u003e\n\u003cp\u003eと思った矢先，\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eなんか遅い気がする！！\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eよくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，\u003cstrong\u003e1秒遅延＋低FPS＋低画質\u003c/strong\u003eという散々な結果に\u003c/p\u003e\n\u003cp\u003eお前 Full HD で 30FPS 出るって言ってたじゃないか\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003eキャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/blog/2023_12_01_getframertsp/\"\u003eRTSP通信でカメラの映像を受信してみる\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこのときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった．\n今回も\u003ca href=\"https://pyav.org/docs/develop/index.html\" target=\"_blank\"\u003ePyAV\u003c/a\u003eを使えば解決するかもしれない．\u003c/p\u003e\n\u003ch2 id=\"ビデオカメラからpyavで映像取得\"\u003eビデオカメラからPyAVで映像取得\u003c/h2\u003e\n\u003cp\u003ePyAVでPCに有線接続されたカメラから映像を取得する．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e cv2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e av\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econ_options  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  dict(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    video_size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1920x1080\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    vcodec\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;mjpeg\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    framerate\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;30\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    rtbufsize\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edevice_name  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;USB Video\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econ_def  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  dict(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;dshow\u0026#39;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    file\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;video=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003edevice_name\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    options\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003edict(con_options, video_device_number\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;0\u0026#39;\u003c/span\u003e) )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econtainer \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e av\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eopen(\u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003econ_def)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e frame \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e container\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edecode(video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto_ndarray(format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;bgr24\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# cv2.imshow(\u0026#34;test_window\u0026#34;, frame)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# cv2.waitKey(1)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-console\" data-lang=\"console\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003effmpeg -list_devices true -f dshow -i dummy\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．\u003c/p\u003e","title":"映像入力の遅延をPyAVで解決してみる"},{"content":" multiprocessing × YOLOv8の詳細解説 Denoを利用した基本的なWebサイト作成 Deno Deploy設定 ","permalink":"http://localhost:1313/blog/posts/20231212_scheduled_update/","summary":"\u003cul\u003e\n\u003cli\u003emultiprocessing × YOLOv8の詳細解説\u003c/li\u003e\n\u003cli\u003eDenoを利用した基本的なWebサイト作成\u003c/li\u003e\n\u003cli\u003eDeno Deploy設定\u003c/li\u003e\n\u003c/ul\u003e","title":"今後の更新予定"},{"content":"RTSPでフレーム取得（OpenCV） RTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．\nimport cv2 url = \u0026#34;rtsp://username:password@ipaddress\u0026#34; # Tapo C200は末尾に画質を指定 url = \u0026#34;rtsp://username:password@ipaddress/stream1\u0026#34; cap = cv2.VideoCapture(url) while True: ret, frame = cap.read() pass ただこれだとTapo C200では1秒程度遅延が発生してしまう． 麻雀自動採譜 の実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．\nRTSPでフレーム取得（PyAV） RTSP映像の受信にはFFmpegのPythonバインディングである PyAV を利用した方法もあり、以下で映像受信が可能でした．\nimport av\rurl = \u0026#34;rtsp://username:password@ipaddress\u0026#34;\rcontainer = av.open(url)\rfor frame in container.decode(video=0):\rframe = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;)\rpass PyAVを利用した場合は遅延が大幅に減少した．やったね．\nRTSPで一定時間毎にフレーム取得（PyAV） 麻雀自動採譜 牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚でtime.sleep(n)を使用したところ以下のエラーが複数出現．\nmax delay reached. need to consume packet\rRTP: missed 6066 packets\rRTP: PT=60: bad cseq a657 expected=8ea5\rmax delay reached. need to consume packet sleepで無理に処理を中断させたため接続が不安定になったのかも． 通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．\nimport av interval = 10 # 10秒ごとに撮影 url = \u0026#34;rtsp://username:password@ipaddress\u0026#34; container = av.open(url) counter = 0 for frame in container.decode(video=0): if frame is not None and frame.time//interval\u0026gt;counter: counter+=1 frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) cv2.imwrite(f\u0026#39;save_{counter:04}.png\u0026#39;, frame) PyAVのcontainerから呼び出したフレームは撮影開始を0としたときの撮影時刻をtimeで取得できる． 撮影開始後数フレームはNoneが返ることがあるため注意． これで安定したn秒毎の撮影が実行できる．\n","permalink":"http://localhost:1313/blog/posts/20231201_getframertsp/","summary":"\u003ch2 id=\"rtspでフレーム取得opencv\"\u003eRTSPでフレーム取得（OpenCV）\u003c/h2\u003e\n\u003cp\u003eRTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e cv2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rtsp://username:password@ipaddress\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Tapo C200は末尾に画質を指定\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rtsp://username:password@ipaddress/stream1\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cv2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eVideoCapture(url)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ret, frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eただこれだとTapo C200では\u003cstrong\u003e1秒程度遅延\u003c/strong\u003eが発生してしまう．\n\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003e\nの実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．\u003c/p\u003e\n\u003ch2 id=\"rtspでフレーム取得pyav\"\u003eRTSPでフレーム取得（PyAV）\u003c/h2\u003e\n\u003cp\u003eRTSP映像の受信にはFFmpegのPythonバインディングである\n\u003ca href=\"https://pyav.org/docs/develop/index.html\" target=\"_blank\"\u003ePyAV\u003c/a\u003e\nを利用した方法もあり、以下で映像受信が可能でした．\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-python:\" data-lang=\"python:\"\u003eimport av\r\n\r\nurl = \u0026#34;rtsp://username:password@ipaddress\u0026#34;\r\ncontainer = av.open(url)\r\nfor frame in container.decode(video=0):\r\n    frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;)\r\n    pass\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ePyAVを利用した場合は遅延が大幅に減少した．やったね．\u003c/p\u003e\n\u003ch2 id=\"rtspで一定時間毎にフレーム取得pyav\"\u003eRTSPで一定時間毎にフレーム取得（PyAV）\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003e\n牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚で\u003ccode\u003etime.sleep(n)\u003c/code\u003eを使用したところ以下のエラーが複数出現．\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-raw:\" data-lang=\"raw:\"\u003emax delay reached. need to consume packet\r\nRTP: missed 6066 packets\r\nRTP: PT=60: bad cseq a657 expected=8ea5\r\nmax delay reached. need to consume packet\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003esleepで無理に処理を中断させたため接続が不安定になったのかも．\n通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．\u003c/p\u003e","title":"RTSP通信でカメラの映像を受信してみる"},{"content":"実装過程あれこれ１ これまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の Tapo C200 からフレームを受信することができるようになりました． 早速各カメラに割り当てたプロセス毎に適当に学習させておいた YOLOv8 による牌推論を実行し，結果を描画してみました．わくわく！\nはい，映像出力が不穏な感じに． Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．\nプロセス間でまったく映像出力のタイミングがあっていない フレーム間隔がまちまち 映像の乱れが酷い FPSが低い（Tapo入力時15→10弱） 特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね\u0026hellip;\u0026hellip;\nフレーム出力がばらつく理由を勝手に予想．こんな感じだろうか\n出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える． こんなときこそマルチプロセス処理の出番でしょ↓\n（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）\n既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．\n映像受信プロセス ×4（カメラの台数） 推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当） 推論結果統合プロセス ×4（カメラの台数） さあやるぞ！\n実装過程あれこれ２ ここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．\nbase64→内部UDPが一瞬頭を過るが無視して 公式ドキュメント を漁ってみます．\nドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか． Value, Array, Queue, Pipe, RawArray, Manager, shared_memory\u0026hellip;\u0026hellip;\n多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．\n試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに． フラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．\nそんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります． Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r概ね同じタイミングでフレームが描画されているように見えますね．\nFPSはいずれのカメラも15程度であり，入力時のFPSを保持したままリアルタイムで推論が実行できました． ちなみに，\nmultiprocessingではなくthreadingを使ってYOLOv8のモデルをスレッド間で共有する方法があるが，提供元である UltralyticsのMedium記事 では推奨方法， 公式ドキュメント では非推奨方法として紹介されていたため今回は見送り． 試験的に使用してるYOLOv8モデルは，現環境（GeForce RTX 3070）で学習可能な最大サイズのモデル（YOLOv8l）に対して複数の公開データセットを統合した5万枚強の牌画像を学習させたものである．統合作業時にデータセット間でラベルのズレが発生したため，検出結果に誤りがある．ラベルが混合した状態のデータセットではあるが安定した検出ができているため，データセットの整理によって更に高精度な検出の実現が期待できる． 背景部分の予期せぬ牌検出は信頼度で棄却できるため問題なし． 検証動画は双方画面録画の仕様上FPSが低下している． ソースコード 複数台のカメラのフレーム入力に対してYOLOv8による推論を並列実行するPythonプログラム ソースコード解説はこちらからどうぞ．\nimport time import msvcrt from multiprocessing import Process, Event, shared_memory import numpy as np import av import cv2 from ultralytics import YOLO # RTSPカメラ設定情報 info_list = [[\u0026#34;toncamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;nancamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;shacamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;peicamera\u0026#34;, \u0026#34;*\u0026#34;]] password = \u0026#34;*\u0026#34; # 1:1280x720, 2:640x360 mode = 1 # 推論プロセス数指定 N = 3 def rec_cam(cam_id, username, ipaddress, pre_flags, stop_flag, frame_info): url = f\u0026#34;rtsp://{username}:{password}@{ipaddress}:554/stream{mode}\u0026#34; container = av.open(url) counter = 0 start = time.time() for frame in container.decode(video=0): counter+=1 frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) # 共有メモリにフレームを書き込み idx = (cam_id)*N+counter%N mem_name = f\u0026#39;shared{idx+1:02}\u0026#39; shm = shared_memory.SharedMemory(name=mem_name) frame_sh = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) frame_sh[:] = frame[:] # 推論開始 pre_flags[idx].set() if stop_flag.is_set(): t = time.time() - start print(f\u0026#34;{username}, TIME:{round(t, 3)}, FRAME:{counter}, FPS:{round(counter/t, 3)}\u0026#34;) break container.close() cv2.destroyAllWindows() return def finish_monitor(stop_flag): while True: if msvcrt.kbhit() and msvcrt.getch() == b\u0026#39;q\u0026#39;: print(\u0026#34;Finished\u0026#34;) stop_flag.set() break return def predict_frame(pre_flag, pre_fin_flag, mem_name, stop_flag, username, frame_info): model = YOLO(\u0026#39;best.pt\u0026#39;) while True: if pre_flag.is_set(): # 推論 pre_flag.clear() shm = shared_memory.SharedMemory(name=mem_name) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # YOLOv8による推論実行 result = model(frame)[0] frame = result.plot() # 結果統合用共有メモリにのせる shm_result = shared_memory.SharedMemory(name=username) frame_new = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm_result.buf) frame_new[:]=frame[:] # 結果統合用 pre_fin_flag.set() if stop_flag.is_set(): break def view_frame(pre_fin_flag, username, stop_flag, frame_info): while True: if pre_fin_flag.is_set(): pre_fin_flag.clear() shm = shared_memory.SharedMemory(name=username) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # Full-HDだと大抵画面が埋まっちゃう # frame = cv2.resize(frame, None, None, 0.6, 0.6) cv2.imshow(username, frame) cv2.waitKey(1) if stop_flag.is_set(): break if __name__ == \u0026#39;__main__\u0026#39;: # メモリ確保情報 url = f\u0026#34;rtsp://{info_list[0][0]}:{password}@{info_list[0][1]}:554/stream{mode}\u0026#34; container = av.open(url) for i, frame in enumerate(container.decode(video=0)): frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) if frame is not None: break container.close() frame_info = {\u0026#34;nbytes\u0026#34; : frame.nbytes, \u0026#34;shape\u0026#34; : frame.shape, \u0026#34;dtype\u0026#34; : frame.dtype} # 停止プロセス stop_flag = Event() p = Process(target=finish_monitor, args=(stop_flag,)) p.start() # 表示プロセス群（カメラ台数分） view_processes = [] pre_fin_flags = [] mem_space = [] for i in range(4): pre_fin_flag = Event() pre_fin_flags.append(pre_fin_flag) username = info_list[i][0] # メモリ確保 今回は画像用 shm = shared_memory.SharedMemory(create=True, size=frame_info[\u0026#34;nbytes\u0026#34;], name = username) mem_space.append(shm) p = Process(target=view_frame, args=(pre_fin_flag, username, stop_flag, frame_info)) view_processes.append(p) p.start() # 推論プロセス群（カメラ台数×推論プロセス数） pre_processes = [] pre_flags = [] mem_names = [] for i in range(N*len(info_list)): pre_flag = Event() pre_flags.append(pre_flag) mem_name = f\u0026#39;shared{i+1:02}\u0026#39; mem_names.append(mem_name) # メモリ確保 shm = shared_memory.SharedMemory(create=True, size=frame_info[\u0026#34;nbytes\u0026#34;], name = mem_name) mem_space.append(shm) # frame_sh = np.ndarray(shape=frame_shape, dtype=frame_dtype, buffer=shm.buf) username = info_list[i//N][0] pre_fin_flag = pre_fin_flags[i//N] p = Process(target=predict_frame, args=(pre_flag, pre_fin_flag, mem_name, stop_flag, username, frame_info)) pre_processes.append(p) p.start() # 撮影プロセス群（カメラ台数） rec_processes = [] for cam_id, info in enumerate(info_list): p = Process(target=rec_cam, args=(cam_id, info[0], info[1], pre_flags, stop_flag, frame_info,)) rec_processes.append(p) p.start() p.join() for p in view_processes: p.join() for p in pre_processes: p.join() for p in rec_processes: p.join() # 共有メモリのリソース開放、削除 for mem_name in mem_names + [info[0] for info in info_list[:4]]: shm = shared_memory.SharedMemory(name = mem_name) shm.close() shm.unlink() ","permalink":"http://localhost:1313/blog/posts/20231201_multiprocessingforyolov8/","summary":"\u003ch2 id=\"実装過程あれこれ１\"\u003e実装過程あれこれ１\u003c/h2\u003e\n\u003cp\u003eこれまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の\n\u003ca href=\"https://www.tp-link.com/jp/smart-home/tapo/tapo-c200/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eTapo C200\u003c/a\u003e\nからフレームを受信することができるようになりました．\n\u003cbr\u003e\n早速各カメラに割り当てたプロセス毎に適当に学習させておいた\n\u003ca href=\"https://docs.ultralytics.com/ja/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eYOLOv8\u003c/a\u003e\nによる牌推論を実行し，結果を描画してみました．わくわく！\u003c/p\u003e\n\u003cp\u003eはい，映像出力が不穏な感じに．\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20231201_multiprocessingforyolov8/before.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20231201_multiprocessingforyolov8/before.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\u003c/p\u003e\n\u003cp\u003e卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプロセス間でまったく映像出力のタイミングがあっていない\u003c/li\u003e\n\u003cli\u003eフレーム間隔がまちまち\u003c/li\u003e\n\u003cli\u003e映像の乱れが酷い\u003c/li\u003e\n\u003cli\u003eFPSが低い（Tapo入力時15→10弱）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね\u0026hellip;\u0026hellip;\u003cbr\u003e\u003c/p\u003e\n\u003cp\u003eフレーム出力がばらつく理由を勝手に予想．こんな感じだろうか\u003c/p\u003e\n\u003cfigure\u003e\r\n    \u003cimg loading=\"lazy\" src=\"multiprocess_fig1.jpg\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える．\n\u003cbr\u003e\u003cbr\u003e\n\u003cspan style=\"font-size: 200%; color: red;\"\u003e\nこんなときこそマルチプロセス処理の出番でしょ↓\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\r\n    \u003cimg loading=\"lazy\" src=\"multiprocess_fig2.jpg\"/\u003e \r\n\u003c/figure\u003e\r\n\n（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）\u003c/p\u003e\n\u003cp\u003e既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e映像受信プロセス ×4（カメラの台数）\u003c/li\u003e\n\u003cli\u003e推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当）\u003c/li\u003e\n\u003cli\u003e推論結果統合プロセス ×4（カメラの台数）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eさあやるぞ！\u003c/p\u003e\n\u003ch2 id=\"実装過程あれこれ２\"\u003e実装過程あれこれ２\u003c/h2\u003e\n\u003cp\u003eここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．\u003cbr\u003e\nbase64→内部UDPが一瞬頭を過るが無視して\n\u003ca href=\"https://docs.python.org/ja/3.10/library/multiprocessing.html\" target=\"_blank\"\u003e公式ドキュメント\u003c/a\u003e\nを漁ってみます．\u003c/p\u003e\n\u003cp\u003eドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか．\nValue, Array, Queue, Pipe, RawArray, Manager, shared_memory\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．\u003c/p\u003e\n\u003cp\u003e試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに．\nフラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．\u003c/p\u003e\n\u003cp\u003eそんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります．\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20231201_multiprocessingforyolov8/after.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20231201_multiprocessingforyolov8/after.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\u003c/p\u003e","title":"複数カメラの入力に対しYOLOv8で並列推論する"},{"content":"ライブラリとは？ ライブラリとは，再利用可能なソフトウェア部品のことで，特定の機能をまとめたコードの再集合のことを指します．簡単に言えば，他の人が作ってくれた機能を自身のプログラムで使える便利なコード集のことです． Pythonには標準ライブラリと外部ライブラリの2種類があります．\n標準ライブラリ 標準ライブラリはPythonをインストールしたときに標準的に入っているライブラリのことです．通常のプログラム作成時に頻繁に必要となる機能をまとめたものが多くあります．例えば，\n乱数を作成する random 時刻を取得できる datetime, time ファイルシステムへのアクセスを実現する os, shutil 基礎的な数学関数 math HTMLサポート html URLを使ってインターネットにアクセスできる urllib スマートフォンで言うところのカメラ，アルバム，時計などのデフォルトアプリみたいなものです．これらはPythonが動く環境であれば勝手に入っているため，import ライブラリ名と記述すればすぐに利用することができます．助かる！！\n# サイコロの出目を出力 import random dice_roll = random.randint(1, 6) print(\u0026#34;サイコロの出目:\u0026#34;, dice_roll) 外部ライブラリ 外部ライブラリはPythonにはもともと組み込まれておらず，自身で導入するタイプのライブラリのことです．実装したいプログラムの機能に応じて適切に選択する必要があります．例えば，\n効率的な多次元配列の数値計算を実現する Numpy 画像処理の機能を提供する OpenCV グラフを簡単に描画できる Matplotlib 様々なアルゴリズムでの機械学習を可能とする scikit-learn Webアプリの開発に役立つ Django スマートフォンで言うところのゲームやチャットアプリなどのストアアプリみたいなものです．これらは自身でインストール作業をする必要がありますが，それさえ済ませば高機能な実装をいとも容易く実現できます！とても便利！！ 有志のプログラマが開発して公開してくれているため，感謝の気持ちを忘れずに使いましょう（Pythonもそうだけど）．\n# サイコロの出目を出力 import numpy as np dice_roll = np.random.randint(1, 7) print(\u0026#34;サイコロの出目:\u0026#34;, dice_roll) ライブラリを確認してみよう では，ここからは実際に手を動かしながら解説を進めます． まずはスタートメニューから Anaconda Prompt を開いてください．\nこれはコマンドプロンプトと呼ばれる画面で，ここにコマンドと呼ばれる命令文を入力することでコンピュータを操作できます． 試しに，ここにtreeと打ち込んでEnterキーを押してみてください．\n\u0026hellip;\u0026hellip;何かツリー構造の記述がたくさん出てきたかと思います．これはディレクトリ構造をツリー形式で表示するコマンドです（あんまり使わないけど）！ なかなか表示が終わらない場合はCtrl+Cキーでキャンセルしてください．\nではコマンドを使って，今使っているPythonで利用可能な外部ライブラリの一覧を見てみましょう！ Anaconda Promptにconda listと打ち込んでEnterキーを押してみてください．\nインストール済みライブラリとバージョンの一覧が表示されました．既にたくさん入っているようです！\nよくよく見てみると，数値計算ができるNumpyや表データを扱えるPandasなど，データサイエンスに役立つライブラリがたくさんあります． 実はAnacondaは科学計算のためのPython配布形式（ディストリビューション）の一つであり，既に役立つライブラリが揃えられているんです．\nこの状態，一見プログラムを書くのに便利な環境であるように見えるんですが実はそうでないケースがあります．\n実はライブラリは，機能を実現するために別のライブラリを参照するケースがあります．例えば，\n画像処理を実現するOpenCVには，画像を配列として扱うためにNumpyが必要 グラフを描画するMatplotlibには，画像描画のためにPillowが必要 機械学習を実現するscikit-learnには，高度な数値計算をするためにScipyが必要 またライブラリはバージョンによって提供される機能が変化することがあります．そのため，同じ実行環境に特定のバージョンを指定して参照ライブラリを導入しておく必要があるのです．\nこれを，ライブラリの依存関係と言います．この依存関係が原因で複数のライブラリを導入するときに問題が発生する可能性があるんです．\n例えば，ライブラリA-C及びB-Cの依存関係があるとき，AとBが共存するためにはCのバージョンが整合している必要があります．\nこうしたライブラリバージョンの試行錯誤を行う場合，先程表示させた複雑に依存関係が絡み合ったライブラリ群を扱うのはなかなか骨が折れます．\nまた，不要なライブラリは更に混乱を引き起こすので必要となる機能のための最小構成で開発環境を整える必要があります．\nそこで役立つのが，仮想環境です！！！（やっと本題）\n仮想環境 仮想環境とは，プログラムを動かすときに仮に作って利用する動作環境のことです． 簡単に言うと「依存関係の解決が面倒だから，まっさらな世界を一から創造すればいいじゃん！！」です．\nでは，実際に作ってみましょう．\n先ほど開いたAnaconda Promptにconda create -n test python=3.12と打ち込んで実行（Proceed([y]/n)?にはyを入力して処理を続行） conda activate testと打ち込んで実行 ここでは，testという名前の仮想環境を作成してその環境に切り替える，という作業をしてもらいました． 最新行の左側が(test)と表示されていれば成功です！\n先ほどまでは(base)と表示されていたはずです．Anacondaではデフォルトでbase環境があり，そこにたくさんのライブラリがありました． ではconda listで現在のtest環境のライブラリ構成を見てみましょう！\nbase環境と比べてライブラリの数がかなり少なくなったはずです．ここに表示されているライブラリはPython3.12に付随する必要最低限のライブラリです． このきれいな環境に最小限の適切なライブラリを導入することで，快適なコーディングライフを送ることができます！！\n番外編：Conda仮想環境の管理 複数の仮想環境を扱うときに便利なコマンドをまとめておきます！\n仮想環境の作成（環境名がtest，Python3.12の場合）\nconda create -n test python=3.12 仮想環境への移行\nconda activate test 仮想環境から離脱\nconda deactivate 仮想環境の削除\nconda remove -n test --all 仮想環境一覧の表示\nconda info -e ","permalink":"http://localhost:1313/blog/posts/20250506_condavenv/","summary":"\u003ch2 id=\"ライブラリとは\"\u003eライブラリとは？\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eライブラリ\u003c/strong\u003eとは，再利用可能なソフトウェア部品のことで，特定の機能をまとめたコードの再集合のことを指します．簡単に言えば，他の人が作ってくれた機能を自身のプログラムで使える便利なコード集のことです．\nPythonには\u003cstrong\u003e標準ライブラリ\u003c/strong\u003eと\u003cstrong\u003e外部ライブラリ\u003c/strong\u003eの2種類があります．\u003c/p\u003e\n\u003ch3 id=\"標準ライブラリ\"\u003e標準ライブラリ\u003c/h3\u003e\n\u003cp\u003e標準ライブラリはPythonをインストールしたときに標準的に入っているライブラリのことです．通常のプログラム作成時に頻繁に必要となる機能をまとめたものが多くあります．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e乱数を作成する random\u003c/li\u003e\n\u003cli\u003e時刻を取得できる datetime, time\u003c/li\u003e\n\u003cli\u003eファイルシステムへのアクセスを実現する os, shutil\u003c/li\u003e\n\u003cli\u003e基礎的な数学関数 math\u003c/li\u003e\n\u003cli\u003eHTMLサポート html\u003c/li\u003e\n\u003cli\u003eURLを使ってインターネットにアクセスできる urllib\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスマートフォンで言うところのカメラ，アルバム，時計などのデフォルトアプリみたいなものです．これらはPythonが動く環境であれば勝手に入っているため，\u003ccode\u003eimport ライブラリ名\u003c/code\u003eと記述すればすぐに利用することができます．助かる！！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# サイコロの出目を出力\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e random\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edice_roll \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e random\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandint(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;サイコロの出目:\u0026#34;\u003c/span\u003e, dice_roll)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"外部ライブラリ\"\u003e外部ライブラリ\u003c/h3\u003e\n\u003cp\u003e外部ライブラリはPythonにはもともと組み込まれておらず，自身で導入するタイプのライブラリのことです．実装したいプログラムの機能に応じて適切に選択する必要があります．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e効率的な多次元配列の数値計算を実現する Numpy\u003c/li\u003e\n\u003cli\u003e画像処理の機能を提供する OpenCV\u003c/li\u003e\n\u003cli\u003eグラフを簡単に描画できる Matplotlib\u003c/li\u003e\n\u003cli\u003e様々なアルゴリズムでの機械学習を可能とする scikit-learn\u003c/li\u003e\n\u003cli\u003eWebアプリの開発に役立つ Django\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスマートフォンで言うところのゲームやチャットアプリなどのストアアプリみたいなものです．これらは自身でインストール作業をする必要がありますが，それさえ済ませば高機能な実装をいとも容易く実現できます！とても便利！！\n有志のプログラマが開発して公開してくれているため，感謝の気持ちを忘れずに使いましょう（Pythonもそうだけど）．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# サイコロの出目を出力\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e np\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edice_roll \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandom\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandint(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;サイコロの出目:\u0026#34;\u003c/span\u003e, dice_roll)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"ライブラリを確認してみよう\"\u003eライブラリを確認してみよう\u003c/h2\u003e\n\u003cp\u003eでは，ここからは実際に手を動かしながら解説を進めます．\nまずはスタートメニューから \u003cstrong\u003eAnaconda Prompt\u003c/strong\u003e を開いてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"img_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"console_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこれは\u003cstrong\u003eコマンドプロンプト\u003c/strong\u003eと呼ばれる画面で，ここに\u003cstrong\u003eコマンド\u003c/strong\u003eと呼ばれる命令文を入力することでコンピュータを操作できます．\n試しに，ここに\u003ccode\u003etree\u003c/code\u003eと打ち込んでEnterキーを押してみてください．\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;何かツリー構造の記述がたくさん出てきたかと思います．これはディレクトリ構造をツリー形式で表示するコマンドです（あんまり使わないけど）！\nなかなか表示が終わらない場合はCtrl+Cキーでキャンセルしてください．\u003c/p\u003e\n\u003cp\u003eではコマンドを使って，今使っているPythonで利用可能な外部ライブラリの一覧を見てみましょう！\nAnaconda Promptに\u003ccode\u003econda list\u003c/code\u003eと打ち込んでEnterキーを押してみてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"modules.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eインストール済みライブラリとバージョンの一覧が表示されました．既にたくさん入っているようです！\u003c/p\u003e\n\u003cp\u003eよくよく見てみると，数値計算ができるNumpyや表データを扱えるPandasなど，データサイエンスに役立つライブラリがたくさんあります．\n実はAnacondaは科学計算のためのPython配布形式（ディストリビューション）の一つであり，既に役立つライブラリが揃えられているんです．\u003c/p\u003e\n\u003cp\u003eこの状態，一見プログラムを書くのに便利な環境であるように見えるんですが実はそうでないケースがあります．\u003c/p\u003e","title":"【入門1】Python仮想環境"},{"content":"JupyterLab+Visual Studio Codeを使ったPython実行 Visual Studio CodeにJupyter拡張機能をインストール 空のファイルを作成し，拡張子を.ipynbとする ＋コードボタンからコードブロックを作成し，Pythonプログラムを記述 右上カーネルの選択から実行したいPython環境を選択し コードブロック左側の実行ボタン▷を押下 ポップアップが出てきたらインストールをクリックしてipykernelをインストール （pip install ipykernelを当該環境で実行してインストールも出来ます） YOLOのセットアップ 今回は最新のYOLOを簡単に扱うことができるUltralyticsパッケージを利用します．\nconda create --name yolo-env python=3.11を実行して仮想環境を作成 conda activate yolo-envで仮想環境を有効化 pip install ultralyticsでUltralyticsパッケージをインストール ここまで済んだら，テストコードを動かしてみましょう！\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolo11n.pt\u0026#34;) results = model(\u0026#34;https://ultralytics.com/images/bus.jpg\u0026#34;) results[0].show() バスを背景にした画像の物体検出が表示されたはずです！\n作業ディレクトリを開くと，yolo11n.ptというファイルがダウンロードされています．これが，YOLOのモデルファイルです．これはYOLOのデフォルトの訓練済みモデルで，COCO Datasetに収録された80種類の物体を識別することが出来ます．\n参考 https://docs.ultralytics.com/ja/quickstart/#understanding-settings\n","permalink":"http://localhost:1313/blog/posts/20250506_yolojupyter/","summary":"\u003ch2 id=\"jupyterlabvisual-studio-codeを使ったpython実行\"\u003eJupyterLab+Visual Studio Codeを使ったPython実行\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eVisual Studio CodeにJupyter拡張機能をインストール\u003c/li\u003e\n\u003cli\u003e空のファイルを作成し，拡張子を\u003ccode\u003e.ipynb\u003c/code\u003eとする\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e＋コード\u003c/code\u003eボタンからコードブロックを作成し，Pythonプログラムを記述\u003c/li\u003e\n\u003cli\u003e右上\u003ccode\u003eカーネルの選択\u003c/code\u003eから実行したいPython環境を選択し\u003c/li\u003e\n\u003cli\u003eコードブロック左側の実行ボタン\u003ccode\u003e▷\u003c/code\u003eを押下\u003c/li\u003e\n\u003cli\u003eポップアップが出てきたら\u003ccode\u003eインストール\u003c/code\u003eをクリックして\u003ccode\u003eipykernel\u003c/code\u003eをインストール\n（\u003ccode\u003epip install ipykernel\u003c/code\u003eを当該環境で実行してインストールも出来ます）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"vsjupyter.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"yoloのセットアップ\"\u003eYOLOのセットアップ\u003c/h2\u003e\n\u003cp\u003e今回は最新のYOLOを簡単に扱うことができるUltralyticsパッケージを利用します．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003econda create --name yolo-env python=3.11\u003c/code\u003eを実行して仮想環境を作成\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate yolo-env\u003c/code\u003eで仮想環境を有効化\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install ultralytics\u003c/code\u003eでUltralyticsパッケージをインストール\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eここまで済んだら，テストコードを動かしてみましょう！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e ultralytics \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e YOLO\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emodel \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e YOLO(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;yolo11n.pt\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresults \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://ultralytics.com/images/bus.jpg\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresults[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eバスを背景にした画像の物体検出が表示されたはずです！\u003c/p\u003e\n\u003cp\u003e作業ディレクトリを開くと，\u003ccode\u003eyolo11n.pt\u003c/code\u003eというファイルがダウンロードされています．これが，YOLOのモデルファイルです．これはYOLOのデフォルトの訓練済みモデルで，COCO Datasetに収録された80種類の物体を識別することが出来ます．\u003c/p\u003e\n\u003ch2 id=\"参考\"\u003e参考\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://docs.ultralytics.com/ja/quickstart/#understanding-settings\"\u003ehttps://docs.ultralytics.com/ja/quickstart/#understanding-settings\u003c/a\u003e\u003c/p\u003e","title":"【入門2】YOLO，Jupyterのセットアップ"},{"content":"アノテーションとは？ 機械学習の分類のひとつである教師あり学習では，まず入力データと出力データの組を複数用意し，それらを使って機械学習モデルを訓練します．\n入力データと出力データの組は，タスクによって様々です．例えば，\n入力：画像データ，出力：それぞれが犬or猫 入力：家の情報（間取り，立地，築年数など），出力：家の価格 入力：これまでの気温，出力：明日の気温 入力：画像データ，出力：各物体の座標 どんなタスクを前提とする場合でも，これら入力データと出力データの組を作る作業が必要となります．これがアノテーションです．\n環境構築 今回はYOLOによる画像物体認識を目標とします．そこで，アノテーションで作るべきデータは以下になります．\n入力：画像データ，出力：各物体の座標 頑張れば用意した画像の座標を数えてアノテーションできそうですが，今回はより簡単に作業を進めるためにLabelmeというツールを使います．GUIが整備されたソフトウェアですが，Pythonライブラリとして無償で提供されています！（ありがたい！！）\nAnaconda Promptを開く conda create -n annot_env python=3.12で仮想環境を作成 conda activate annot_envで仮想環境を有効化 pip install --upgrade labelmeと入力しLabelmeをインストール これでインストールは完了です．そのままannot_env環境内でlabelmeコマンドを実行してみてください！\nこの画面が表示されたら成功です！\nいざ，アノテーション 画像読み込み 今回は画像から麻雀牌を物体認識するタスクに挑戦してみます！サンプル画像は以下からダウンロード，解凍してください．\nサンプル画像 Zipファイルの解凍が済んだらLabelmeの画面左側にあるOpen Dirから，画像のディレクトリを開いてください．\nこれでアノテーションの準備は完了です！\n範囲選択，ラベル付け アノテーションの流れは以下になります．\n画像を右クリック，Create Rectangleをクリック 物体の範囲を四角形で囲んで選択，間違えたらEscキーで戻る 出てきたポップアップの上部Enter object labelにラベルを入力してOKをクリック 2と3を物体の数だけひたすら繰り返す 画像内のアノテーションが終わったら画面左部Saveから画像と同名のJSONファイルを保存 ちょっとやってみますね～\nはい，できました．ラベルは牌の種類（萬子筒子索子字牌）の略称です．\n\u0026hellip;\u0026hellip;\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\nめんどくさい\n機械学習では一般的にデータの数が大きいほど性能が上がりやすいとされています．つまり，（要件の要求レベルにもよりますが）精度を上げるためにはたくさんアノテーションする必要があるんです．\nそこで，特定のタスクの機械学習を考える場合はアノテーションに取り掛かる前に，データセットが利用可能なライセンスで公開されていないかどうか調べるところからスタートします．例えばYOLOの場合，Roboflow，VisualData，Google Open Images Datasetなどが画像データセットを公開している他，タスクに応じて論文と併せて公開されているデータセットも多数あります．\nまた今回はアノテーション入門ということで手作業での方法を紹介していますが，画像認識AIによる半自動ツールもあるので興味があれば調べてみてください．\n今回は，なんと，特別に，アノテーション済みファイルを用意してあります！\nアノテーション結果（JSON） ZIPファイルをダウンロード＆解凍したら，datasetsという名前のディレクトリを作成し，先程の画像ファイルとJSONファイルを以下の構造で配置してください．（image_1.jsonは自作のファイルで構いません！）\n.\r└── datasets/\r├── image_1.jpg\r├── image_1.json\r├── image_2.jpg\r├── image_2.json\r├── 〜省略〜\r├── image_10.jpg\r└── image_10.json データセットフォーマットの変換 では，学習させるデータの中身を一度見てみましょう！image_1.jsonを開いてみます．\nJSONはデータ記述形式のひとつで，キーと値を並べて書くのが特徴です．Pythonの辞書型そっくりですね． ラベルや座標がファイルの中に複数記述されていることがわかります．\n少しスクロールすると，よくわからない文字列が大量に記述されています．実はこれ，Base64と呼ばれるエンコード形式で画像そのものが書きつけられているんです．これにより，このJSONファイル単体で入力データを表現することが出来ます．\nさて，ひととおり学習データの中身を見てもらいましたが，実はこのままではYOLOの学習が出来ないんです．\nYOLOに学習させるためには，画像のアノテーションを以下の形式で記述する必要があります．\n画像ファイル名.txt\nlabel_id X Y Width Height 例えば，image_1.jsonをこの形式で記述すると以下のようになります．\nimage_1.txt\n4 0.171569 0.375817 0.142157 0.183007\r6 0.286765 0.382353 0.124183 0.183007\r0 0.401144 0.398693 0.127451 0.185185\r5 0.517157 0.408497 0.117647 0.178649\r10 0.638889 0.418301 0.125817 0.189542\r7 0.767157 0.436819 0.127451 0.174292 YOLOのデータセットでは，ラベルの番号，X座標，Y座標，幅，高さを列挙して記述します．\nこれを手作業で変換するのはかなり面倒です．\nそこで今回は，Labelmeで保存したJSONファイルをYOLOのデータセット形式に変換するツール，labelme2yoloを使います．condaのannot_env環境下で以下の手順で変換します．\npip install labelme2yoloでlabelme2yoloをインストール datasetディレクトリの一つ上の階層で以下のコマンドを実行 labelme2yolo --json_dir ./datasets/ --val_size 0.10 --output_format \u0026quot;bbox\u0026quot; これが済んだら，datasetsディレクトリの中にYOLODatasetが生成されているはずです．構造は以下のようになっています．\n.\r└── YOLODatasets/\r├── images/\r│ ├── train/\r│ │ ├── image_1.jpg\r│ │ └── ～省略～\r│ └── val/\r│ └── image_7.jpg\r├── labels/\r│ ├── train/\r│ │ ├── image_1.txt\r│ │ └── ～省略～\r│ └── val/\r│ └── image_7.txt\r└── dataset.yaml 先ほどのデータセット構造より複雑になっていますね．画像とラベルが別のフォルダに格納され，更にその中で訓練用(train)と評価用(val)に分かれています．\nlabelsディレクトリのテキストファイルを見てみましょう．YOLOのフォーマットに変換され，ラベルIDや座標が記録されているはずです．\ndataset.yamlの中身を見てみましょう．\nデータセットまでの絶対パス，訓練評価用ディレクトリのパス，ラベルIDとラベル名の対応が記載されています．YOLOの学習時にはこのdataset.yamlを利用してデータセットを読み込みます．\nYOLOの学習 それでは，いよいよ作ったデータセットを使ってYOLOの学習をしてみましょう！！\nまず，テスト用の画像を用意します．以下のリンクから画像ファイルimage_11.jpgをダウンロードしてdataset.yamlと同階層に配置します．\nテスト画像image_11.jpg dataset.yamlと同階層に.ipynbファイルを作成して，以下のPythonプログラムを実行してみましょう．\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolo11n.pt\u0026#34;) model.train(data=\u0026#39;dataset.yaml\u0026#39;, epochs=300, verbose=False) results = model(\u0026#34;image_11.jpg\u0026#34;) results[0].show() CPUのみの実行環境ではかなり時間がかかるので気長に待ちましょう～\n学習が終了すると，image_11.jpgに対する推論結果が表示されます．\n未検出や誤検出がちらほらありますが，部分的に検出が成功していることがわかります．データセットサイズやエポック数を変えることで性能の改善が期待できます．\nまた，データセットディレクトリのruns内を見ると，weightディレクトリにbest.ptが保存されていることがわかります． これは今回学習したモデルファイルで，実際に物体検出をシステム開発に組み込むときに使用します．\n","permalink":"http://localhost:1313/blog/posts/20250506_yoloannotation/","summary":"\u003ch2 id=\"アノテーションとは\"\u003eアノテーションとは？\u003c/h2\u003e\n\u003cp\u003e機械学習の分類のひとつである\u003cstrong\u003e教師あり学習\u003c/strong\u003eでは，まず入力データと出力データの組を複数用意し，それらを使って機械学習モデルを訓練します．\u003c/p\u003e\n\u003cp\u003e入力データと出力データの組は，タスクによって様々です．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e入力：画像データ，出力：それぞれが犬or猫\u003c/li\u003e\n\u003cli\u003e入力：家の情報（間取り，立地，築年数など），出力：家の価格\u003c/li\u003e\n\u003cli\u003e入力：これまでの気温，出力：明日の気温\u003c/li\u003e\n\u003cli\u003e入力：画像データ，出力：各物体の座標\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eどんなタスクを前提とする場合でも，これら\u003cstrong\u003e入力データと出力データの組を作る作業\u003c/strong\u003eが必要となります．これが\u003cstrong\u003eアノテーション\u003c/strong\u003eです．\u003c/p\u003e\n\u003ch2 id=\"環境構築\"\u003e環境構築\u003c/h2\u003e\n\u003cp\u003e今回はYOLOによる画像物体認識を目標とします．そこで，アノテーションで作るべきデータは以下になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e入力：画像データ，出力：各物体の座標\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e頑張れば用意した画像の座標を数えてアノテーションできそうですが，今回はより簡単に作業を進めるために\u003cstrong\u003eLabelme\u003c/strong\u003eというツールを使います．GUIが整備されたソフトウェアですが，Pythonライブラリとして無償で提供されています！（ありがたい！！）\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAnaconda Promptを開く\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda create -n annot_env python=3.12\u003c/code\u003eで仮想環境を作成\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate annot_env\u003c/code\u003eで仮想環境を有効化\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install --upgrade labelme\u003c/code\u003eと入力しLabelmeをインストール\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eこれでインストールは完了です．そのまま\u003ccode\u003eannot_env\u003c/code\u003e環境内で\u003ccode\u003elabelme\u003c/code\u003eコマンドを実行してみてください！\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"labelme.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこの画面が表示されたら成功です！\u003c/p\u003e\n\u003ch2 id=\"いざアノテーション\"\u003eいざ，アノテーション\u003c/h2\u003e\n\u003ch3 id=\"画像読み込み\"\u003e画像読み込み\u003c/h3\u003e\n\u003cp\u003e今回は画像から麻雀牌を物体認識するタスクに挑戦してみます！サンプル画像は以下からダウンロード，解凍してください．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hakoshi-normal/yolo_sample/raw/refs/heads/main/images/images.zip\"\u003eサンプル画像\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eZipファイルの解凍が済んだらLabelmeの画面左側にある\u003ccode\u003eOpen Dir\u003c/code\u003eから，画像のディレクトリを開いてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"labelme_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこれでアノテーションの準備は完了です！\u003c/p\u003e\n\u003ch3 id=\"範囲選択ラベル付け\"\u003e範囲選択，ラベル付け\u003c/h3\u003e\n\u003cp\u003eアノテーションの流れは以下になります．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e画像を右クリック，\u003ccode\u003eCreate Rectangle\u003c/code\u003eをクリック\u003c/li\u003e\n\u003cli\u003e物体の範囲を四角形で囲んで選択，間違えたら\u003ccode\u003eEsc\u003c/code\u003eキーで戻る\u003c/li\u003e\n\u003cli\u003e出てきたポップアップの上部\u003ccode\u003eEnter object label\u003c/code\u003eにラベルを入力して\u003ccode\u003eOK\u003c/code\u003eをクリック\u003c/li\u003e\n\u003cli\u003e2と3を物体の数だけひたすら繰り返す\u003c/li\u003e\n\u003cli\u003e画像内のアノテーションが終わったら画面左部\u003ccode\u003eSave\u003c/code\u003eから画像と同名のJSONファイルを保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eちょっとやってみますね～\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"annotated.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eはい，できました．ラベルは牌の種類（萬子筒子索子字牌）の略称です．\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 150%; color: red;\"\u003eめんどくさい\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e機械学習では一般的にデータの数が大きいほど性能が上がりやすいとされています．つまり，（要件の要求レベルにもよりますが）精度を上げるためにはたくさんアノテーションする必要があるんです．\u003c/p\u003e\n\u003cp\u003eそこで，特定のタスクの機械学習を考える場合はアノテーションに取り掛かる前に，データセットが利用可能なライセンスで公開されていないかどうか調べるところからスタートします．例えばYOLOの場合，\u003ca href=\"https://universe.roboflow.com/\"\u003eRoboflow\u003c/a\u003e，\u003ca href=\"https://visualdata.io/discovery\"\u003eVisualData\u003c/a\u003e，\u003ca href=\"https://storage.googleapis.com/openimages/web/index.html\"\u003eGoogle Open Images Dataset\u003c/a\u003eなどが画像データセットを公開している他，タスクに応じて論文と併せて公開されているデータセットも多数あります．\u003c/p\u003e\n\u003cp\u003eまた今回はアノテーション入門ということで手作業での方法を紹介していますが，画像認識AIによる半自動ツールもあるので興味があれば調べてみてください．\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e今回は，なんと，特別に，アノテーション済みファイルを用意してあります！\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hakoshi-normal/yolo_sample/raw/refs/heads/main/images/labels.zip\"\u003eアノテーション結果（JSON）\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eZIPファイルをダウンロード＆解凍したら，\u003ccode\u003edatasets\u003c/code\u003eという名前のディレクトリを作成し，先程の画像ファイルとJSONファイルを以下の構造で配置してください．（\u003ccode\u003eimage_1.json\u003c/code\u003eは自作のファイルで構いません！）\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-planetext\" data-lang=\"planetext\"\u003e.\r\n└── datasets/\r\n    ├── image_1.jpg\r\n    ├── image_1.json\r\n    ├── image_2.jpg\r\n    ├── image_2.json\r\n    ├── 〜省略〜\r\n    ├── image_10.jpg\r\n    └── image_10.json\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"データセットフォーマットの変換\"\u003eデータセットフォーマットの変換\u003c/h3\u003e\n\u003cp\u003eでは，学習させるデータの中身を一度見てみましょう！\u003ccode\u003eimage_1.json\u003c/code\u003eを開いてみます．\u003c/p\u003e","title":"【入門3】YOLOアノテーションと学習"},{"content":"はじめに 自動採譜プロジェクトの息抜き的な記事です．\nせっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして， ところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．\nカメラ切り替えトリガー カメラの切り替えトリガーを「牌山にプレーヤーが触れたタイミング」と設定しました．\n自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．\nツモ位置の検出 まず，天カメの映像から卓上のツモ位置を特定する必要があります．\n牌山の形を見てみましょう．\n対局開始前はこんな感じ．\n対局開始直後はこんな感じ．\n対局中はこんな感じ．\n対局開始前は4つ牌山がありますが，対局開始以降は牌山が必ず3つ以下になります王牌の扱い上，4つのときもあります．\n牌山が3つのとき\n牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．\n牌山が4つのとき\n一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．\n実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．\n牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．\n実装概要 元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます． cv2.inRange()を使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．\n二値化した画像に対し，cv2.findContours()を使用し，牌山となりうる矩形を検出します．\n手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます． その後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．\nツモプレイヤーの検出 ツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します． MediaPipe のハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します． 今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．\nプレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．\nオートスイッチャー実装！！！ OBSを使用して配信するため，カメラ切り替えには obs-websocket-py を使用しました．こちらの サンプル がそのまま動作して助かりました．\nGUI実装にはHTML/JSでの記述が可能な Eel を使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．\n実際の動作の様子がこちら．\nYour browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．\nあと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．\n","permalink":"http://localhost:1313/blog/posts/20240210_mahjongautocamswitcher/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e自動採譜プロジェクト\u003c/a\u003eの息抜き的な記事です．\u003c/p\u003e\n\u003cp\u003eせっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして，\nところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．\u003c/p\u003e\n\u003ch2 id=\"カメラ切り替えトリガー\"\u003eカメラ切り替えトリガー\u003c/h2\u003e\n\u003cp\u003eカメラの切り替えトリガーを「\u003cstrong\u003e牌山にプレーヤーが触れたタイミング\u003c/strong\u003e」と設定しました．\u003c/p\u003e\n\u003cp\u003e自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．\u003c/p\u003e\n\u003ch2 id=\"ツモ位置の検出\"\u003eツモ位置の検出\u003c/h2\u003e\n\u003cp\u003eまず，天カメの映像から卓上のツモ位置を特定する必要があります．\u003cbr\u003e牌山の形を見てみましょう．\u003c/p\u003e\n\u003cp\u003e対局開始前はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_0.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局開始直後はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局中はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_2.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局開始前は4つ牌山がありますが，対局開始以降は牌山が\u003cdel\u003e必ず3つ以下になります\u003c/del\u003e王牌の扱い上，4つのときもあります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e牌山が3つのとき\u003c/p\u003e\n\u003cp\u003e牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e牌山が4つのとき\u003c/p\u003e\n\u003cp\u003e一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"fig1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．\u003c/p\u003e\n\u003cp\u003e牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．\u003c/p\u003e\n\u003ch3 id=\"実装概要\"\u003e実装概要\u003c/h3\u003e\n\u003cp\u003e元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます．\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"original_image.png\"/\u003e \r\n\u003c/figure\u003e\r\n\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecv2.inRange()\u003c/code\u003eを使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"mask_hai.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e二値化した画像に対し，\u003ccode\u003ecv2.findContours()\u003c/code\u003eを使用し，牌山となりうる矩形を検出します．\u003c/p\u003e\n\u003cp\u003e手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます．\nその後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumohai.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"ツモプレイヤーの検出\"\u003eツモプレイヤーの検出\u003c/h2\u003e\n\u003cp\u003eツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します．\n\u003ca href=\"https://developers.google.com/mediapipe/solutions/vision/hand_landmarker\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eMediaPipe\u003c/a\u003e\nのハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します．\n今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumo1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eプレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumo2.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"オートスイッチャー実装\"\u003eオートスイッチャー実装！！！\u003c/h2\u003e\n\u003cp\u003eOBSを使用して配信するため，カメラ切り替えには\n\u003ca href=\"https://github.com/Elektordi/obs-websocket-py/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eobs-websocket-py\u003c/a\u003e\nを使用しました．こちらの\n\u003ca href=\"https://github.com/Elektordi/obs-websocket-py/blob/master/samples/switch_scenes.py\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eサンプル\u003c/a\u003e\nがそのまま動作して助かりました．\u003c/p\u003e\n\u003cp\u003eGUI実装にはHTML/JSでの記述が可能な\n\u003ca href=\"https://github.com/python-eel/Eel\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eEel\u003c/a\u003e\nを使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．\u003c/p\u003e\n\u003cp\u003e実際の動作の様子がこちら．\u003c/p\u003e\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\n\u003cp\u003e現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．\u003c/p\u003e\n\u003cp\u003eあと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．\u003c/p\u003e","title":"麻雀放送対局用のオートスイッチャーを作りたい"},{"content":"はじめに 複数カメラの入力に対しYOLOv8で並列推論するのソースコード解説です． 解説のため，一部表現を変えている部分があります．\n大まかな流れは以下の通り．\n共有メモリ確保 フレーム入力プロセス カメラからフレームを取得 フレームを共有メモリ1に保存 推論プロセス 共有フレームを呼び出し 推論 推論結果を描画したフレームを共有メモリ2に保存 フレーム表示プロセス 共有フレームを呼び出し フレームの画面描画 プロセス間の遷移はmultiprocessingモジュールのEvent関数をフラグとして利用します． カメラの台数が増えた場合はプロセス数を増やせば対応できます．\n事前準備 設定情報 cam_N = 4 # カメラ台数 proccess_N = 3 # カメラ1台あたりのプロセス数 frame_info = {\u0026#34;nbytes\u0026#34; : frame.nbytes, \u0026#34;shape\u0026#34; : frame.shape, \u0026#34;dtype\u0026#34; : frame.dtype} フレームのサイズや型はメモリ確保，呼び出し時に必要になります．\n共有メモリの確保 shm = shared_memory.SharedMemory(create=True, size=frame.nbytes, name = mem_name) 共有メモリの確保はこの一文でできます．めっちゃ便利ですね． ただし，mem_nameを動的に作成して複数の共有メモリを確保する場合，注意事項があります． shared_memory.SharedMemoryの戻り値を格納する変数（上記コードではshm）を設定しなかった場合や上書きした場合，それ以前の共有メモリ領域は参照できなくなります．その場合はshmを格納するリストを用意しておくとアクセス可能になります．\nプロセスの立ち上げ p = Process(target=関数名, args=(引数,)) p.start() # プロセス開始 p.join() # プロセス終了 複数プロセスを立ち上げる場合は p をリストに入れておくと管理が楽です．\nフレーム入力プロセス def rec_cam(cam_id, pre_flags, frame_info): cap = cv2.VideoCapture(cam_id) while True: _, frame = cap.read() idx = (cam_id)*proccess_N+counter%proccess_N mem_name = f\u0026#39;shared{idx+1:02}\u0026#39; shm = shared_memory.SharedMemory(name=mem_name) frame_sh = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) frame_sh[:] = frame[:] # 推論開始 pre_flags[idx].set() cap.close() multiprocessingのSharedMemoryでは共有メモリを名前をつけて呼び出すことができます．便利！ ここでは，\u0026ldquo;shared1[番号]\u0026ldquo;という名前で共有メモリを確保している（後述）．カメラ台数✕カメラあたりのプロセス数分の共有メモリ領域が必要になるため， あるカメラのあるフレームに対する保存領域は，カメラ番号✕カメラ1台あたりのプロセス数＋フレーム番号%カメラ1台あたりのプロセス数で求めることができる．と同時にこの番号は推論プロセスを動作させるフラグ管理のインデックスとしても使う．\n推論プロセス def predict_frame(pre_flag, view_flag, mem_name, cam_id, frame_info): while True: if pre_flag.is_set(): # 推論開始 pre_flag.clear() shm = shared_memory.SharedMemory(name=mem_name) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # 重い推論処理がここに入る（frameに結果を反映する） # 結果統合用共有メモリにのせる shm_result = shared_memory.SharedMemory(name=f\u0026#39;shared2{cam_id}\u0026#39;) frame_new = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm_result.buf) frame_new[:]=frame[:] # 結果表示 view_flag.set() カメラ台数✕カメラあたりのプロセス数分の推論プロセスを立ち上げます． 推論開始用のフラグや共有メモリの呼び出し名は予め与えた状態でプロセスを立ち上げる．フレーム入力プロセスでの推論開始用フラグの有効化を待ち受けています． 処理済みのフレーを格納する共有メモリ領域名は\u0026rsquo;shared2[番号]\u0026lsquo;としており，カメラの台数分事前に確保してあります． 検出結果を描画する必要がない場合は，フレーム表示プロセスを介する必要がないため，別プロセスに推論結果を渡したり，UDP通信などで端末間の結果送信が実装できると思います．\nフレーム表示プロセス def view_frame(view_flag, cam_id, frame_info): while True: if view_flag.is_set(): view_flag.clear() shm = shared_memory.SharedMemory(name=f\u0026#39;shared2{cam_id}\u0026#39;) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) cv2.imshow(f\u0026#39;window_{cam_id}\u0026#39;, frame) cv2.waitKey(1) 推論プロセスでのフレーム表示用フラグの有効化を待ち受けています．\nおわりに 今回は，以前実装した並列画像処理プログラムの解説でした．コードブロックを断片的に示しただけなので，具体的な実装コードを知りたい方はソースコードから参照できます．\n","permalink":"http://localhost:1313/blog/posts/20231226_multiprocessingforrealtimevideo/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/2023_12_01_multiprocessingforyolov8/\" target=\"_blank\"\u003e複数カメラの入力に対しYOLOv8で並列推論する\u003c/a\u003eのソースコード解説です．\n解説のため，一部表現を変えている部分があります．\u003c/p\u003e\n\u003cp\u003e大まかな流れは以下の通り．\u003c/p\u003e\n\u003col start=\"0\"\u003e\n\u003cli\u003e共有メモリ確保\u003c/li\u003e\n\u003cli\u003eフレーム入力プロセス\n\u003col\u003e\n\u003cli\u003eカメラからフレームを取得\u003c/li\u003e\n\u003cli\u003eフレームを共有メモリ1に保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e推論プロセス\n\u003col\u003e\n\u003cli\u003e共有フレームを呼び出し\u003c/li\u003e\n\u003cli\u003e推論\u003c/li\u003e\n\u003cli\u003e推論結果を描画したフレームを共有メモリ2に保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eフレーム表示プロセス\n\u003col\u003e\n\u003cli\u003e共有フレームを呼び出し\u003c/li\u003e\n\u003cli\u003eフレームの画面描画\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eプロセス間の遷移はmultiprocessingモジュールのEvent関数をフラグとして利用します．\nカメラの台数が増えた場合はプロセス数を増やせば対応できます．\u003c/p\u003e\n\u003ch2 id=\"事前準備\"\u003e事前準備\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e設定情報\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecam_N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# カメラ台数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eproccess_N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# カメラ1台あたりのプロセス数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eframe_info \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;nbytes\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enbytes,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;shape\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshape,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dtype\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edtype}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eフレームのサイズや型はメモリ確保，呼び出し時に必要になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e共有メモリの確保\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eshm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e shared_memory\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSharedMemory(create\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enbytes, name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e mem_name)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e共有メモリの確保はこの一文でできます．めっちゃ便利ですね．\nただし，mem_nameを動的に作成して複数の共有メモリを確保する場合，注意事項があります．\nshared_memory.SharedMemoryの戻り値を格納する変数（上記コードではshm）を設定しなかった場合や上書きした場合，それ以前の共有メモリ領域は参照できなくなります．その場合はshmを格納するリストを用意しておくとアクセス可能になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプロセスの立ち上げ\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Process(target\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e関数名, args\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e(引数,))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estart() \u003cspan style=\"color:#75715e\"\u003e# プロセス開始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ejoin() \u003cspan style=\"color:#75715e\"\u003e# プロセス終了\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e複数プロセスを立ち上げる場合は p をリストに入れておくと管理が楽です．\u003c/p\u003e\n\u003ch2 id=\"フレーム入力プロセス\"\u003eフレーム入力プロセス\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erec_cam\u003c/span\u003e(cam_id, pre_flags, frame_info):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cv2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eVideoCapture(cam_id)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            _, frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            idx \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (cam_id)\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003eproccess_N\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003ecounter\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003eproccess_N\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            mem_name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;shared\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003eidx\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e02\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            shm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e shared_memory\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSharedMemory(name\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003emem_name)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            frame_sh \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003endarray(shape\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;shape\u0026#34;\u003c/span\u003e], dtype\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dtype\u0026#34;\u003c/span\u003e], buffer\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eshm\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebuf)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            frame_sh[:] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e frame[:]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# 推論開始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            pre_flags[idx]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eset()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003emultiprocessingのSharedMemoryでは共有メモリを名前をつけて呼び出すことができます．便利！\nここでは，\u0026ldquo;shared1[番号]\u0026ldquo;という名前で共有メモリを確保している（後述）．カメラ台数✕カメラあたりのプロセス数分の共有メモリ領域が必要になるため，\nあるカメラのあるフレームに対する保存領域は，カメラ番号✕カメラ1台あたりのプロセス数＋フレーム番号%カメラ1台あたりのプロセス数で求めることができる．と同時にこの番号は推論プロセスを動作させるフラグ管理のインデックスとしても使う．\u003c/p\u003e","title":"リアルタイムの映像入力に対してFPSを落とさずに画像処理したい"},{"content":"ビデオカメラからOpenCVで映像取得（低品質） 麻雀自動採譜の実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．\nビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できるビデオキャプチャカードを導入． これでビデオカメラをWebカメラみたく使えるぞ！やった！！\nと思った矢先，\nなんか遅い気がする！！\nよくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，1秒遅延＋低FPS＋低画質という散々な結果に\nお前 Full HD で 30FPS 出るって言ってたじゃないか\u0026hellip;\u0026hellip;\nキャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．\nRTSP通信でカメラの映像を受信してみる\nこのときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった． 今回もPyAVを使えば解決するかもしれない．\nビデオカメラからPyAVで映像取得 PyAVでPCに有線接続されたカメラから映像を取得する．\nimport cv2 import av con_options = dict( video_size=\u0026#39;1920x1080\u0026#39;, vcodec=\u0026#39;mjpeg\u0026#39;, framerate=\u0026#39;30\u0026#39;, rtbufsize=\u0026#39;1\u0026#39;, ) device_name = \u0026#34;USB Video\u0026#34; con_def = dict( format=\u0026#39;dshow\u0026#39;, file=f\u0026#39;video={device_name}\u0026#39;, options=dict(con_options, video_device_number=\u0026#39;0\u0026#39;) ) container = av.open(**con_def) for frame in container.decode(video=0): frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) # cv2.imshow(\u0026#34;test_window\u0026#34;, frame) # cv2.waitKey(1) ちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．\nffmpeg -list_devices true -f dshow -i dummy PyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．\nこれで遅延ほぼなし，30FPSでの映像受信ができた．\n","permalink":"http://localhost:1313/blog/posts/20231225_getframebypyav/","summary":"\u003ch2 id=\"ビデオカメラからopencvで映像取得低品質\"\u003eビデオカメラからOpenCVで映像取得（低品質）\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003eの実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．\u003c/p\u003e\n\u003cp\u003eビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できる\u003ca href=\"https://amzn.asia/d/jclJASJ\"\u003eビデオキャプチャカード\u003c/a\u003eを導入．\nこれでビデオカメラをWebカメラみたく使えるぞ！やった！！\u003c/p\u003e\n\u003cp\u003eと思った矢先，\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eなんか遅い気がする！！\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eよくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，\u003cstrong\u003e1秒遅延＋低FPS＋低画質\u003c/strong\u003eという散々な結果に\u003c/p\u003e\n\u003cp\u003eお前 Full HD で 30FPS 出るって言ってたじゃないか\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003eキャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/blog/2023_12_01_getframertsp/\"\u003eRTSP通信でカメラの映像を受信してみる\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこのときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった．\n今回も\u003ca href=\"https://pyav.org/docs/develop/index.html\" target=\"_blank\"\u003ePyAV\u003c/a\u003eを使えば解決するかもしれない．\u003c/p\u003e\n\u003ch2 id=\"ビデオカメラからpyavで映像取得\"\u003eビデオカメラからPyAVで映像取得\u003c/h2\u003e\n\u003cp\u003ePyAVでPCに有線接続されたカメラから映像を取得する．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e cv2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e av\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econ_options  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  dict(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    video_size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1920x1080\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    vcodec\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;mjpeg\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    framerate\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;30\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    rtbufsize\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edevice_name  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;USB Video\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econ_def  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  dict(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;dshow\u0026#39;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    file\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;video=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003edevice_name\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    options\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003edict(con_options, video_device_number\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;0\u0026#39;\u003c/span\u003e) )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econtainer \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e av\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eopen(\u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003econ_def)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e frame \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e container\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edecode(video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto_ndarray(format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;bgr24\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# cv2.imshow(\u0026#34;test_window\u0026#34;, frame)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# cv2.waitKey(1)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-console\" data-lang=\"console\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003effmpeg -list_devices true -f dshow -i dummy\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．\u003c/p\u003e","title":"映像入力の遅延をPyAVで解決してみる"},{"content":" multiprocessing × YOLOv8の詳細解説 Denoを利用した基本的なWebサイト作成 Deno Deploy設定 ","permalink":"http://localhost:1313/blog/posts/20231212_scheduled_update/","summary":"\u003cul\u003e\n\u003cli\u003emultiprocessing × YOLOv8の詳細解説\u003c/li\u003e\n\u003cli\u003eDenoを利用した基本的なWebサイト作成\u003c/li\u003e\n\u003cli\u003eDeno Deploy設定\u003c/li\u003e\n\u003c/ul\u003e","title":"今後の更新予定"},{"content":"RTSPでフレーム取得（OpenCV） RTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．\nimport cv2 url = \u0026#34;rtsp://username:password@ipaddress\u0026#34; # Tapo C200は末尾に画質を指定 url = \u0026#34;rtsp://username:password@ipaddress/stream1\u0026#34; cap = cv2.VideoCapture(url) while True: ret, frame = cap.read() pass ただこれだとTapo C200では1秒程度遅延が発生してしまう． 麻雀自動採譜 の実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．\nRTSPでフレーム取得（PyAV） RTSP映像の受信にはFFmpegのPythonバインディングである PyAV を利用した方法もあり、以下で映像受信が可能でした．\nimport av\rurl = \u0026#34;rtsp://username:password@ipaddress\u0026#34;\rcontainer = av.open(url)\rfor frame in container.decode(video=0):\rframe = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;)\rpass PyAVを利用した場合は遅延が大幅に減少した．やったね．\nRTSPで一定時間毎にフレーム取得（PyAV） 麻雀自動採譜 牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚でtime.sleep(n)を使用したところ以下のエラーが複数出現．\nmax delay reached. need to consume packet\rRTP: missed 6066 packets\rRTP: PT=60: bad cseq a657 expected=8ea5\rmax delay reached. need to consume packet sleepで無理に処理を中断させたため接続が不安定になったのかも． 通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．\nimport av interval = 10 # 10秒ごとに撮影 url = \u0026#34;rtsp://username:password@ipaddress\u0026#34; container = av.open(url) counter = 0 for frame in container.decode(video=0): if frame is not None and frame.time//interval\u0026gt;counter: counter+=1 frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) cv2.imwrite(f\u0026#39;save_{counter:04}.png\u0026#39;, frame) PyAVのcontainerから呼び出したフレームは撮影開始を0としたときの撮影時刻をtimeで取得できる． 撮影開始後数フレームはNoneが返ることがあるため注意． これで安定したn秒毎の撮影が実行できる．\n","permalink":"http://localhost:1313/blog/posts/20231201_getframertsp/","summary":"\u003ch2 id=\"rtspでフレーム取得opencv\"\u003eRTSPでフレーム取得（OpenCV）\u003c/h2\u003e\n\u003cp\u003eRTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e cv2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rtsp://username:password@ipaddress\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Tapo C200は末尾に画質を指定\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rtsp://username:password@ipaddress/stream1\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cv2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eVideoCapture(url)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ret, frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eただこれだとTapo C200では\u003cstrong\u003e1秒程度遅延\u003c/strong\u003eが発生してしまう．\n\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003e\nの実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．\u003c/p\u003e\n\u003ch2 id=\"rtspでフレーム取得pyav\"\u003eRTSPでフレーム取得（PyAV）\u003c/h2\u003e\n\u003cp\u003eRTSP映像の受信にはFFmpegのPythonバインディングである\n\u003ca href=\"https://pyav.org/docs/develop/index.html\" target=\"_blank\"\u003ePyAV\u003c/a\u003e\nを利用した方法もあり、以下で映像受信が可能でした．\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-python:\" data-lang=\"python:\"\u003eimport av\r\n\r\nurl = \u0026#34;rtsp://username:password@ipaddress\u0026#34;\r\ncontainer = av.open(url)\r\nfor frame in container.decode(video=0):\r\n    frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;)\r\n    pass\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ePyAVを利用した場合は遅延が大幅に減少した．やったね．\u003c/p\u003e\n\u003ch2 id=\"rtspで一定時間毎にフレーム取得pyav\"\u003eRTSPで一定時間毎にフレーム取得（PyAV）\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003e\n牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚で\u003ccode\u003etime.sleep(n)\u003c/code\u003eを使用したところ以下のエラーが複数出現．\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-raw:\" data-lang=\"raw:\"\u003emax delay reached. need to consume packet\r\nRTP: missed 6066 packets\r\nRTP: PT=60: bad cseq a657 expected=8ea5\r\nmax delay reached. need to consume packet\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003esleepで無理に処理を中断させたため接続が不安定になったのかも．\n通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．\u003c/p\u003e","title":"RTSP通信でカメラの映像を受信してみる"},{"content":"実装過程あれこれ１ これまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の Tapo C200 からフレームを受信することができるようになりました． 早速各カメラに割り当てたプロセス毎に適当に学習させておいた YOLOv8 による牌推論を実行し，結果を描画してみました．わくわく！\nはい，映像出力が不穏な感じに． Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．\nプロセス間でまったく映像出力のタイミングがあっていない フレーム間隔がまちまち 映像の乱れが酷い FPSが低い（Tapo入力時15→10弱） 特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね\u0026hellip;\u0026hellip;\nフレーム出力がばらつく理由を勝手に予想．こんな感じだろうか\n出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える． こんなときこそマルチプロセス処理の出番でしょ↓\n（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）\n既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．\n映像受信プロセス ×4（カメラの台数） 推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当） 推論結果統合プロセス ×4（カメラの台数） さあやるぞ！\n実装過程あれこれ２ ここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．\nbase64→内部UDPが一瞬頭を過るが無視して 公式ドキュメント を漁ってみます．\nドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか． Value, Array, Queue, Pipe, RawArray, Manager, shared_memory\u0026hellip;\u0026hellip;\n多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．\n試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに． フラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．\nそんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります． Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r概ね同じタイミングでフレームが描画されているように見えますね．\nFPSはいずれのカメラも15程度であり，入力時のFPSを保持したままリアルタイムで推論が実行できました． ちなみに，\nmultiprocessingではなくthreadingを使ってYOLOv8のモデルをスレッド間で共有する方法があるが，提供元である UltralyticsのMedium記事 では推奨方法， 公式ドキュメント では非推奨方法として紹介されていたため今回は見送り． 試験的に使用してるYOLOv8モデルは，現環境（GeForce RTX 3070）で学習可能な最大サイズのモデル（YOLOv8l）に対して複数の公開データセットを統合した5万枚強の牌画像を学習させたものである．統合作業時にデータセット間でラベルのズレが発生したため，検出結果に誤りがある．ラベルが混合した状態のデータセットではあるが安定した検出ができているため，データセットの整理によって更に高精度な検出の実現が期待できる． 背景部分の予期せぬ牌検出は信頼度で棄却できるため問題なし． 検証動画は双方画面録画の仕様上FPSが低下している． ソースコード 複数台のカメラのフレーム入力に対してYOLOv8による推論を並列実行するPythonプログラム ソースコード解説はこちらからどうぞ．\nimport time import msvcrt from multiprocessing import Process, Event, shared_memory import numpy as np import av import cv2 from ultralytics import YOLO # RTSPカメラ設定情報 info_list = [[\u0026#34;toncamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;nancamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;shacamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;peicamera\u0026#34;, \u0026#34;*\u0026#34;]] password = \u0026#34;*\u0026#34; # 1:1280x720, 2:640x360 mode = 1 # 推論プロセス数指定 N = 3 def rec_cam(cam_id, username, ipaddress, pre_flags, stop_flag, frame_info): url = f\u0026#34;rtsp://{username}:{password}@{ipaddress}:554/stream{mode}\u0026#34; container = av.open(url) counter = 0 start = time.time() for frame in container.decode(video=0): counter+=1 frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) # 共有メモリにフレームを書き込み idx = (cam_id)*N+counter%N mem_name = f\u0026#39;shared{idx+1:02}\u0026#39; shm = shared_memory.SharedMemory(name=mem_name) frame_sh = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) frame_sh[:] = frame[:] # 推論開始 pre_flags[idx].set() if stop_flag.is_set(): t = time.time() - start print(f\u0026#34;{username}, TIME:{round(t, 3)}, FRAME:{counter}, FPS:{round(counter/t, 3)}\u0026#34;) break container.close() cv2.destroyAllWindows() return def finish_monitor(stop_flag): while True: if msvcrt.kbhit() and msvcrt.getch() == b\u0026#39;q\u0026#39;: print(\u0026#34;Finished\u0026#34;) stop_flag.set() break return def predict_frame(pre_flag, pre_fin_flag, mem_name, stop_flag, username, frame_info): model = YOLO(\u0026#39;best.pt\u0026#39;) while True: if pre_flag.is_set(): # 推論 pre_flag.clear() shm = shared_memory.SharedMemory(name=mem_name) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # YOLOv8による推論実行 result = model(frame)[0] frame = result.plot() # 結果統合用共有メモリにのせる shm_result = shared_memory.SharedMemory(name=username) frame_new = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm_result.buf) frame_new[:]=frame[:] # 結果統合用 pre_fin_flag.set() if stop_flag.is_set(): break def view_frame(pre_fin_flag, username, stop_flag, frame_info): while True: if pre_fin_flag.is_set(): pre_fin_flag.clear() shm = shared_memory.SharedMemory(name=username) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # Full-HDだと大抵画面が埋まっちゃう # frame = cv2.resize(frame, None, None, 0.6, 0.6) cv2.imshow(username, frame) cv2.waitKey(1) if stop_flag.is_set(): break if __name__ == \u0026#39;__main__\u0026#39;: # メモリ確保情報 url = f\u0026#34;rtsp://{info_list[0][0]}:{password}@{info_list[0][1]}:554/stream{mode}\u0026#34; container = av.open(url) for i, frame in enumerate(container.decode(video=0)): frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) if frame is not None: break container.close() frame_info = {\u0026#34;nbytes\u0026#34; : frame.nbytes, \u0026#34;shape\u0026#34; : frame.shape, \u0026#34;dtype\u0026#34; : frame.dtype} # 停止プロセス stop_flag = Event() p = Process(target=finish_monitor, args=(stop_flag,)) p.start() # 表示プロセス群（カメラ台数分） view_processes = [] pre_fin_flags = [] mem_space = [] for i in range(4): pre_fin_flag = Event() pre_fin_flags.append(pre_fin_flag) username = info_list[i][0] # メモリ確保 今回は画像用 shm = shared_memory.SharedMemory(create=True, size=frame_info[\u0026#34;nbytes\u0026#34;], name = username) mem_space.append(shm) p = Process(target=view_frame, args=(pre_fin_flag, username, stop_flag, frame_info)) view_processes.append(p) p.start() # 推論プロセス群（カメラ台数×推論プロセス数） pre_processes = [] pre_flags = [] mem_names = [] for i in range(N*len(info_list)): pre_flag = Event() pre_flags.append(pre_flag) mem_name = f\u0026#39;shared{i+1:02}\u0026#39; mem_names.append(mem_name) # メモリ確保 shm = shared_memory.SharedMemory(create=True, size=frame_info[\u0026#34;nbytes\u0026#34;], name = mem_name) mem_space.append(shm) # frame_sh = np.ndarray(shape=frame_shape, dtype=frame_dtype, buffer=shm.buf) username = info_list[i//N][0] pre_fin_flag = pre_fin_flags[i//N] p = Process(target=predict_frame, args=(pre_flag, pre_fin_flag, mem_name, stop_flag, username, frame_info)) pre_processes.append(p) p.start() # 撮影プロセス群（カメラ台数） rec_processes = [] for cam_id, info in enumerate(info_list): p = Process(target=rec_cam, args=(cam_id, info[0], info[1], pre_flags, stop_flag, frame_info,)) rec_processes.append(p) p.start() p.join() for p in view_processes: p.join() for p in pre_processes: p.join() for p in rec_processes: p.join() # 共有メモリのリソース開放、削除 for mem_name in mem_names + [info[0] for info in info_list[:4]]: shm = shared_memory.SharedMemory(name = mem_name) shm.close() shm.unlink() ","permalink":"http://localhost:1313/blog/posts/20231201_multiprocessingforyolov8/","summary":"\u003ch2 id=\"実装過程あれこれ１\"\u003e実装過程あれこれ１\u003c/h2\u003e\n\u003cp\u003eこれまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の\n\u003ca href=\"https://www.tp-link.com/jp/smart-home/tapo/tapo-c200/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eTapo C200\u003c/a\u003e\nからフレームを受信することができるようになりました．\n\u003cbr\u003e\n早速各カメラに割り当てたプロセス毎に適当に学習させておいた\n\u003ca href=\"https://docs.ultralytics.com/ja/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eYOLOv8\u003c/a\u003e\nによる牌推論を実行し，結果を描画してみました．わくわく！\u003c/p\u003e\n\u003cp\u003eはい，映像出力が不穏な感じに．\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20231201_multiprocessingforyolov8/before.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20231201_multiprocessingforyolov8/before.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\u003c/p\u003e\n\u003cp\u003e卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプロセス間でまったく映像出力のタイミングがあっていない\u003c/li\u003e\n\u003cli\u003eフレーム間隔がまちまち\u003c/li\u003e\n\u003cli\u003e映像の乱れが酷い\u003c/li\u003e\n\u003cli\u003eFPSが低い（Tapo入力時15→10弱）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね\u0026hellip;\u0026hellip;\u003cbr\u003e\u003c/p\u003e\n\u003cp\u003eフレーム出力がばらつく理由を勝手に予想．こんな感じだろうか\u003c/p\u003e\n\u003cfigure\u003e\r\n    \u003cimg loading=\"lazy\" src=\"multiprocess_fig1.jpg\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える．\n\u003cbr\u003e\u003cbr\u003e\n\u003cspan style=\"font-size: 200%; color: red;\"\u003e\nこんなときこそマルチプロセス処理の出番でしょ↓\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\r\n    \u003cimg loading=\"lazy\" src=\"multiprocess_fig2.jpg\"/\u003e \r\n\u003c/figure\u003e\r\n\n（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）\u003c/p\u003e\n\u003cp\u003e既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e映像受信プロセス ×4（カメラの台数）\u003c/li\u003e\n\u003cli\u003e推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当）\u003c/li\u003e\n\u003cli\u003e推論結果統合プロセス ×4（カメラの台数）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eさあやるぞ！\u003c/p\u003e\n\u003ch2 id=\"実装過程あれこれ２\"\u003e実装過程あれこれ２\u003c/h2\u003e\n\u003cp\u003eここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．\u003cbr\u003e\nbase64→内部UDPが一瞬頭を過るが無視して\n\u003ca href=\"https://docs.python.org/ja/3.10/library/multiprocessing.html\" target=\"_blank\"\u003e公式ドキュメント\u003c/a\u003e\nを漁ってみます．\u003c/p\u003e\n\u003cp\u003eドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか．\nValue, Array, Queue, Pipe, RawArray, Manager, shared_memory\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．\u003c/p\u003e\n\u003cp\u003e試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに．\nフラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．\u003c/p\u003e\n\u003cp\u003eそんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります．\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20231201_multiprocessingforyolov8/after.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20231201_multiprocessingforyolov8/after.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\u003c/p\u003e","title":"複数カメラの入力に対しYOLOv8で並列推論する"},{"content":"ライブラリとは？ ライブラリとは，再利用可能なソフトウェア部品のことで，特定の機能をまとめたコードの再集合のことを指します．簡単に言えば，他の人が作ってくれた機能を自身のプログラムで使える便利なコード集のことです． Pythonには標準ライブラリと外部ライブラリの2種類があります．\n標準ライブラリ 標準ライブラリはPythonをインストールしたときに標準的に入っているライブラリのことです．通常のプログラム作成時に頻繁に必要となる機能をまとめたものが多くあります．例えば，\n乱数を作成する random 時刻を取得できる datetime, time ファイルシステムへのアクセスを実現する os, shutil 基礎的な数学関数 math HTMLサポート html URLを使ってインターネットにアクセスできる urllib スマートフォンで言うところのカメラ，アルバム，時計などのデフォルトアプリみたいなものです．これらはPythonが動く環境であれば勝手に入っているため，import ライブラリ名と記述すればすぐに利用することができます．助かる！！\n# サイコロの出目を出力 import random dice_roll = random.randint(1, 6) print(\u0026#34;サイコロの出目:\u0026#34;, dice_roll) 外部ライブラリ 外部ライブラリはPythonにはもともと組み込まれておらず，自身で導入するタイプのライブラリのことです．実装したいプログラムの機能に応じて適切に選択する必要があります．例えば，\n効率的な多次元配列の数値計算を実現する Numpy 画像処理の機能を提供する OpenCV グラフを簡単に描画できる Matplotlib 様々なアルゴリズムでの機械学習を可能とする scikit-learn Webアプリの開発に役立つ Django スマートフォンで言うところのゲームやチャットアプリなどのストアアプリみたいなものです．これらは自身でインストール作業をする必要がありますが，それさえ済ませば高機能な実装をいとも容易く実現できます！とても便利！！ 有志のプログラマが開発して公開してくれているため，感謝の気持ちを忘れずに使いましょう（Pythonもそうだけど）．\n# サイコロの出目を出力 import numpy as np dice_roll = np.random.randint(1, 7) print(\u0026#34;サイコロの出目:\u0026#34;, dice_roll) ライブラリを確認してみよう では，ここからは実際に手を動かしながら解説を進めます． まずはスタートメニューから Anaconda Prompt を開いてください．\nこれはコマンドプロンプトと呼ばれる画面で，ここにコマンドと呼ばれる命令文を入力することでコンピュータを操作できます． 試しに，ここにtreeと打ち込んでEnterキーを押してみてください．\n\u0026hellip;\u0026hellip;何かツリー構造の記述がたくさん出てきたかと思います．これはディレクトリ構造をツリー形式で表示するコマンドです（あんまり使わないけど）！ なかなか表示が終わらない場合はCtrl+Cキーでキャンセルしてください．\nではコマンドを使って，今使っているPythonで利用可能な外部ライブラリの一覧を見てみましょう！ Anaconda Promptにconda listと打ち込んでEnterキーを押してみてください．\nインストール済みライブラリとバージョンの一覧が表示されました．既にたくさん入っているようです！\nよくよく見てみると，数値計算ができるNumpyや表データを扱えるPandasなど，データサイエンスに役立つライブラリがたくさんあります． 実はAnacondaは科学計算のためのPython配布形式（ディストリビューション）の一つであり，既に役立つライブラリが揃えられているんです．\nこの状態，一見プログラムを書くのに便利な環境であるように見えるんですが実はそうでないケースがあります．\n実はライブラリは，機能を実現するために別のライブラリを参照するケースがあります．例えば，\n画像処理を実現するOpenCVには，画像を配列として扱うためにNumpyが必要 グラフを描画するMatplotlibには，画像描画のためにPillowが必要 機械学習を実現するscikit-learnには，高度な数値計算をするためにScipyが必要 またライブラリはバージョンによって提供される機能が変化することがあります．そのため，同じ実行環境に特定のバージョンを指定して参照ライブラリを導入しておく必要があるのです．\nこれを，ライブラリの依存関係と言います．この依存関係が原因で複数のライブラリを導入するときに問題が発生する可能性があるんです．\n例えば，ライブラリA-C及びB-Cの依存関係があるとき，AとBが共存するためにはCのバージョンが整合している必要があります．\nこうしたライブラリバージョンの試行錯誤を行う場合，先程表示させた複雑に依存関係が絡み合ったライブラリ群を扱うのはなかなか骨が折れます．\nまた，不要なライブラリは更に混乱を引き起こすので必要となる機能のための最小構成で開発環境を整える必要があります．\nそこで役立つのが，仮想環境です！！！（やっと本題）\n仮想環境 仮想環境とは，プログラムを動かすときに仮に作って利用する動作環境のことです． 簡単に言うと「依存関係の解決が面倒だから，まっさらな世界を一から創造すればいいじゃん！！」です．\nでは，実際に作ってみましょう．\n先ほど開いたAnaconda Promptにconda create -n test python=3.12と打ち込んで実行（Proceed([y]/n)?にはyを入力して処理を続行） conda activate testと打ち込んで実行 ここでは，testという名前の仮想環境を作成してその環境に切り替える，という作業をしてもらいました． 最新行の左側が(test)と表示されていれば成功です！\n先ほどまでは(base)と表示されていたはずです．Anacondaではデフォルトでbase環境があり，そこにたくさんのライブラリがありました． ではconda listで現在のtest環境のライブラリ構成を見てみましょう！\nbase環境と比べてライブラリの数がかなり少なくなったはずです．ここに表示されているライブラリはPython3.12に付随する必要最低限のライブラリです． このきれいな環境に最小限の適切なライブラリを導入することで，快適なコーディングライフを送ることができます！！\n番外編：Conda仮想環境の管理 複数の仮想環境を扱うときに便利なコマンドをまとめておきます！\n仮想環境の作成（環境名がtest，Python3.12の場合）\nconda create -n test python=3.12 仮想環境への移行\nconda activate test 仮想環境から離脱\nconda deactivate 仮想環境の削除\nconda remove -n test --all 仮想環境一覧の表示\nconda info -e ","permalink":"http://localhost:1313/blog/posts/20250506_condavenv/","summary":"\u003ch2 id=\"ライブラリとは\"\u003eライブラリとは？\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eライブラリ\u003c/strong\u003eとは，再利用可能なソフトウェア部品のことで，特定の機能をまとめたコードの再集合のことを指します．簡単に言えば，他の人が作ってくれた機能を自身のプログラムで使える便利なコード集のことです．\nPythonには\u003cstrong\u003e標準ライブラリ\u003c/strong\u003eと\u003cstrong\u003e外部ライブラリ\u003c/strong\u003eの2種類があります．\u003c/p\u003e\n\u003ch3 id=\"標準ライブラリ\"\u003e標準ライブラリ\u003c/h3\u003e\n\u003cp\u003e標準ライブラリはPythonをインストールしたときに標準的に入っているライブラリのことです．通常のプログラム作成時に頻繁に必要となる機能をまとめたものが多くあります．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e乱数を作成する random\u003c/li\u003e\n\u003cli\u003e時刻を取得できる datetime, time\u003c/li\u003e\n\u003cli\u003eファイルシステムへのアクセスを実現する os, shutil\u003c/li\u003e\n\u003cli\u003e基礎的な数学関数 math\u003c/li\u003e\n\u003cli\u003eHTMLサポート html\u003c/li\u003e\n\u003cli\u003eURLを使ってインターネットにアクセスできる urllib\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスマートフォンで言うところのカメラ，アルバム，時計などのデフォルトアプリみたいなものです．これらはPythonが動く環境であれば勝手に入っているため，\u003ccode\u003eimport ライブラリ名\u003c/code\u003eと記述すればすぐに利用することができます．助かる！！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# サイコロの出目を出力\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e random\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edice_roll \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e random\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandint(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e6\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;サイコロの出目:\u0026#34;\u003c/span\u003e, dice_roll)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"外部ライブラリ\"\u003e外部ライブラリ\u003c/h3\u003e\n\u003cp\u003e外部ライブラリはPythonにはもともと組み込まれておらず，自身で導入するタイプのライブラリのことです．実装したいプログラムの機能に応じて適切に選択する必要があります．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e効率的な多次元配列の数値計算を実現する Numpy\u003c/li\u003e\n\u003cli\u003e画像処理の機能を提供する OpenCV\u003c/li\u003e\n\u003cli\u003eグラフを簡単に描画できる Matplotlib\u003c/li\u003e\n\u003cli\u003e様々なアルゴリズムでの機械学習を可能とする scikit-learn\u003c/li\u003e\n\u003cli\u003eWebアプリの開発に役立つ Django\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eスマートフォンで言うところのゲームやチャットアプリなどのストアアプリみたいなものです．これらは自身でインストール作業をする必要がありますが，それさえ済ませば高機能な実装をいとも容易く実現できます！とても便利！！\n有志のプログラマが開発して公開してくれているため，感謝の気持ちを忘れずに使いましょう（Pythonもそうだけど）．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# サイコロの出目を出力\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#66d9ef\"\u003eas\u003c/span\u003e np\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edice_roll \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandom\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003erandint(\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;サイコロの出目:\u0026#34;\u003c/span\u003e, dice_roll)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"ライブラリを確認してみよう\"\u003eライブラリを確認してみよう\u003c/h2\u003e\n\u003cp\u003eでは，ここからは実際に手を動かしながら解説を進めます．\nまずはスタートメニューから \u003cstrong\u003eAnaconda Prompt\u003c/strong\u003e を開いてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"img_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"console_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこれは\u003cstrong\u003eコマンドプロンプト\u003c/strong\u003eと呼ばれる画面で，ここに\u003cstrong\u003eコマンド\u003c/strong\u003eと呼ばれる命令文を入力することでコンピュータを操作できます．\n試しに，ここに\u003ccode\u003etree\u003c/code\u003eと打ち込んでEnterキーを押してみてください．\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;何かツリー構造の記述がたくさん出てきたかと思います．これはディレクトリ構造をツリー形式で表示するコマンドです（あんまり使わないけど）！\nなかなか表示が終わらない場合はCtrl+Cキーでキャンセルしてください．\u003c/p\u003e\n\u003cp\u003eではコマンドを使って，今使っているPythonで利用可能な外部ライブラリの一覧を見てみましょう！\nAnaconda Promptに\u003ccode\u003econda list\u003c/code\u003eと打ち込んでEnterキーを押してみてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"modules.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eインストール済みライブラリとバージョンの一覧が表示されました．既にたくさん入っているようです！\u003c/p\u003e\n\u003cp\u003eよくよく見てみると，数値計算ができるNumpyや表データを扱えるPandasなど，データサイエンスに役立つライブラリがたくさんあります．\n実はAnacondaは科学計算のためのPython配布形式（ディストリビューション）の一つであり，既に役立つライブラリが揃えられているんです．\u003c/p\u003e\n\u003cp\u003eこの状態，一見プログラムを書くのに便利な環境であるように見えるんですが実はそうでないケースがあります．\u003c/p\u003e","title":"【入門1】Python仮想環境"},{"content":"JupyterLab+Visual Studio Codeを使ったPython実行 Visual Studio CodeにJupyter拡張機能をインストール 空のファイルを作成し，拡張子を.ipynbとする ＋コードボタンからコードブロックを作成し，Pythonプログラムを記述 右上カーネルの選択から実行したいPython環境を選択し コードブロック左側の実行ボタン▷を押下 ポップアップが出てきたらインストールをクリックしてipykernelをインストール （pip install ipykernelを当該環境で実行してインストールも出来ます） YOLOのセットアップ 今回は最新のYOLOを簡単に扱うことができるUltralyticsパッケージを利用します．\nconda create --name yolo-env python=3.11を実行して仮想環境を作成 conda activate yolo-envで仮想環境を有効化 pip install ultralyticsでUltralyticsパッケージをインストール ここまで済んだら，テストコードを動かしてみましょう！\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolo11n.pt\u0026#34;) results = model(\u0026#34;https://ultralytics.com/images/bus.jpg\u0026#34;) results[0].show() バスを背景にした画像の物体検出が表示されたはずです！\n作業ディレクトリを開くと，yolo11n.ptというファイルがダウンロードされています．これが，YOLOのモデルファイルです．これはYOLOのデフォルトの訓練済みモデルで，COCO Datasetに収録された80種類の物体を識別することが出来ます．\n参考 https://docs.ultralytics.com/ja/quickstart/#understanding-settings\n","permalink":"http://localhost:1313/blog/posts/20250506_yolojupyter/","summary":"\u003ch2 id=\"jupyterlabvisual-studio-codeを使ったpython実行\"\u003eJupyterLab+Visual Studio Codeを使ったPython実行\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eVisual Studio CodeにJupyter拡張機能をインストール\u003c/li\u003e\n\u003cli\u003e空のファイルを作成し，拡張子を\u003ccode\u003e.ipynb\u003c/code\u003eとする\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e＋コード\u003c/code\u003eボタンからコードブロックを作成し，Pythonプログラムを記述\u003c/li\u003e\n\u003cli\u003e右上\u003ccode\u003eカーネルの選択\u003c/code\u003eから実行したいPython環境を選択し\u003c/li\u003e\n\u003cli\u003eコードブロック左側の実行ボタン\u003ccode\u003e▷\u003c/code\u003eを押下\u003c/li\u003e\n\u003cli\u003eポップアップが出てきたら\u003ccode\u003eインストール\u003c/code\u003eをクリックして\u003ccode\u003eipykernel\u003c/code\u003eをインストール\n（\u003ccode\u003epip install ipykernel\u003c/code\u003eを当該環境で実行してインストールも出来ます）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"vsjupyter.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"yoloのセットアップ\"\u003eYOLOのセットアップ\u003c/h2\u003e\n\u003cp\u003e今回は最新のYOLOを簡単に扱うことができるUltralyticsパッケージを利用します．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003econda create --name yolo-env python=3.11\u003c/code\u003eを実行して仮想環境を作成\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate yolo-env\u003c/code\u003eで仮想環境を有効化\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install ultralytics\u003c/code\u003eでUltralyticsパッケージをインストール\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eここまで済んだら，テストコードを動かしてみましょう！\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e ultralytics \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e YOLO\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emodel \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e YOLO(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;yolo11n.pt\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresults \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e model(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;https://ultralytics.com/images/bus.jpg\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eresults[\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eバスを背景にした画像の物体検出が表示されたはずです！\u003c/p\u003e\n\u003cp\u003e作業ディレクトリを開くと，\u003ccode\u003eyolo11n.pt\u003c/code\u003eというファイルがダウンロードされています．これが，YOLOのモデルファイルです．これはYOLOのデフォルトの訓練済みモデルで，COCO Datasetに収録された80種類の物体を識別することが出来ます．\u003c/p\u003e\n\u003ch2 id=\"参考\"\u003e参考\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://docs.ultralytics.com/ja/quickstart/#understanding-settings\"\u003ehttps://docs.ultralytics.com/ja/quickstart/#understanding-settings\u003c/a\u003e\u003c/p\u003e","title":"【入門2】YOLO，Jupyterのセットアップ"},{"content":"アノテーションとは？ 機械学習の分類のひとつである教師あり学習では，まず入力データと出力データの組を複数用意し，それらを使って機械学習モデルを訓練します．\n入力データと出力データの組は，タスクによって様々です．例えば，\n入力：画像データ，出力：それぞれが犬or猫 入力：家の情報（間取り，立地，築年数など），出力：家の価格 入力：これまでの気温，出力：明日の気温 入力：画像データ，出力：各物体の座標 どんなタスクを前提とする場合でも，これら入力データと出力データの組を作る作業が必要となります．これがアノテーションです．\n環境構築 今回はYOLOによる画像物体認識を目標とします．そこで，アノテーションで作るべきデータは以下になります．\n入力：画像データ，出力：各物体の座標 頑張れば用意した画像の座標を数えてアノテーションできそうですが，今回はより簡単に作業を進めるためにLabelmeというツールを使います．GUIが整備されたソフトウェアですが，Pythonライブラリとして無償で提供されています！（ありがたい！！）\nAnaconda Promptを開く conda create -n annot_env python=3.12で仮想環境を作成 conda activate annot_envで仮想環境を有効化 pip install --upgrade labelmeと入力しLabelmeをインストール これでインストールは完了です．そのままannot_env環境内でlabelmeコマンドを実行してみてください！\nこの画面が表示されたら成功です！\nいざ，アノテーション 画像読み込み 今回は画像から麻雀牌を物体認識するタスクに挑戦してみます！サンプル画像は以下からダウンロード，解凍してください．\nサンプル画像 Zipファイルの解凍が済んだらLabelmeの画面左側にあるOpen Dirから，画像のディレクトリを開いてください．\nこれでアノテーションの準備は完了です！\n範囲選択，ラベル付け アノテーションの流れは以下になります．\n画像を右クリック，Create Rectangleをクリック 物体の範囲を四角形で囲んで選択，間違えたらEscキーで戻る 出てきたポップアップの上部Enter object labelにラベルを入力してOKをクリック 2と3を物体の数だけひたすら繰り返す 画像内のアノテーションが終わったら画面左部Saveから画像と同名のJSONファイルを保存 ちょっとやってみますね～\nはい，できました．ラベルは牌の種類（萬子筒子索子字牌）の略称です．\n\u0026hellip;\u0026hellip;\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\n\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\nめんどくさい\n機械学習では一般的にデータの数が大きいほど性能が上がりやすいとされています．つまり，（要件の要求レベルにもよりますが）精度を上げるためにはたくさんアノテーションする必要があるんです．\nそこで，特定のタスクの機械学習を考える場合はアノテーションに取り掛かる前に，データセットが利用可能なライセンスで公開されていないかどうか調べるところからスタートします．例えばYOLOの場合，Roboflow，VisualData，Google Open Images Datasetなどが画像データセットを公開している他，タスクに応じて論文と併せて公開されているデータセットも多数あります．\nまた今回はアノテーション入門ということで手作業での方法を紹介していますが，画像認識AIによる半自動ツールもあるので興味があれば調べてみてください．\n今回は，なんと，特別に，アノテーション済みファイルを用意してあります！\nアノテーション結果（JSON） ZIPファイルをダウンロード＆解凍したら，datasetsという名前のディレクトリを作成し，先程の画像ファイルとJSONファイルを以下の構造で配置してください．（image_1.jsonは自作のファイルで構いません！）\n.\r└── datasets/\r├── image_1.jpg\r├── image_1.json\r├── image_2.jpg\r├── image_2.json\r├── 〜省略〜\r├── image_10.jpg\r└── image_10.json データセットフォーマットの変換 では，学習させるデータの中身を一度見てみましょう！image_1.jsonを開いてみます．\nJSONはデータ記述形式のひとつで，キーと値を並べて書くのが特徴です．Pythonの辞書型そっくりですね． ラベルや座標がファイルの中に複数記述されていることがわかります．\n少しスクロールすると，よくわからない文字列が大量に記述されています．実はこれ，Base64と呼ばれるエンコード形式で画像そのものが書きつけられているんです．これにより，このJSONファイル単体で入力データを表現することが出来ます．\nさて，ひととおり学習データの中身を見てもらいましたが，実はこのままではYOLOの学習が出来ないんです．\nYOLOに学習させるためには，画像のアノテーションを以下の形式で記述する必要があります．\n画像ファイル名.txt\nlabel_id X Y Width Height 例えば，image_1.jsonをこの形式で記述すると以下のようになります．\nimage_1.txt\n4 0.171569 0.375817 0.142157 0.183007\r6 0.286765 0.382353 0.124183 0.183007\r0 0.401144 0.398693 0.127451 0.185185\r5 0.517157 0.408497 0.117647 0.178649\r10 0.638889 0.418301 0.125817 0.189542\r7 0.767157 0.436819 0.127451 0.174292 YOLOのデータセットでは，ラベルの番号，X座標，Y座標，幅，高さを列挙して記述します．\nこれを手作業で変換するのはかなり面倒です．\nそこで今回は，Labelmeで保存したJSONファイルをYOLOのデータセット形式に変換するツール，labelme2yoloを使います．condaのannot_env環境下で以下の手順で変換します．\npip install labelme2yoloでlabelme2yoloをインストール datasetディレクトリの一つ上の階層で以下のコマンドを実行 labelme2yolo --json_dir ./datasets/ --val_size 0.10 --output_format \u0026quot;bbox\u0026quot; これが済んだら，datasetsディレクトリの中にYOLODatasetが生成されているはずです．構造は以下のようになっています．\n.\r└── YOLODatasets/\r├── images/\r│ ├── train/\r│ │ ├── image_1.jpg\r│ │ └── ～省略～\r│ └── val/\r│ └── image_7.jpg\r├── labels/\r│ ├── train/\r│ │ ├── image_1.txt\r│ │ └── ～省略～\r│ └── val/\r│ └── image_7.txt\r└── dataset.yaml 先ほどのデータセット構造より複雑になっていますね．画像とラベルが別のフォルダに格納され，更にその中で訓練用(train)と評価用(val)に分かれています．\nlabelsディレクトリのテキストファイルを見てみましょう．YOLOのフォーマットに変換され，ラベルIDや座標が記録されているはずです．\ndataset.yamlの中身を見てみましょう．\nデータセットまでの絶対パス，訓練評価用ディレクトリのパス，ラベルIDとラベル名の対応が記載されています．YOLOの学習時にはこのdataset.yamlを利用してデータセットを読み込みます．\nYOLOの学習 それでは，いよいよ作ったデータセットを使ってYOLOの学習をしてみましょう！！\nまず，テスト用の画像を用意します．以下のリンクから画像ファイルimage_11.jpgをダウンロードしてdataset.yamlと同階層に配置します．\nテスト画像image_11.jpg dataset.yamlと同階層に.ipynbファイルを作成して，以下のPythonプログラムを実行してみましょう．\nfrom ultralytics import YOLO model = YOLO(\u0026#34;yolo11n.pt\u0026#34;) model.train(data=\u0026#39;dataset.yaml\u0026#39;, epochs=300, verbose=False) results = model(\u0026#34;image_11.jpg\u0026#34;) results[0].show() CPUのみの実行環境ではかなり時間がかかるので気長に待ちましょう～\n学習が終了すると，image_11.jpgに対する推論結果が表示されます．\n未検出や誤検出がちらほらありますが，部分的に検出が成功していることがわかります．データセットサイズやエポック数を変えることで性能の改善が期待できます．\nまた，データセットディレクトリのruns内を見ると，weightディレクトリにbest.ptが保存されていることがわかります． これは今回学習したモデルファイルで，実際に物体検出をシステム開発に組み込むときに使用します．\n参考 https://github.com/wkentaro/labelme https://github.com/rooneysh/Labelme2YOLO https://docs.ultralytics.com/ja/datasets/detect/\n","permalink":"http://localhost:1313/blog/posts/20250506_yoloannotation/","summary":"\u003ch2 id=\"アノテーションとは\"\u003eアノテーションとは？\u003c/h2\u003e\n\u003cp\u003e機械学習の分類のひとつである\u003cstrong\u003e教師あり学習\u003c/strong\u003eでは，まず入力データと出力データの組を複数用意し，それらを使って機械学習モデルを訓練します．\u003c/p\u003e\n\u003cp\u003e入力データと出力データの組は，タスクによって様々です．例えば，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e入力：画像データ，出力：それぞれが犬or猫\u003c/li\u003e\n\u003cli\u003e入力：家の情報（間取り，立地，築年数など），出力：家の価格\u003c/li\u003e\n\u003cli\u003e入力：これまでの気温，出力：明日の気温\u003c/li\u003e\n\u003cli\u003e入力：画像データ，出力：各物体の座標\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eどんなタスクを前提とする場合でも，これら\u003cstrong\u003e入力データと出力データの組を作る作業\u003c/strong\u003eが必要となります．これが\u003cstrong\u003eアノテーション\u003c/strong\u003eです．\u003c/p\u003e\n\u003ch2 id=\"環境構築\"\u003e環境構築\u003c/h2\u003e\n\u003cp\u003e今回はYOLOによる画像物体認識を目標とします．そこで，アノテーションで作るべきデータは以下になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e入力：画像データ，出力：各物体の座標\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e頑張れば用意した画像の座標を数えてアノテーションできそうですが，今回はより簡単に作業を進めるために\u003cstrong\u003eLabelme\u003c/strong\u003eというツールを使います．GUIが整備されたソフトウェアですが，Pythonライブラリとして無償で提供されています！（ありがたい！！）\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAnaconda Promptを開く\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda create -n annot_env python=3.12\u003c/code\u003eで仮想環境を作成\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate annot_env\u003c/code\u003eで仮想環境を有効化\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install --upgrade labelme\u003c/code\u003eと入力しLabelmeをインストール\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eこれでインストールは完了です．そのまま\u003ccode\u003eannot_env\u003c/code\u003e環境内で\u003ccode\u003elabelme\u003c/code\u003eコマンドを実行してみてください！\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"labelme.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこの画面が表示されたら成功です！\u003c/p\u003e\n\u003ch2 id=\"いざアノテーション\"\u003eいざ，アノテーション\u003c/h2\u003e\n\u003ch3 id=\"画像読み込み\"\u003e画像読み込み\u003c/h3\u003e\n\u003cp\u003e今回は画像から麻雀牌を物体認識するタスクに挑戦してみます！サンプル画像は以下からダウンロード，解凍してください．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hakoshi-normal/yolo_sample/raw/refs/heads/main/images/images.zip\"\u003eサンプル画像\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eZipファイルの解凍が済んだらLabelmeの画面左側にある\u003ccode\u003eOpen Dir\u003c/code\u003eから，画像のディレクトリを開いてください．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"labelme_start.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eこれでアノテーションの準備は完了です！\u003c/p\u003e\n\u003ch3 id=\"範囲選択ラベル付け\"\u003e範囲選択，ラベル付け\u003c/h3\u003e\n\u003cp\u003eアノテーションの流れは以下になります．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e画像を右クリック，\u003ccode\u003eCreate Rectangle\u003c/code\u003eをクリック\u003c/li\u003e\n\u003cli\u003e物体の範囲を四角形で囲んで選択，間違えたら\u003ccode\u003eEsc\u003c/code\u003eキーで戻る\u003c/li\u003e\n\u003cli\u003e出てきたポップアップの上部\u003ccode\u003eEnter object label\u003c/code\u003eにラベルを入力して\u003ccode\u003eOK\u003c/code\u003eをクリック\u003c/li\u003e\n\u003cli\u003e2と3を物体の数だけひたすら繰り返す\u003c/li\u003e\n\u003cli\u003e画像内のアノテーションが終わったら画面左部\u003ccode\u003eSave\u003c/code\u003eから画像と同名のJSONファイルを保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eちょっとやってみますね～\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"annotated.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eはい，できました．ラベルは牌の種類（萬子筒子索子字牌）の略称です．\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 150%; color: red;\"\u003eめんどくさい\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e機械学習では一般的にデータの数が大きいほど性能が上がりやすいとされています．つまり，（要件の要求レベルにもよりますが）精度を上げるためにはたくさんアノテーションする必要があるんです．\u003c/p\u003e\n\u003cp\u003eそこで，特定のタスクの機械学習を考える場合はアノテーションに取り掛かる前に，データセットが利用可能なライセンスで公開されていないかどうか調べるところからスタートします．例えばYOLOの場合，\u003ca href=\"https://universe.roboflow.com/\"\u003eRoboflow\u003c/a\u003e，\u003ca href=\"https://visualdata.io/discovery\"\u003eVisualData\u003c/a\u003e，\u003ca href=\"https://storage.googleapis.com/openimages/web/index.html\"\u003eGoogle Open Images Dataset\u003c/a\u003eなどが画像データセットを公開している他，タスクに応じて論文と併せて公開されているデータセットも多数あります．\u003c/p\u003e\n\u003cp\u003eまた今回はアノテーション入門ということで手作業での方法を紹介していますが，画像認識AIによる半自動ツールもあるので興味があれば調べてみてください．\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e今回は，なんと，特別に，アノテーション済みファイルを用意してあります！\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hakoshi-normal/yolo_sample/raw/refs/heads/main/images/labels.zip\"\u003eアノテーション結果（JSON）\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eZIPファイルをダウンロード＆解凍したら，\u003ccode\u003edatasets\u003c/code\u003eという名前のディレクトリを作成し，先程の画像ファイルとJSONファイルを以下の構造で配置してください．（\u003ccode\u003eimage_1.json\u003c/code\u003eは自作のファイルで構いません！）\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-planetext\" data-lang=\"planetext\"\u003e.\r\n└── datasets/\r\n    ├── image_1.jpg\r\n    ├── image_1.json\r\n    ├── image_2.jpg\r\n    ├── image_2.json\r\n    ├── 〜省略〜\r\n    ├── image_10.jpg\r\n    └── image_10.json\n\u003c/code\u003e\u003c/pre\u003e\u003ch3 id=\"データセットフォーマットの変換\"\u003eデータセットフォーマットの変換\u003c/h3\u003e\n\u003cp\u003eでは，学習させるデータの中身を一度見てみましょう！\u003ccode\u003eimage_1.json\u003c/code\u003eを開いてみます．\u003c/p\u003e","title":"【入門3】YOLOアノテーションと学習"},{"content":"はじめに 自動採譜プロジェクトの息抜き的な記事です．\nせっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして， ところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．\nカメラ切り替えトリガー カメラの切り替えトリガーを「牌山にプレーヤーが触れたタイミング」と設定しました．\n自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．\nツモ位置の検出 まず，天カメの映像から卓上のツモ位置を特定する必要があります．\n牌山の形を見てみましょう．\n対局開始前はこんな感じ．\n対局開始直後はこんな感じ．\n対局中はこんな感じ．\n対局開始前は4つ牌山がありますが，対局開始以降は牌山が必ず3つ以下になります王牌の扱い上，4つのときもあります．\n牌山が3つのとき\n牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．\n牌山が4つのとき\n一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．\n実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．\n牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．\n実装概要 元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます． cv2.inRange()を使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．\n二値化した画像に対し，cv2.findContours()を使用し，牌山となりうる矩形を検出します．\n手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます． その後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．\nツモプレイヤーの検出 ツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します． MediaPipe のハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します． 今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．\nプレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．\nオートスイッチャー実装！！！ OBSを使用して配信するため，カメラ切り替えには obs-websocket-py を使用しました．こちらの サンプル がそのまま動作して助かりました．\nGUI実装にはHTML/JSでの記述が可能な Eel を使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．\n実際の動作の様子がこちら．\nYour browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．\nあと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．\n","permalink":"http://localhost:1313/blog/posts/20240210_mahjongautocamswitcher/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e自動採譜プロジェクト\u003c/a\u003eの息抜き的な記事です．\u003c/p\u003e\n\u003cp\u003eせっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして，\nところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．\u003c/p\u003e\n\u003ch2 id=\"カメラ切り替えトリガー\"\u003eカメラ切り替えトリガー\u003c/h2\u003e\n\u003cp\u003eカメラの切り替えトリガーを「\u003cstrong\u003e牌山にプレーヤーが触れたタイミング\u003c/strong\u003e」と設定しました．\u003c/p\u003e\n\u003cp\u003e自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．\u003c/p\u003e\n\u003ch2 id=\"ツモ位置の検出\"\u003eツモ位置の検出\u003c/h2\u003e\n\u003cp\u003eまず，天カメの映像から卓上のツモ位置を特定する必要があります．\u003cbr\u003e牌山の形を見てみましょう．\u003c/p\u003e\n\u003cp\u003e対局開始前はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_0.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局開始直後はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局中はこんな感じ．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"t_2.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e対局開始前は4つ牌山がありますが，対局開始以降は牌山が\u003cdel\u003e必ず3つ以下になります\u003c/del\u003e王牌の扱い上，4つのときもあります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e牌山が3つのとき\u003c/p\u003e\n\u003cp\u003e牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e牌山が4つのとき\u003c/p\u003e\n\u003cp\u003e一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"fig1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．\u003c/p\u003e\n\u003cp\u003e牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．\u003c/p\u003e\n\u003ch3 id=\"実装概要\"\u003e実装概要\u003c/h3\u003e\n\u003cp\u003e元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます．\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"original_image.png\"/\u003e \r\n\u003c/figure\u003e\r\n\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecv2.inRange()\u003c/code\u003eを使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"mask_hai.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e二値化した画像に対し，\u003ccode\u003ecv2.findContours()\u003c/code\u003eを使用し，牌山となりうる矩形を検出します．\u003c/p\u003e\n\u003cp\u003e手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます．\nその後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumohai.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"ツモプレイヤーの検出\"\u003eツモプレイヤーの検出\u003c/h2\u003e\n\u003cp\u003eツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します．\n\u003ca href=\"https://developers.google.com/mediapipe/solutions/vision/hand_landmarker\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eMediaPipe\u003c/a\u003e\nのハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します．\n今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumo1.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003eプレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．\u003c/p\u003e\n\u003cfigure class=\"center\"\u003e\r\n    \u003cimg loading=\"lazy\" src=\"tsumo2.png\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003ch2 id=\"オートスイッチャー実装\"\u003eオートスイッチャー実装！！！\u003c/h2\u003e\n\u003cp\u003eOBSを使用して配信するため，カメラ切り替えには\n\u003ca href=\"https://github.com/Elektordi/obs-websocket-py/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eobs-websocket-py\u003c/a\u003e\nを使用しました．こちらの\n\u003ca href=\"https://github.com/Elektordi/obs-websocket-py/blob/master/samples/switch_scenes.py\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eサンプル\u003c/a\u003e\nがそのまま動作して助かりました．\u003c/p\u003e\n\u003cp\u003eGUI実装にはHTML/JSでの記述が可能な\n\u003ca href=\"https://github.com/python-eel/Eel\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eEel\u003c/a\u003e\nを使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．\u003c/p\u003e\n\u003cp\u003e実際の動作の様子がこちら．\u003c/p\u003e\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\n\u003cp\u003e現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．\u003c/p\u003e\n\u003cp\u003eあと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．\u003c/p\u003e","title":"麻雀放送対局用のオートスイッチャーを作りたい"},{"content":"はじめに 複数カメラの入力に対しYOLOv8で並列推論するのソースコード解説です． 解説のため，一部表現を変えている部分があります．\n大まかな流れは以下の通り．\n共有メモリ確保 フレーム入力プロセス カメラからフレームを取得 フレームを共有メモリ1に保存 推論プロセス 共有フレームを呼び出し 推論 推論結果を描画したフレームを共有メモリ2に保存 フレーム表示プロセス 共有フレームを呼び出し フレームの画面描画 プロセス間の遷移はmultiprocessingモジュールのEvent関数をフラグとして利用します． カメラの台数が増えた場合はプロセス数を増やせば対応できます．\n事前準備 設定情報 cam_N = 4 # カメラ台数 proccess_N = 3 # カメラ1台あたりのプロセス数 frame_info = {\u0026#34;nbytes\u0026#34; : frame.nbytes, \u0026#34;shape\u0026#34; : frame.shape, \u0026#34;dtype\u0026#34; : frame.dtype} フレームのサイズや型はメモリ確保，呼び出し時に必要になります．\n共有メモリの確保 shm = shared_memory.SharedMemory(create=True, size=frame.nbytes, name = mem_name) 共有メモリの確保はこの一文でできます．めっちゃ便利ですね． ただし，mem_nameを動的に作成して複数の共有メモリを確保する場合，注意事項があります． shared_memory.SharedMemoryの戻り値を格納する変数（上記コードではshm）を設定しなかった場合や上書きした場合，それ以前の共有メモリ領域は参照できなくなります．その場合はshmを格納するリストを用意しておくとアクセス可能になります．\nプロセスの立ち上げ p = Process(target=関数名, args=(引数,)) p.start() # プロセス開始 p.join() # プロセス終了 複数プロセスを立ち上げる場合は p をリストに入れておくと管理が楽です．\nフレーム入力プロセス def rec_cam(cam_id, pre_flags, frame_info): cap = cv2.VideoCapture(cam_id) while True: _, frame = cap.read() idx = (cam_id)*proccess_N+counter%proccess_N mem_name = f\u0026#39;shared{idx+1:02}\u0026#39; shm = shared_memory.SharedMemory(name=mem_name) frame_sh = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) frame_sh[:] = frame[:] # 推論開始 pre_flags[idx].set() cap.close() multiprocessingのSharedMemoryでは共有メモリを名前をつけて呼び出すことができます．便利！ ここでは，\u0026ldquo;shared1[番号]\u0026ldquo;という名前で共有メモリを確保している（後述）．カメラ台数✕カメラあたりのプロセス数分の共有メモリ領域が必要になるため， あるカメラのあるフレームに対する保存領域は，カメラ番号✕カメラ1台あたりのプロセス数＋フレーム番号%カメラ1台あたりのプロセス数で求めることができる．と同時にこの番号は推論プロセスを動作させるフラグ管理のインデックスとしても使う．\n推論プロセス def predict_frame(pre_flag, view_flag, mem_name, cam_id, frame_info): while True: if pre_flag.is_set(): # 推論開始 pre_flag.clear() shm = shared_memory.SharedMemory(name=mem_name) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # 重い推論処理がここに入る（frameに結果を反映する） # 結果統合用共有メモリにのせる shm_result = shared_memory.SharedMemory(name=f\u0026#39;shared2{cam_id}\u0026#39;) frame_new = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm_result.buf) frame_new[:]=frame[:] # 結果表示 view_flag.set() カメラ台数✕カメラあたりのプロセス数分の推論プロセスを立ち上げます． 推論開始用のフラグや共有メモリの呼び出し名は予め与えた状態でプロセスを立ち上げる．フレーム入力プロセスでの推論開始用フラグの有効化を待ち受けています． 処理済みのフレーを格納する共有メモリ領域名は\u0026rsquo;shared2[番号]\u0026lsquo;としており，カメラの台数分事前に確保してあります． 検出結果を描画する必要がない場合は，フレーム表示プロセスを介する必要がないため，別プロセスに推論結果を渡したり，UDP通信などで端末間の結果送信が実装できると思います．\nフレーム表示プロセス def view_frame(view_flag, cam_id, frame_info): while True: if view_flag.is_set(): view_flag.clear() shm = shared_memory.SharedMemory(name=f\u0026#39;shared2{cam_id}\u0026#39;) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) cv2.imshow(f\u0026#39;window_{cam_id}\u0026#39;, frame) cv2.waitKey(1) 推論プロセスでのフレーム表示用フラグの有効化を待ち受けています．\nおわりに 今回は，以前実装した並列画像処理プログラムの解説でした．コードブロックを断片的に示しただけなので，具体的な実装コードを知りたい方はソースコードから参照できます．\n","permalink":"http://localhost:1313/blog/posts/20231226_multiprocessingforrealtimevideo/","summary":"\u003ch2 id=\"はじめに\"\u003eはじめに\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/2023_12_01_multiprocessingforyolov8/\" target=\"_blank\"\u003e複数カメラの入力に対しYOLOv8で並列推論する\u003c/a\u003eのソースコード解説です．\n解説のため，一部表現を変えている部分があります．\u003c/p\u003e\n\u003cp\u003e大まかな流れは以下の通り．\u003c/p\u003e\n\u003col start=\"0\"\u003e\n\u003cli\u003e共有メモリ確保\u003c/li\u003e\n\u003cli\u003eフレーム入力プロセス\n\u003col\u003e\n\u003cli\u003eカメラからフレームを取得\u003c/li\u003e\n\u003cli\u003eフレームを共有メモリ1に保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e推論プロセス\n\u003col\u003e\n\u003cli\u003e共有フレームを呼び出し\u003c/li\u003e\n\u003cli\u003e推論\u003c/li\u003e\n\u003cli\u003e推論結果を描画したフレームを共有メモリ2に保存\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eフレーム表示プロセス\n\u003col\u003e\n\u003cli\u003e共有フレームを呼び出し\u003c/li\u003e\n\u003cli\u003eフレームの画面描画\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eプロセス間の遷移はmultiprocessingモジュールのEvent関数をフラグとして利用します．\nカメラの台数が増えた場合はプロセス数を増やせば対応できます．\u003c/p\u003e\n\u003ch2 id=\"事前準備\"\u003e事前準備\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e設定情報\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecam_N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e4\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# カメラ台数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eproccess_N \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# カメラ1台あたりのプロセス数\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eframe_info \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e {\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;nbytes\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enbytes,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;shape\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshape,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dtype\u0026#34;\u003c/span\u003e : frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edtype}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eフレームのサイズや型はメモリ確保，呼び出し時に必要になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e共有メモリの確保\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eshm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e shared_memory\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSharedMemory(create\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e, size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enbytes, name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e mem_name)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e共有メモリの確保はこの一文でできます．めっちゃ便利ですね．\nただし，mem_nameを動的に作成して複数の共有メモリを確保する場合，注意事項があります．\nshared_memory.SharedMemoryの戻り値を格納する変数（上記コードではshm）を設定しなかった場合や上書きした場合，それ以前の共有メモリ領域は参照できなくなります．その場合はshmを格納するリストを用意しておくとアクセス可能になります．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプロセスの立ち上げ\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e Process(target\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e関数名, args\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e(引数,))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estart() \u003cspan style=\"color:#75715e\"\u003e# プロセス開始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ep\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ejoin() \u003cspan style=\"color:#75715e\"\u003e# プロセス終了\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e複数プロセスを立ち上げる場合は p をリストに入れておくと管理が楽です．\u003c/p\u003e\n\u003ch2 id=\"フレーム入力プロセス\"\u003eフレーム入力プロセス\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003erec_cam\u003c/span\u003e(cam_id, pre_flags, frame_info):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cv2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eVideoCapture(cam_id)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            _, frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            idx \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (cam_id)\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003eproccess_N\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003ecounter\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003eproccess_N\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            mem_name \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;shared\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003eidx\u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e02\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            shm \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e shared_memory\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eSharedMemory(name\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003emem_name)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            frame_sh \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003endarray(shape\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;shape\u0026#34;\u003c/span\u003e], dtype\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eframe_info[\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;dtype\u0026#34;\u003c/span\u003e], buffer\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003eshm\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ebuf)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            frame_sh[:] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e frame[:]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#75715e\"\u003e# 推論開始\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            pre_flags[idx]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eset()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eclose()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003emultiprocessingのSharedMemoryでは共有メモリを名前をつけて呼び出すことができます．便利！\nここでは，\u0026ldquo;shared1[番号]\u0026ldquo;という名前で共有メモリを確保している（後述）．カメラ台数✕カメラあたりのプロセス数分の共有メモリ領域が必要になるため，\nあるカメラのあるフレームに対する保存領域は，カメラ番号✕カメラ1台あたりのプロセス数＋フレーム番号%カメラ1台あたりのプロセス数で求めることができる．と同時にこの番号は推論プロセスを動作させるフラグ管理のインデックスとしても使う．\u003c/p\u003e","title":"リアルタイムの映像入力に対してFPSを落とさずに画像処理したい"},{"content":"ビデオカメラからOpenCVで映像取得（低品質） 麻雀自動採譜の実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．\nビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できるビデオキャプチャカードを導入． これでビデオカメラをWebカメラみたく使えるぞ！やった！！\nと思った矢先，\nなんか遅い気がする！！\nよくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，1秒遅延＋低FPS＋低画質という散々な結果に\nお前 Full HD で 30FPS 出るって言ってたじゃないか\u0026hellip;\u0026hellip;\nキャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．\nRTSP通信でカメラの映像を受信してみる\nこのときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった． 今回もPyAVを使えば解決するかもしれない．\nビデオカメラからPyAVで映像取得 PyAVでPCに有線接続されたカメラから映像を取得する．\nimport cv2 import av con_options = dict( video_size=\u0026#39;1920x1080\u0026#39;, vcodec=\u0026#39;mjpeg\u0026#39;, framerate=\u0026#39;30\u0026#39;, rtbufsize=\u0026#39;1\u0026#39;, ) device_name = \u0026#34;USB Video\u0026#34; con_def = dict( format=\u0026#39;dshow\u0026#39;, file=f\u0026#39;video={device_name}\u0026#39;, options=dict(con_options, video_device_number=\u0026#39;0\u0026#39;) ) container = av.open(**con_def) for frame in container.decode(video=0): frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) # cv2.imshow(\u0026#34;test_window\u0026#34;, frame) # cv2.waitKey(1) ちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．\nffmpeg -list_devices true -f dshow -i dummy PyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．\nこれで遅延ほぼなし，30FPSでの映像受信ができた．\n","permalink":"http://localhost:1313/blog/posts/20231225_getframebypyav/","summary":"\u003ch2 id=\"ビデオカメラからopencvで映像取得低品質\"\u003eビデオカメラからOpenCVで映像取得（低品質）\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003eの実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．\u003c/p\u003e\n\u003cp\u003eビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できる\u003ca href=\"https://amzn.asia/d/jclJASJ\"\u003eビデオキャプチャカード\u003c/a\u003eを導入．\nこれでビデオカメラをWebカメラみたく使えるぞ！やった！！\u003c/p\u003e\n\u003cp\u003eと思った矢先，\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eなんか遅い気がする！！\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eよくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，\u003cstrong\u003e1秒遅延＋低FPS＋低画質\u003c/strong\u003eという散々な結果に\u003c/p\u003e\n\u003cp\u003eお前 Full HD で 30FPS 出るって言ってたじゃないか\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003eキャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/blog/2023_12_01_getframertsp/\"\u003eRTSP通信でカメラの映像を受信してみる\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eこのときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった．\n今回も\u003ca href=\"https://pyav.org/docs/develop/index.html\" target=\"_blank\"\u003ePyAV\u003c/a\u003eを使えば解決するかもしれない．\u003c/p\u003e\n\u003ch2 id=\"ビデオカメラからpyavで映像取得\"\u003eビデオカメラからPyAVで映像取得\u003c/h2\u003e\n\u003cp\u003ePyAVでPCに有線接続されたカメラから映像を取得する．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e cv2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e av\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econ_options  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  dict(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    video_size\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1920x1080\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    vcodec\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;mjpeg\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    framerate\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;30\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    rtbufsize\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;1\u0026#39;\u003c/span\u003e,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edevice_name  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;USB Video\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econ_def  \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e  dict(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;dshow\u0026#39;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    file\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;video=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e{\u003c/span\u003edevice_name\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;\u003c/span\u003e, \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    options\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003edict(con_options, video_device_number\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;0\u0026#39;\u003c/span\u003e) )\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003econtainer \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e av\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eopen(\u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003econ_def)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e frame \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e container\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edecode(video\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e frame\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto_ndarray(format\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;bgr24\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# cv2.imshow(\u0026#34;test_window\u0026#34;, frame)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e# cv2.waitKey(1)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-console\" data-lang=\"console\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003effmpeg -list_devices true -f dshow -i dummy\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．\u003c/p\u003e","title":"映像入力の遅延をPyAVで解決してみる"},{"content":" multiprocessing × YOLOv8の詳細解説 Denoを利用した基本的なWebサイト作成 Deno Deploy設定 ","permalink":"http://localhost:1313/blog/posts/20231212_scheduled_update/","summary":"\u003cul\u003e\n\u003cli\u003emultiprocessing × YOLOv8の詳細解説\u003c/li\u003e\n\u003cli\u003eDenoを利用した基本的なWebサイト作成\u003c/li\u003e\n\u003cli\u003eDeno Deploy設定\u003c/li\u003e\n\u003c/ul\u003e","title":"今後の更新予定"},{"content":"RTSPでフレーム取得（OpenCV） RTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．\nimport cv2 url = \u0026#34;rtsp://username:password@ipaddress\u0026#34; # Tapo C200は末尾に画質を指定 url = \u0026#34;rtsp://username:password@ipaddress/stream1\u0026#34; cap = cv2.VideoCapture(url) while True: ret, frame = cap.read() pass ただこれだとTapo C200では1秒程度遅延が発生してしまう． 麻雀自動採譜 の実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．\nRTSPでフレーム取得（PyAV） RTSP映像の受信にはFFmpegのPythonバインディングである PyAV を利用した方法もあり、以下で映像受信が可能でした．\nimport av\rurl = \u0026#34;rtsp://username:password@ipaddress\u0026#34;\rcontainer = av.open(url)\rfor frame in container.decode(video=0):\rframe = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;)\rpass PyAVを利用した場合は遅延が大幅に減少した．やったね．\nRTSPで一定時間毎にフレーム取得（PyAV） 麻雀自動採譜 牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚でtime.sleep(n)を使用したところ以下のエラーが複数出現．\nmax delay reached. need to consume packet\rRTP: missed 6066 packets\rRTP: PT=60: bad cseq a657 expected=8ea5\rmax delay reached. need to consume packet sleepで無理に処理を中断させたため接続が不安定になったのかも． 通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．\nimport av interval = 10 # 10秒ごとに撮影 url = \u0026#34;rtsp://username:password@ipaddress\u0026#34; container = av.open(url) counter = 0 for frame in container.decode(video=0): if frame is not None and frame.time//interval\u0026gt;counter: counter+=1 frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) cv2.imwrite(f\u0026#39;save_{counter:04}.png\u0026#39;, frame) PyAVのcontainerから呼び出したフレームは撮影開始を0としたときの撮影時刻をtimeで取得できる． 撮影開始後数フレームはNoneが返ることがあるため注意． これで安定したn秒毎の撮影が実行できる．\n","permalink":"http://localhost:1313/blog/posts/20231201_getframertsp/","summary":"\u003ch2 id=\"rtspでフレーム取得opencv\"\u003eRTSPでフレーム取得（OpenCV）\u003c/h2\u003e\n\u003cp\u003eRTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e cv2\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rtsp://username:password@ipaddress\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Tapo C200は末尾に画質を指定\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eurl \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;rtsp://username:password@ipaddress/stream1\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecap \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cv2\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eVideoCapture(url)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003ewhile\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    ret, frame \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e cap\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eread()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003epass\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eただこれだとTapo C200では\u003cstrong\u003e1秒程度遅延\u003c/strong\u003eが発生してしまう．\n\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003e\nの実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．\u003c/p\u003e\n\u003ch2 id=\"rtspでフレーム取得pyav\"\u003eRTSPでフレーム取得（PyAV）\u003c/h2\u003e\n\u003cp\u003eRTSP映像の受信にはFFmpegのPythonバインディングである\n\u003ca href=\"https://pyav.org/docs/develop/index.html\" target=\"_blank\"\u003ePyAV\u003c/a\u003e\nを利用した方法もあり、以下で映像受信が可能でした．\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-python:\" data-lang=\"python:\"\u003eimport av\r\n\r\nurl = \u0026#34;rtsp://username:password@ipaddress\u0026#34;\r\ncontainer = av.open(url)\r\nfor frame in container.decode(video=0):\r\n    frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;)\r\n    pass\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003ePyAVを利用した場合は遅延が大幅に減少した．やったね．\u003c/p\u003e\n\u003ch2 id=\"rtspで一定時間毎にフレーム取得pyav\"\u003eRTSPで一定時間毎にフレーム取得（PyAV）\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"/blog/tags/麻雀自動採譜/\" target=\"_blank\"\u003e麻雀自動採譜\u003c/a\u003e\n牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚で\u003ccode\u003etime.sleep(n)\u003c/code\u003eを使用したところ以下のエラーが複数出現．\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-raw:\" data-lang=\"raw:\"\u003emax delay reached. need to consume packet\r\nRTP: missed 6066 packets\r\nRTP: PT=60: bad cseq a657 expected=8ea5\r\nmax delay reached. need to consume packet\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003esleepで無理に処理を中断させたため接続が不安定になったのかも．\n通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．\u003c/p\u003e","title":"RTSP通信でカメラの映像を受信してみる"},{"content":"実装過程あれこれ１ これまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の Tapo C200 からフレームを受信することができるようになりました． 早速各カメラに割り当てたプロセス毎に適当に学習させておいた YOLOv8 による牌推論を実行し，結果を描画してみました．わくわく！\nはい，映像出力が不穏な感じに． Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．\nプロセス間でまったく映像出力のタイミングがあっていない フレーム間隔がまちまち 映像の乱れが酷い FPSが低い（Tapo入力時15→10弱） 特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね\u0026hellip;\u0026hellip;\nフレーム出力がばらつく理由を勝手に予想．こんな感じだろうか\n出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える． こんなときこそマルチプロセス処理の出番でしょ↓\n（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）\n既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．\n映像受信プロセス ×4（カメラの台数） 推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当） 推論結果統合プロセス ×4（カメラの台数） さあやるぞ！\n実装過程あれこれ２ ここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．\nbase64→内部UDPが一瞬頭を過るが無視して 公式ドキュメント を漁ってみます．\nドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか． Value, Array, Queue, Pipe, RawArray, Manager, shared_memory\u0026hellip;\u0026hellip;\n多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．\n試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに． フラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．\nそんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります． Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player!\r概ね同じタイミングでフレームが描画されているように見えますね．\nFPSはいずれのカメラも15程度であり，入力時のFPSを保持したままリアルタイムで推論が実行できました． ちなみに，\nmultiprocessingではなくthreadingを使ってYOLOv8のモデルをスレッド間で共有する方法があるが，提供元である UltralyticsのMedium記事 では推奨方法， 公式ドキュメント では非推奨方法として紹介されていたため今回は見送り． 試験的に使用してるYOLOv8モデルは，現環境（GeForce RTX 3070）で学習可能な最大サイズのモデル（YOLOv8l）に対して複数の公開データセットを統合した5万枚強の牌画像を学習させたものである．統合作業時にデータセット間でラベルのズレが発生したため，検出結果に誤りがある．ラベルが混合した状態のデータセットではあるが安定した検出ができているため，データセットの整理によって更に高精度な検出の実現が期待できる． 背景部分の予期せぬ牌検出は信頼度で棄却できるため問題なし． 検証動画は双方画面録画の仕様上FPSが低下している． ソースコード 複数台のカメラのフレーム入力に対してYOLOv8による推論を並列実行するPythonプログラム ソースコード解説はこちらからどうぞ．\nimport time import msvcrt from multiprocessing import Process, Event, shared_memory import numpy as np import av import cv2 from ultralytics import YOLO # RTSPカメラ設定情報 info_list = [[\u0026#34;toncamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;nancamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;shacamera\u0026#34;, \u0026#34;*\u0026#34;], [\u0026#34;peicamera\u0026#34;, \u0026#34;*\u0026#34;]] password = \u0026#34;*\u0026#34; # 1:1280x720, 2:640x360 mode = 1 # 推論プロセス数指定 N = 3 def rec_cam(cam_id, username, ipaddress, pre_flags, stop_flag, frame_info): url = f\u0026#34;rtsp://{username}:{password}@{ipaddress}:554/stream{mode}\u0026#34; container = av.open(url) counter = 0 start = time.time() for frame in container.decode(video=0): counter+=1 frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) # 共有メモリにフレームを書き込み idx = (cam_id)*N+counter%N mem_name = f\u0026#39;shared{idx+1:02}\u0026#39; shm = shared_memory.SharedMemory(name=mem_name) frame_sh = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) frame_sh[:] = frame[:] # 推論開始 pre_flags[idx].set() if stop_flag.is_set(): t = time.time() - start print(f\u0026#34;{username}, TIME:{round(t, 3)}, FRAME:{counter}, FPS:{round(counter/t, 3)}\u0026#34;) break container.close() cv2.destroyAllWindows() return def finish_monitor(stop_flag): while True: if msvcrt.kbhit() and msvcrt.getch() == b\u0026#39;q\u0026#39;: print(\u0026#34;Finished\u0026#34;) stop_flag.set() break return def predict_frame(pre_flag, pre_fin_flag, mem_name, stop_flag, username, frame_info): model = YOLO(\u0026#39;best.pt\u0026#39;) while True: if pre_flag.is_set(): # 推論 pre_flag.clear() shm = shared_memory.SharedMemory(name=mem_name) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # YOLOv8による推論実行 result = model(frame)[0] frame = result.plot() # 結果統合用共有メモリにのせる shm_result = shared_memory.SharedMemory(name=username) frame_new = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm_result.buf) frame_new[:]=frame[:] # 結果統合用 pre_fin_flag.set() if stop_flag.is_set(): break def view_frame(pre_fin_flag, username, stop_flag, frame_info): while True: if pre_fin_flag.is_set(): pre_fin_flag.clear() shm = shared_memory.SharedMemory(name=username) frame = np.ndarray(shape=frame_info[\u0026#34;shape\u0026#34;], dtype=frame_info[\u0026#34;dtype\u0026#34;], buffer=shm.buf) # Full-HDだと大抵画面が埋まっちゃう # frame = cv2.resize(frame, None, None, 0.6, 0.6) cv2.imshow(username, frame) cv2.waitKey(1) if stop_flag.is_set(): break if __name__ == \u0026#39;__main__\u0026#39;: # メモリ確保情報 url = f\u0026#34;rtsp://{info_list[0][0]}:{password}@{info_list[0][1]}:554/stream{mode}\u0026#34; container = av.open(url) for i, frame in enumerate(container.decode(video=0)): frame = frame.to_ndarray(format=\u0026#39;bgr24\u0026#39;) if frame is not None: break container.close() frame_info = {\u0026#34;nbytes\u0026#34; : frame.nbytes, \u0026#34;shape\u0026#34; : frame.shape, \u0026#34;dtype\u0026#34; : frame.dtype} # 停止プロセス stop_flag = Event() p = Process(target=finish_monitor, args=(stop_flag,)) p.start() # 表示プロセス群（カメラ台数分） view_processes = [] pre_fin_flags = [] mem_space = [] for i in range(4): pre_fin_flag = Event() pre_fin_flags.append(pre_fin_flag) username = info_list[i][0] # メモリ確保 今回は画像用 shm = shared_memory.SharedMemory(create=True, size=frame_info[\u0026#34;nbytes\u0026#34;], name = username) mem_space.append(shm) p = Process(target=view_frame, args=(pre_fin_flag, username, stop_flag, frame_info)) view_processes.append(p) p.start() # 推論プロセス群（カメラ台数×推論プロセス数） pre_processes = [] pre_flags = [] mem_names = [] for i in range(N*len(info_list)): pre_flag = Event() pre_flags.append(pre_flag) mem_name = f\u0026#39;shared{i+1:02}\u0026#39; mem_names.append(mem_name) # メモリ確保 shm = shared_memory.SharedMemory(create=True, size=frame_info[\u0026#34;nbytes\u0026#34;], name = mem_name) mem_space.append(shm) # frame_sh = np.ndarray(shape=frame_shape, dtype=frame_dtype, buffer=shm.buf) username = info_list[i//N][0] pre_fin_flag = pre_fin_flags[i//N] p = Process(target=predict_frame, args=(pre_flag, pre_fin_flag, mem_name, stop_flag, username, frame_info)) pre_processes.append(p) p.start() # 撮影プロセス群（カメラ台数） rec_processes = [] for cam_id, info in enumerate(info_list): p = Process(target=rec_cam, args=(cam_id, info[0], info[1], pre_flags, stop_flag, frame_info,)) rec_processes.append(p) p.start() p.join() for p in view_processes: p.join() for p in pre_processes: p.join() for p in rec_processes: p.join() # 共有メモリのリソース開放、削除 for mem_name in mem_names + [info[0] for info in info_list[:4]]: shm = shared_memory.SharedMemory(name = mem_name) shm.close() shm.unlink() ","permalink":"http://localhost:1313/blog/posts/20231201_multiprocessingforyolov8/","summary":"\u003ch2 id=\"実装過程あれこれ１\"\u003e実装過程あれこれ１\u003c/h2\u003e\n\u003cp\u003eこれまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の\n\u003ca href=\"https://www.tp-link.com/jp/smart-home/tapo/tapo-c200/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eTapo C200\u003c/a\u003e\nからフレームを受信することができるようになりました．\n\u003cbr\u003e\n早速各カメラに割り当てたプロセス毎に適当に学習させておいた\n\u003ca href=\"https://docs.ultralytics.com/ja/\" target=\"_blank\" rel=\"noopener noreferrer\"\u003eYOLOv8\u003c/a\u003e\nによる牌推論を実行し，結果を描画してみました．わくわく！\u003c/p\u003e\n\u003cp\u003eはい，映像出力が不穏な感じに．\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20231201_multiprocessingforyolov8/before.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20231201_multiprocessingforyolov8/before.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\u003c/p\u003e\n\u003cp\u003e卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eプロセス間でまったく映像出力のタイミングがあっていない\u003c/li\u003e\n\u003cli\u003eフレーム間隔がまちまち\u003c/li\u003e\n\u003cli\u003e映像の乱れが酷い\u003c/li\u003e\n\u003cli\u003eFPSが低い（Tapo入力時15→10弱）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね\u0026hellip;\u0026hellip;\u003cbr\u003e\u003c/p\u003e\n\u003cp\u003eフレーム出力がばらつく理由を勝手に予想．こんな感じだろうか\u003c/p\u003e\n\u003cfigure\u003e\r\n    \u003cimg loading=\"lazy\" src=\"multiprocess_fig1.jpg\"/\u003e \r\n\u003c/figure\u003e\r\n\n\u003cp\u003e出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える．\n\u003cbr\u003e\u003cbr\u003e\n\u003cspan style=\"font-size: 200%; color: red;\"\u003e\nこんなときこそマルチプロセス処理の出番でしょ↓\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\r\n    \u003cimg loading=\"lazy\" src=\"multiprocess_fig2.jpg\"/\u003e \r\n\u003c/figure\u003e\r\n\n（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）\u003c/p\u003e\n\u003cp\u003e既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e映像受信プロセス ×4（カメラの台数）\u003c/li\u003e\n\u003cli\u003e推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当）\u003c/li\u003e\n\u003cli\u003e推論結果統合プロセス ×4（カメラの台数）\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eさあやるぞ！\u003c/p\u003e\n\u003ch2 id=\"実装過程あれこれ２\"\u003e実装過程あれこれ２\u003c/h2\u003e\n\u003cp\u003eここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．\u003cbr\u003e\nbase64→内部UDPが一瞬頭を過るが無視して\n\u003ca href=\"https://docs.python.org/ja/3.10/library/multiprocessing.html\" target=\"_blank\"\u003e公式ドキュメント\u003c/a\u003e\nを漁ってみます．\u003c/p\u003e\n\u003cp\u003eドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか．\nValue, Array, Queue, Pipe, RawArray, Manager, shared_memory\u0026hellip;\u0026hellip;\u003c/p\u003e\n\u003cp\u003e多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．\u003c/p\u003e\n\u003cp\u003e試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに．\nフラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．\u003c/p\u003e\n\u003cp\u003eそんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります．\n\u003cvideo controls preload=\"auto\" width=\"100%\"  playsinline class=\"html-video\"\u003e\r\n    \u003csource src=\"/blog/posts/20231201_multiprocessingforyolov8/after.mp4\" type=\"video/mp4\"\u003e\r\n  \u003cspan\u003eYour browser doesn't support embedded videos, but don't worry, you can \u003ca href=\"/blog/posts/20231201_multiprocessingforyolov8/after.mp4\"\u003edownload it\u003c/a\u003e and watch it with your favorite video player!\u003c/span\u003e\r\n\u003c/video\u003e\u003c/p\u003e","title":"複数カメラの入力に対しYOLOv8で並列推論する"}]