<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>麻雀自動採譜 on 箱四のブログ</title>
    <link>http://localhost:1313/blog/tags/%E9%BA%BB%E9%9B%80%E8%87%AA%E5%8B%95%E6%8E%A1%E8%AD%9C/</link>
    <description>Recent content in 麻雀自動採譜 on 箱四のブログ</description>
    <generator>Hugo -- 0.147.1</generator>
    <language>ja-jp</language>
    <lastBuildDate>Sat, 10 Feb 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/blog/tags/%E9%BA%BB%E9%9B%80%E8%87%AA%E5%8B%95%E6%8E%A1%E8%AD%9C/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>麻雀放送対局用のオートスイッチャーを作りたい</title>
      <link>http://localhost:1313/blog/posts/20240210_mahjongautocamswitcher/</link>
      <pubDate>Sat, 10 Feb 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/posts/20240210_mahjongautocamswitcher/</guid>
      <description>&lt;h2 id=&#34;はじめに&#34;&gt;はじめに&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/blog/tags/麻雀自動採譜/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;自動採譜プロジェクト&lt;/a&gt;の息抜き的な記事です．&lt;/p&gt;
&lt;p&gt;せっかく全自動雀卓があって，手元用カメラが4台あるんだから，麻雀の配信とか出来たら嬉しいよね～とか言う話の流れになりまして，
ところが同卓者を募るので精一杯な状況で配信スタッフをおくわけにもいかない．だったらカメラの切り替えぐらいは自動化しちゃおう！！ってのがこの記事の主旨です．&lt;/p&gt;
&lt;h2 id=&#34;カメラ切り替えトリガー&#34;&gt;カメラ切り替えトリガー&lt;/h2&gt;
&lt;p&gt;カメラの切り替えトリガーを「&lt;strong&gt;牌山にプレーヤーが触れたタイミング&lt;/strong&gt;」と設定しました．&lt;/p&gt;
&lt;p&gt;自動採譜プロジェクトの実装作業も視野に入れて，手牌変化を監視する手法も無くはないです．ただし，牌検出に計算コストがかかること，カメラ切り替えが牌検出精度に依存してしまうことを考慮し，今回はよりシンプルな方法を選択しました．&lt;/p&gt;
&lt;h2 id=&#34;ツモ位置の検出&#34;&gt;ツモ位置の検出&lt;/h2&gt;
&lt;p&gt;まず，天カメの映像から卓上のツモ位置を特定する必要があります．&lt;br&gt;牌山の形を見てみましょう．&lt;/p&gt;
&lt;p&gt;対局開始前はこんな感じ．&lt;/p&gt;
&lt;figure class=&#34;center&#34;&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;t_0.png&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;対局開始直後はこんな感じ．&lt;/p&gt;
&lt;figure class=&#34;center&#34;&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;t_1.png&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;対局中はこんな感じ．&lt;/p&gt;
&lt;figure class=&#34;center&#34;&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;t_2.png&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;対局開始前は4つ牌山がありますが，対局開始以降は牌山が&lt;del&gt;必ず3つ以下になります&lt;/del&gt;王牌の扱い上，4つのときもあります．&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;牌山が3つのとき&lt;/p&gt;
&lt;p&gt;牌山の並びを時計回りに見た際に，牌山が何もない場合は隣の牌山の端点がツモ位置になります．&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;牌山が4つのとき&lt;/p&gt;
&lt;p&gt;一つの家に2つ矩形が検出された場合，それを王牌と判定し，ツモ位置を特定します．&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&#34;center&#34;&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;fig1.png&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;実装の流れとしては，麻雀牌背面色部分を天カメ映像から取得し，そこから矩形を検出しています．手牌や河などから発生する検出ミスは矩形領域の面積や画素成分などで例外処理をしています．&lt;/p&gt;
&lt;p&gt;牌山検出後，卓全体をXの字に4分割し，矩形の重心位置に基づき分類しています．&lt;/p&gt;
&lt;h3 id=&#34;実装概要&#34;&gt;実装概要&lt;/h3&gt;
&lt;p&gt;元画像です．読み込み時に卓の形状に合わせてトリミングしておきます．リアルタイムの牌山検出を行う際はノイズ軽減のため，鳴き牌表示部分と卓中央部分は予めマスク処理を行っておきます．
&lt;figure class=&#34;center&#34;&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;original_image.png&#34;/&gt; 
&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cv2.inRange()&lt;/code&gt;を使用し，画像中の麻雀牌の背面色を絞り込みます．その際，クロージング処理を行いノイズの発生を抑制します．&lt;/p&gt;
&lt;figure class=&#34;center&#34;&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;mask_hai.png&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;二値化した画像に対し，&lt;code&gt;cv2.findContours()&lt;/code&gt;を使用し，牌山となりうる矩形を検出します．&lt;/p&gt;
&lt;p&gt;手牌や鳴き牌，不意に伏せた牌が矩形として検出されてしまうため，矩形の座標，矩形の最小サイズ，矩形内の画素成分などにより牌山を絞り込みます．
その後，特定した牌山の中心位置に基づきツモ牌の位置を特定します．&lt;/p&gt;
&lt;figure class=&#34;center&#34;&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;tsumohai.png&#34;/&gt; 
&lt;/figure&gt;

&lt;h2 id=&#34;ツモプレイヤーの検出&#34;&gt;ツモプレイヤーの検出&lt;/h2&gt;
&lt;p&gt;ツモ動作をしたプレイヤーをリアルタイムで監視するため，画像中から手指検出をおこない，人差し指の位置に基づきツモ動作を検出します．
&lt;a href=&#34;https://developers.google.com/mediapipe/solutions/vision/hand_landmarker&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;MediaPipe&lt;/a&gt;
のハンドトラッキングを使って，前節のツモ位置と人差し指が近づいたタイミングをツモとして判定します．
今回は処理負荷軽減のため，手指検出をツモ牌の周辺に限定して推論を行っています．&lt;/p&gt;
&lt;figure class=&#34;center&#34;&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;tsumo1.png&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;プレイヤーの識別には，手首の座標と中指付け根の座標から成る線分の角度を求め，角度を4分割して識別しました．&lt;/p&gt;
&lt;figure class=&#34;center&#34;&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;tsumo2.png&#34;/&gt; 
&lt;/figure&gt;

&lt;h2 id=&#34;オートスイッチャー実装&#34;&gt;オートスイッチャー実装！！！&lt;/h2&gt;
&lt;p&gt;OBSを使用して配信するため，カメラ切り替えには
&lt;a href=&#34;https://github.com/Elektordi/obs-websocket-py/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;obs-websocket-py&lt;/a&gt;
を使用しました．こちらの
&lt;a href=&#34;https://github.com/Elektordi/obs-websocket-py/blob/master/samples/switch_scenes.py&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;サンプル&lt;/a&gt;
がそのまま動作して助かりました．&lt;/p&gt;
&lt;p&gt;GUI実装にはHTML/JSでの記述が可能な
&lt;a href=&#34;https://github.com/python-eel/Eel&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Eel&lt;/a&gt;
を使用しました．GUI設計ライブラリ毎の独自記法を覚える必要がないので，かなり重宝しています．&lt;/p&gt;
&lt;p&gt;実際の動作の様子がこちら．&lt;/p&gt;
&lt;video controls preload=&#34;auto&#34; width=&#34;100%&#34;  playsinline class=&#34;html-video&#34;&gt;
    &lt;source src=&#34;http://localhost:1313/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;span&gt;Your browser doesn&#39;t support embedded videos, but don&#39;t worry, you can &lt;a href=&#34;http://localhost:1313/blog/posts/20240210_mahjongautocamswitcher/cam_switcher_test.mp4&#34;&gt;download it&lt;/a&gt; and watch it with your favorite video player!&lt;/span&gt;
&lt;/video&gt;
&lt;p&gt;現時点では鳴きには対応してないので，手動切替機能も実装しています．とはいえ鳴いた牌を監視すればたぶん容易に検出可能なので，追々実装したいですね．&lt;/p&gt;
&lt;p&gt;あと，牌山をずらす行為についても実装上では判定の対象となっています．これは両手で牌山をずらした場合に例外としてカメラ切り替えを行わない設計にすればある程度は回避できる問題です．ただし，自身のツモ番に牌山をずらすのは問題ないので，とりあえず保留ですね．&lt;/p&gt;</description>
    </item>
    <item>
      <title>映像入力の遅延をPyAVで解決してみる</title>
      <link>http://localhost:1313/blog/posts/20231225_getframebypyav/</link>
      <pubDate>Mon, 25 Dec 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/posts/20231225_getframebypyav/</guid>
      <description>&lt;h2 id=&#34;ビデオカメラからopencvで映像取得低品質&#34;&gt;ビデオカメラからOpenCVで映像取得（低品質）&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/blog/tags/麻雀自動採譜/&#34; target=&#34;_blank&#34;&gt;麻雀自動採譜&lt;/a&gt;の実装において，卓上を撮影する天カメとしてFull HDで映像記録が可能な家庭用ビデオカメラを取り付けた．&lt;/p&gt;
&lt;p&gt;ビデオカメラにはHDMIの出力端子があり，HDMI to USB Cに変換できる&lt;a href=&#34;https://amzn.asia/d/jclJASJ&#34;&gt;ビデオキャプチャカード&lt;/a&gt;を導入．
これでビデオカメラをWebカメラみたく使えるぞ！やった！！&lt;/p&gt;
&lt;p&gt;と思った矢先，&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;なんか遅い気がする！！&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;よくあるOpenCVでWebカメラの映像を取得するプログラムを走らせると，&lt;strong&gt;1秒遅延＋低FPS＋低画質&lt;/strong&gt;という散々な結果に&lt;/p&gt;
&lt;p&gt;お前 Full HD で 30FPS 出るって言ってたじゃないか&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;キャプチャカードが悪いのか，もともとそんな高品質は無理なのか，いろいろ考えた挙げ句，似た症状を以前どっかで見たのを思い出しました．&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/blog/2023_12_01_getframertsp/&#34;&gt;RTSP通信でカメラの映像を受信してみる&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;このときもRTSP通信による映像入力がOpenCVでは遅延＆低FPSだった．
今回も&lt;a href=&#34;https://pyav.org/docs/develop/index.html&#34; target=&#34;_blank&#34;&gt;PyAV&lt;/a&gt;を使えば解決するかもしれない．&lt;/p&gt;
&lt;h2 id=&#34;ビデオカメラからpyavで映像取得&#34;&gt;ビデオカメラからPyAVで映像取得&lt;/h2&gt;
&lt;p&gt;PyAVでPCに有線接続されたカメラから映像を取得する．&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; cv2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; av
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;con_options  &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  dict(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    video_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;1920x1080&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    vcodec&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;mjpeg&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    framerate&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;30&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    rtbufsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;1&amp;#39;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;device_name  &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;USB Video&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;con_def  &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;  dict(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    format&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;dshow&amp;#39;&lt;/span&gt;, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    file&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;video=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;device_name&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;, 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    options&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;dict(con_options, video_device_number&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;0&amp;#39;&lt;/span&gt;) )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;container &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; av&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;con_def)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; frame &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; container&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;decode(video&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    frame &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; frame&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_ndarray(format&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bgr24&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# cv2.imshow(&amp;#34;test_window&amp;#34;, frame)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# cv2.waitKey(1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;ちなみにカメラデバイス名はffmpegをインストールしてるなら以下のコマンドで確認できる．&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ffmpeg -list_devices true -f dshow -i dummy
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;PyAVはFFmpegのPythonバインディングらしいので，デバイス名の自動取得も可能かもしれない．subprocessモジュールを使えば上のコマンドでも可能．&lt;/p&gt;</description>
    </item>
    <item>
      <title>RTSP通信でカメラの映像を受信してみる</title>
      <link>http://localhost:1313/blog/posts/20231201_getframertsp/</link>
      <pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/posts/20231201_getframertsp/</guid>
      <description>&lt;h2 id=&#34;rtspでフレーム取得opencv&#34;&gt;RTSPでフレーム取得（OpenCV）&lt;/h2&gt;
&lt;p&gt;RTSP通信でカメラの映像を受け取る際，OpenCVを使って記述できる．&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; cv2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;url &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rtsp://username:password@ipaddress&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Tapo C200は末尾に画質を指定&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;url &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rtsp://username:password@ipaddress/stream1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cap &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cv2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;VideoCapture(url)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ret, frame &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cap&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;pass&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;ただこれだとTapo C200では&lt;strong&gt;1秒程度遅延&lt;/strong&gt;が発生してしまう．
&lt;a href=&#34;http://localhost:1313/blog/tags/麻雀自動採譜/&#34; target=&#34;_blank&#34;&gt;麻雀自動採譜&lt;/a&gt;
の実装上，天井カメラを含めたカメラ間での同期が必須なんですよね．&lt;/p&gt;
&lt;h2 id=&#34;rtspでフレーム取得pyav&#34;&gt;RTSPでフレーム取得（PyAV）&lt;/h2&gt;
&lt;p&gt;RTSP映像の受信にはFFmpegのPythonバインディングである
&lt;a href=&#34;https://pyav.org/docs/develop/index.html&#34; target=&#34;_blank&#34;&gt;PyAV&lt;/a&gt;
を利用した方法もあり、以下で映像受信が可能でした．&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-python:&#34; data-lang=&#34;python:&#34;&gt;import av

url = &amp;#34;rtsp://username:password@ipaddress&amp;#34;
container = av.open(url)
for frame in container.decode(video=0):
    frame = frame.to_ndarray(format=&amp;#39;bgr24&amp;#39;)
    pass
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;PyAVを利用した場合は遅延が大幅に減少した．やったね．&lt;/p&gt;
&lt;h2 id=&#34;rtspで一定時間毎にフレーム取得pyav&#34;&gt;RTSPで一定時間毎にフレーム取得（PyAV）&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://localhost:1313/blog/tags/麻雀自動採譜/&#34; target=&#34;_blank&#34;&gt;麻雀自動採譜&lt;/a&gt;
牌認識に向けて，学習画像収集のためn秒おきに画像を撮影してみた．いつものOpenCVと同じ感覚で&lt;code&gt;time.sleep(n)&lt;/code&gt;を使用したところ以下のエラーが複数出現．&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-raw:&#34; data-lang=&#34;raw:&#34;&gt;max delay reached. need to consume packet
RTP: missed 6066 packets
RTP: PT=60: bad cseq a657 expected=8ea5
max delay reached. need to consume packet
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;sleepで無理に処理を中断させたため接続が不安定になったのかも．
通常の受信が安定していたことを踏まえ，とりあえず以下のコードで対応してみる．&lt;/p&gt;</description>
    </item>
    <item>
      <title>複数カメラの入力に対しYOLOv8で並列推論する</title>
      <link>http://localhost:1313/blog/posts/20231201_multiprocessingforyolov8/</link>
      <pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/posts/20231201_multiprocessingforyolov8/</guid>
      <description>&lt;h2 id=&#34;実装過程あれこれ１&#34;&gt;実装過程あれこれ１&lt;/h2&gt;
&lt;p&gt;これまでの作業でmultiprocessingモジュールを利用しプレーヤーの手牌を写す4台の
&lt;a href=&#34;https://www.tp-link.com/jp/smart-home/tapo/tapo-c200/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Tapo C200&lt;/a&gt;
からフレームを受信することができるようになりました．
&lt;br&gt;
早速各カメラに割り当てたプロセス毎に適当に学習させておいた
&lt;a href=&#34;https://docs.ultralytics.com/ja/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;YOLOv8&lt;/a&gt;
による牌推論を実行し，結果を描画してみました．わくわく！&lt;/p&gt;
&lt;p&gt;はい，映像出力が不穏な感じに．
&lt;video controls preload=&#34;auto&#34; width=&#34;100%&#34;  playsinline class=&#34;html-video&#34;&gt;
    &lt;source src=&#34;http://localhost:1313/blog/posts/20231201_multiprocessingforyolov8/before.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;span&gt;Your browser doesn&#39;t support embedded videos, but don&#39;t worry, you can &lt;a href=&#34;http://localhost:1313/blog/posts/20231201_multiprocessingforyolov8/before.mp4&#34;&gt;download it&lt;/a&gt; and watch it with your favorite video player!&lt;/span&gt;
&lt;/video&gt;&lt;/p&gt;
&lt;p&gt;卓の四隅にカメラを配置し，中央でお洒落カバーをバッサバッサしてます．&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;プロセス間でまったく映像出力のタイミングがあっていない&lt;/li&gt;
&lt;li&gt;フレーム間隔がまちまち&lt;/li&gt;
&lt;li&gt;映像の乱れが酷い&lt;/li&gt;
&lt;li&gt;FPSが低い（Tapo入力時15→10弱）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;特に映像出力タイミングがあっていない状況だと自動採譜なんてとてもじゃないけど実現できないよね&amp;hellip;&amp;hellip;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;フレーム出力がばらつく理由を勝手に予想．こんな感じだろうか&lt;/p&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;multiprocess_fig1.jpg&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;出力結果を見る感じ，どうやらフレームに映り込んでいる牌が多いほど推論時間が増加してるっぽい．次のフレーム入力までに推論が追いつかず，プロセス間でフレーム出力に差が出ているように見える．
&lt;br&gt;&lt;br&gt;
&lt;span style=&#34;font-size: 200%; color: red;&#34;&gt;
こんなときこそマルチプロセス処理の出番でしょ↓&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;multiprocess_fig2.jpg&#34;/&gt; 
&lt;/figure&gt;

（推論プロセスをフレームごとに分散させれば安定した出力が可能だよね）&lt;/p&gt;
&lt;p&gt;既にカメラごとにプロセスを分割しているため，その拡張のつもりで実装に取り掛かりました．設計は以下．&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;映像受信プロセス ×4（カメラの台数）&lt;/li&gt;
&lt;li&gt;推論処理プロセス ×n×4（任意で設定可能，カメラごとに割当）&lt;/li&gt;
&lt;li&gt;推論結果統合プロセス ×4（カメラの台数）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;さあやるぞ！&lt;/p&gt;
&lt;h2 id=&#34;実装過程あれこれ２&#34;&gt;実装過程あれこれ２&lt;/h2&gt;
&lt;p&gt;ここで問題発生．フラグによるイベント発生伝達や事前に渡してある引数を処理に使うのはできそうだが，起動済みのプロセス1,2,3の間でフレームndarrayを引き渡す方法がわからない．&lt;br&gt;
base64→内部UDPが一瞬頭を過るが無視して
&lt;a href=&#34;https://docs.python.org/ja/3.10/library/multiprocessing.html&#34; target=&#34;_blank&#34;&gt;公式ドキュメント&lt;/a&gt;
を漁ってみます．&lt;/p&gt;
&lt;p&gt;ドキュメントによると，python標準モジュールmultiprocessingにはメモリ共有やプロキシ経由でのオブジェクト操作をサポートする機能が幾つかあるんだとか．
Value, Array, Queue, Pipe, RawArray, Manager, shared_memory&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;多い．更にここ4,5年のPythonアップデートでmultiprocessingの仕様が大幅に変わっており，どれを使えばいいのか分かりづらい．&lt;/p&gt;
&lt;p&gt;試行錯誤の後，名前で共有メモリ領域を指定するshared_memoryを利用することに．
フラグと共有メモリ領域名を予め設定しておけば任意のタイミングで他プロセスが処理したndarrayを受け取れる．かなり便利ですね．&lt;/p&gt;
&lt;p&gt;そんなこんなで推論のマルチプロセス処理を実装しました．映像出力がこちらになります．
&lt;video controls preload=&#34;auto&#34; width=&#34;100%&#34;  playsinline class=&#34;html-video&#34;&gt;
    &lt;source src=&#34;http://localhost:1313/blog/posts/20231201_multiprocessingforyolov8/after.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;span&gt;Your browser doesn&#39;t support embedded videos, but don&#39;t worry, you can &lt;a href=&#34;http://localhost:1313/blog/posts/20231201_multiprocessingforyolov8/after.mp4&#34;&gt;download it&lt;/a&gt; and watch it with your favorite video player!&lt;/span&gt;
&lt;/video&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
